{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bbf255",
   "metadata": {},
   "source": [
    "# AudioCLIP Demo\n",
    "\n",
    "Authored by [Andrey Guzhov](https://github.com/AndreyGuzhov)\n",
    "\n",
    "This notebook covers common use cases of AudioCLIP and provides the typical workflow.\n",
    "Below, you will find information on:\n",
    "\n",
    "0. [Binary Assets](#Downloading-Binary-Assets)\n",
    "1. [Required imports](#Imports-&-Constants)\n",
    "2. [Model Instantiation](#Model-Instantiation)\n",
    "3. [Data Transformation](#Audio-&-Image-Transforms)\n",
    "4. Data Loading\n",
    "    * [Audio](#Audio-Loading)\n",
    "    * [Images](#Image-Loading)\n",
    "5. [Preparation of the Input](#Input-Preparation)\n",
    "6. [Acquisition of the Output](#Obtaining-Embeddings)\n",
    "7. [Normalization of Embeddings](#Normalization-of-Embeddings)\n",
    "8. [Calculation of Logit Scales](#Obtaining-Logit-Scales)\n",
    "9. [Computation of Similarities](#Computing-Similarities)\n",
    "10. Performing Tasks\n",
    "    1. [Classification](#Classification)\n",
    "        1. [Audio](#Audio)\n",
    "        2. [Images](#Images)\n",
    "    2. [Querying](#Querying)\n",
    "        1. [Audio by Text](#Audio-by-Text)\n",
    "        2. [Images by Text](#Images-by-Text)\n",
    "        3. [Audio by Images](#Audio-by-Images)\n",
    "        4. [Images by Audio](#Images-by-Audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a262a9",
   "metadata": {},
   "source": [
    "## Downloading Binary Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86673fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -P ../assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/bpe_simple_vocab_16e6.txt.gz\n",
    "! wget -P ../assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Full-Training.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7262c7e",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dbdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..'))\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "LABELS = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a327cc6",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclp = AudioCLIP(pretrained=f'../assets/{MODEL_FILENAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4643de",
   "metadata": {},
   "outputs": [],
   "source": [
    "statedict = torch.load(f'../assets/{MODEL_FILENAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the pretrained weight for the audio encoder part only\n",
    "# statedict.keys()\n",
    "from typing import OrderedDict\n",
    "\n",
    "audio_statedict = OrderedDict()\n",
    "for param_name, param_v in statedict.items():\n",
    "    if param_name.startswith(\"audio.\"):\n",
    "        stripped_param_name = param_name.replace(\"audio.\", \"\")\n",
    "\n",
    "        audio_statedict[stripped_param_name] = param_v\n",
    "\n",
    "audio_statedict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56e0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the audio model only\n",
    "# statedict.keys()\n",
    "from typing import Optional\n",
    "\n",
    "embed_dim: int = 1024\n",
    "n_fft: int = 2048\n",
    "hop_length: Optional[int] = 561\n",
    "win_length: Optional[int] = 1654\n",
    "window: Optional[str] = 'blackmanharris'\n",
    "normalized: bool = True\n",
    "onesided: bool = True\n",
    "spec_height: int = -1\n",
    "spec_width: int = -1\n",
    "apply_attention: bool = True\n",
    "multilabel: bool = True\n",
    "\n",
    "from model.esresnet import ESResNeXtFBSP\n",
    "\n",
    "# audio_model = ESResNeXtFBSP(\n",
    "#             n_fft=n_fft,\n",
    "#             hop_length=hop_length,\n",
    "#             win_length=win_length,\n",
    "#             window=window,\n",
    "#             normalized=normalized,\n",
    "#             onesided=onesided,\n",
    "#             spec_height=spec_height,\n",
    "#             spec_width=spec_width,\n",
    "#             num_classes=embed_dim,\n",
    "#             apply_attention=apply_attention,\n",
    "#             pretrained=False\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa672fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.esresnet import ESResNeXtFBSP\n",
    "\n",
    "class ESResNeXtFBSP_Binaural(nn.Module):\n",
    "    def __init__(self, pretrained=None):\n",
    "        super().__init__()\n",
    "        monoraul_net = ESResNeXtFBSP(\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            window=window,\n",
    "            normalized=normalized,\n",
    "            onesided=onesided,\n",
    "            spec_height=spec_height,\n",
    "            spec_width=spec_width,\n",
    "            num_classes=embed_dim,\n",
    "            apply_attention=apply_attention,\n",
    "            pretrained=False\n",
    "        )\n",
    "        if pretrained is not None and isinstance(pretrained, str):\n",
    "            # Loads pre-trained weights from AudioCLIP's audio encoder only\n",
    "            audioclip_statedict = torch.load(pretrained, map_location=\"cpu\")\n",
    "            # Extract the audio module's weight only\n",
    "            audio_statedict = OrderedDict()\n",
    "            for param_name, param_v in statedict.items():\n",
    "                if param_name.startswith(\"audio.\"):\n",
    "                    stripped_param_name = param_name.replace(\"audio.\", \"\")\n",
    "\n",
    "                    audio_statedict[stripped_param_name] = param_v\n",
    "            \n",
    "            monoraul_net.load_state_dict(audio_statedict)\n",
    "            # NOTE: is this necessary ?\n",
    "            audioclip_statedict, audio_statedict = None, None\n",
    "\n",
    "        # TODO: Any more efficient way of re-using the model ?\n",
    "        self.left_net = monoraul_net\n",
    "        self.right_net = deepcopy(monoraul_net)\n",
    "\n",
    "        self.fuse_nn = nn.Sequential(*[\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_left, x_right = x[:, 0, :], x[:, 1, :]\n",
    "\n",
    "        x_left = self.left_net(x_left)\n",
    "        x_right = self.right_net(x_right)\n",
    "\n",
    "        x = th.cat([x_left, x_right], dim=-1)\n",
    "        x = self.fuse_nn(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a30e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioclip_audio = ESResNeXtFBSP(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    window=window,\n",
    "    normalized=normalized,\n",
    "    onesided=onesided,\n",
    "    spec_height=spec_height,\n",
    "    spec_width=spec_width,\n",
    "    num_classes=embed_dim,\n",
    "    apply_attention=apply_attention,\n",
    "    pretrained=False\n",
    ")\n",
    "audioclip_encoder_rir = ESResNeXtFBSP_Binaural(audioclip_audio)\n",
    "audioclip_encoder_rir = audioclip_encoder_rir.to(th.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c54ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "audio_dummy_input = th.randn(3, 2, 44100).to(th.device(\"cuda\"))\n",
    "\n",
    "dummy_output = audioclip_encoder_rir(audio_dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the parameter names in the audio model\n",
    "# audio_model.state_dict().keys()\n",
    "\n",
    "# Load the pretrained weights\n",
    "audio_model.load_state_dict(audio_statedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32509c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the AudioClip's audio module for SS integration\n",
    "# import torchinfo\n",
    "# from torchinfo import summary\n",
    "# summary(aclp.audio)\n",
    "aclp.audio\n",
    "\n",
    "# Testing putting the audio model onto GPU\n",
    "# import torch as th\n",
    "# device = th.device(\"cuda\")\n",
    "# aclp.audio.to(device)\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39421f88",
   "metadata": {},
   "source": [
    "## Audio & Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transforms = ToTensor1D()\n",
    "\n",
    "image_transforms = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Resize(IMAGE_SIZE, interpolation=Image.BICUBIC),\n",
    "    tv.transforms.CenterCrop(IMAGE_SIZE),\n",
    "    tv.transforms.Normalize(IMAGE_MEAN, IMAGE_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5beab9",
   "metadata": {},
   "source": [
    "## Audio Loading\n",
    "Audio samples are drawn from the [ESC-50](https://github.com/karolpiczak/ESC-50) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_audio = glob.glob('audio/*.wav')\n",
    "\n",
    "audio = list()\n",
    "for path_to_audio in paths_to_audio:\n",
    "    track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "\n",
    "    # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "    # thus, the actual time-frequency representation will be visualized\n",
    "    spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "    spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "    pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "\n",
    "    audio.append((track, pow_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the expected input specs for the audio component of the model\n",
    "audio[0][0].shape, audio[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b239d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa.display import waveshow\n",
    "fig, axes = plt.subplots(2, len(audio), figsize=(20, 5), dpi=100)\n",
    "\n",
    "for idx in range(len(audio)):\n",
    "    track, pow_spec = audio[idx]\n",
    "\n",
    "    # draw the waveform\n",
    "    # librosa.display.waveplot(track, sr=SAMPLE_RATE, ax=axes[0, idx], color='k')\n",
    "    waveshow(track, sr=SAMPLE_RATE, ax=axes[0, idx], color='k')\n",
    "    # show the corresponding power spectrogram\n",
    "    axes[1, idx].imshow(pow_spec, origin='lower', aspect='auto', cmap='gray', vmin=-180.0, vmax=20.0)\n",
    "\n",
    "    # modify legend\n",
    "    axes[0, idx].set_title(os.path.basename(paths_to_audio[idx]))\n",
    "    axes[0, idx].set_xlabel('')\n",
    "    axes[0, idx].set_xticklabels([])\n",
    "    axes[0, idx].grid(True)\n",
    "    axes[0, idx].set_ylim(bottom=-1, top=1)\n",
    "\n",
    "    axes[1, idx].set_xlabel('Time (s)')\n",
    "    axes[1, idx].set_xticks(np.linspace(0, pow_spec.shape[1], len(axes[0, idx].get_xticks())))\n",
    "    axes[1, idx].set_xticklabels([f'{tick:.1f}' if tick == int(tick) else '' for tick in axes[0, idx].get_xticks()])\n",
    "    axes[1, idx].set_yticks(np.linspace(0, pow_spec.shape[0] - 1, 5))\n",
    "\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[1, 0].set_ylabel('Filter ID')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "for idx, path in enumerate(paths_to_audio):\n",
    "    print(os.path.basename(path))\n",
    "    display(Audio(audio[idx][0], rate=SAMPLE_RATE, embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc725f3f",
   "metadata": {},
   "source": [
    "## Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014356a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_images = glob.glob('images/*.jpg')\n",
    "\n",
    "images = list()\n",
    "for path_to_image in paths_to_images:\n",
    "    with open(path_to_image, 'rb') as jpg:\n",
    "        image = simplejpeg.decode_jpeg(jpg.read())\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, len(images) // 2, figsize=(16, 4), dpi=100)\n",
    "\n",
    "for idx, jdx in np.ndindex(axes.shape):\n",
    "    # re-arrange order to show the images column-wise\n",
    "    image_idx = np.ravel_multi_index(((jdx,), (idx,)), axes.shape[::-1]).item()\n",
    "    axes[idx, jdx].imshow(images[image_idx])\n",
    "\n",
    "    # modify legend\n",
    "    axes[idx, jdx].axis('off')\n",
    "    axes[idx, jdx].set_title(os.path.basename(paths_to_images[image_idx]))\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c11976",
   "metadata": {},
   "source": [
    "## Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
    "audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "# standard channel-first shape [batch x channels x height x width]\n",
    "images = torch.stack([image_transforms(image) for image in images])\n",
    "# textual input is processed internally, so no need to transform it beforehand\n",
    "text = [[label] for label in LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape of raw audio input\n",
    "audio.shape # torch.Size([5, 1, 220500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb9ef5",
   "metadata": {},
   "source": [
    "## Obtaining Embeddings\n",
    "For the sake of clarity, all three modalities are processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c71e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioCLIP's output: Tuple[Tuple[Features, Logits], Loss]\n",
    "# Features = Tuple[AudioFeatures, ImageFeatures, TextFeatures]\n",
    "# Logits = Tuple[AudioImageLogits, AudioTextLogits, ImageTextLogits]\n",
    "\n",
    "((audio_features, _, _), _), _ = aclp(audio=audio)\n",
    "((_, image_features, _), _), _ = aclp(image=images)\n",
    "((_, _, text_features), _), _ = aclp(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking the audio features\n",
    "audio_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45ed0",
   "metadata": {},
   "source": [
    "## Normalization of Embeddings\n",
    "The AudioCLIP's output is normalized using L<sub>2</sub>-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9758c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "image_features = image_features / torch.linalg.norm(image_features, dim=-1, keepdim=True)\n",
    "text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92adfb5",
   "metadata": {},
   "source": [
    "## Obtaining Logit Scales\n",
    "Outputs of the text-, image- and audio-heads are made consistent using dedicated scaling terms for each pair of modalities.\n",
    "The scaling factors are clamped between 1.0 and 100.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a89e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_audio_image = torch.clamp(aclp.logit_scale_ai.exp(), min=1.0, max=100.0)\n",
    "scale_audio_text = torch.clamp(aclp.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "scale_image_text = torch.clamp(aclp.logit_scale.exp(), min=1.0, max=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3dfd0",
   "metadata": {},
   "source": [
    "## Computing Similarities\n",
    "Similarities between different representations of a same concept are computed using [scaled](#Obtaining-Logit-Scales) dot product (cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3121148",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_audio_image = scale_audio_image * audio_features @ image_features.T\n",
    "logits_audio_text = scale_audio_text * audio_features @ text_features.T\n",
    "logits_image_text = scale_image_text * image_features @ text_features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0dfa2",
   "metadata": {},
   "source": [
    "## Classification\n",
    "This task is a specific case of a more general one, which is [querying](#Querying).\n",
    "However, this setup is mentioned as a standalone because it demonstrates clearly how to perform usual classification (including [zero-shot inference](https://github.com/openai/CLIP#zero-shot-prediction)) using AudioCLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd15af",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tFilename, Audio\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=1)\n",
    "for audio_idx in range(len(paths_to_audio)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[audio_idx].topk(3)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
    "    results = ', '.join([f'{LABELS[i]:>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc0350",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020de4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\tFilename, Image\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_image_text.softmax(dim=1)\n",
    "for image_idx in range(len(paths_to_images)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[image_idx].topk(3)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_images[image_idx]):>20s} ->\\t\\t'\n",
    "    results = ', '.join([f'{LABELS[i]:>20s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a1a06",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939334c",
   "metadata": {},
   "source": [
    "### Audio by Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0b4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\t\\tTextual Label\\t\\tFilename, Audio (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=0)\n",
    "for label_idx in range(len(LABELS)):\n",
    "    # acquire Top-2 most similar results\n",
    "    conf_values, ids = confidence[:, label_idx].topk(2)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{LABELS[label_idx]:>25s} ->\\t\\t'\n",
    "    results = ', '.join([f'{os.path.basename(paths_to_audio[i]):>30s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19165854",
   "metadata": {},
   "source": [
    "### Images by Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7133e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\tTextual Label\\t\\t\\tFilename, Image (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_image_text.softmax(dim=0)\n",
    "for label_idx in range(len(LABELS)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[:, label_idx].topk(3)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{LABELS[label_idx]:>20s} ->\\t\\t'\n",
    "    results = ', '.join([f'{os.path.basename(paths_to_images[i]):>20s} ({v:>06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae8c15",
   "metadata": {},
   "source": [
    "### Audio by Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea504a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\tTextual Label\\t\\t\\tFilename, Image (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_image.softmax(dim=0)\n",
    "for image_idx in range(len(paths_to_images)):\n",
    "    # acquire Top-2 most similar results\n",
    "    conf_values, ids = confidence[:, image_idx].topk(2)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_images[image_idx]):>25s} ->\\t\\t'\n",
    "    results = ', '.join([f'{os.path.basename(paths_to_audio[i]):>30s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44e32e",
   "metadata": {},
   "source": [
    "### Images by Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe3821",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\tTextual Label\\t\\t\\tFilename, Image (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_image.softmax(dim=1)\n",
    "for audio_idx in range(len(paths_to_audio)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[audio_idx].topk(3)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
    "    results = ', '.join([f'{os.path.basename(paths_to_images[i]):>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9f5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
