{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "from habitat.datasets import make_dataset\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.common.environments import AudioNavRLEnv\n",
    "\n",
    "config = get_config(\n",
    "    config_paths=\"ss_baselines/av_nav/config/audionav/mp3d/env_test_0.yaml\", # RGB + AudiogoalSensor\n",
    "    # opts=[\"CONTINUOUS\", \"True\"],\n",
    "    run_type=\"eval\")\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.MEASUREMENTS.append(\"TOP_DOWN_MAP\") # Note: can we add audio sensory info fields here too ?\n",
    "config.TASK_CONFIG.DATASET.SPLIT = config.EVAL.SPLIT\n",
    "config.freeze()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(id_dataset=config.TASK_CONFIG.DATASET.TYPE, config=config.TASK_CONFIG.DATASET)\n",
    "env = AudioNavRLEnv(config=config, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(observation) # habitat.core.simulator.Observations\n",
    "list(observation.keys()) # ['rgb', 'spectrogram'] or \n",
    "# observation[\"rgb\"].shape # (128, 128, 3)\n",
    "# observation[\"spectrogram\"].shape # (65, 26, 2), available when TASK.SENSORS = [\"SPECTROGRAM_SENSOR\"]\n",
    "# observation[\"audiogoal\"].shape # (2, 16000), Available wwhen TASK.SENSORS = ['AUDIOGOAL_SENSOR']\n",
    "# env.action_space # ActionSpace(MOVE_FORWARD:EmptySpace(), STOP:EmptySpace(), TURN_LEFT:EmptySpace(), TURN_RIGHT:EmptySpace())\n",
    "# action = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.imshow(observation[\"rgb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_fwd = {\"action\": \"MOVE_FORWARD\", \"action_args\": None}\n",
    "observation, reward, done, info = env.step(**action_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(observation.keys())\n",
    "# reward\n",
    "# done\n",
    "# list(info.keys()) # ['distance_to_goal', 'normalized_distance_to_goal', 'success', 'spl', 'softspl', 'na', 'sna', 'top_down_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect one full episode\n",
    "observation, done, ep_length = env.reset(), False, 0\n",
    "ep_observations = [observation]\n",
    "\n",
    "while not done:\n",
    "    # The \"STOP\" action will make the episode finish early, but \n",
    "    # this work around is not very efficient.\n",
    "    action = {\"action\": 'STOP', \"action_args\": None}\n",
    "    while action[\"action\"] == 'STOP':\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, done, info = env.step(**action)\n",
    "    ep_length += 1\n",
    "\n",
    "    ep_observations.append(observation)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"###########################################\")\n",
    "    print(f\"# DEBUG: Episode length: {ep_length}\")\n",
    "    print(f\"DEBUG: Done value {done}\")\n",
    "    print(\"###########################################\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'info' variable after the end of the peisode: do we get any additional infromation of the full episode ? For example all the RGB frames / audio waves as a nice list ?\n",
    "list(info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ep_observations) # 501 for randomly sampled episode\n",
    "# list(ep_observations[0].keys()) # ['rgb', 'audiogoal']\n",
    "ep_rgb_observations = [obs[\"rgb\"] for obs in ep_observations]\n",
    "ep_audiogoal_observations = [obs[\"audiogoal\"] for obs in ep_observations]\n",
    "# ep_rgb_observations.shape, ep_audiogoal_observations.shape # ((501, 128, 128, 3), (501, 2, 16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import moviepy.editor as mpy\n",
    "from moviepy.audio.AudioClip import CompositeAudioClip, AudioArrayClip\n",
    "\n",
    "def images_to_video_with_audio(\n",
    "    images: List[np.ndarray],\n",
    "    output_dir: str,\n",
    "    video_name: str,\n",
    "    audios: List[str],\n",
    "    sr: int,\n",
    "    fps: int = 1,\n",
    "    quality: Optional[float] = 5,\n",
    "    **kwargs\n",
    "):\n",
    "    r\"\"\"Calls imageio to run FFMPEG on a list of images. For more info on\n",
    "    parameters, see https://imageio.readthedocs.io/en/stable/format_ffmpeg.html\n",
    "    Args:\n",
    "        images: The list of images. Images should be HxWx3 in RGB order.\n",
    "        output_dir: The folder to put the video in.\n",
    "        video_name: The name for the video.\n",
    "        audios: raw audio files\n",
    "        fps: Frames per second for the video. Not all values work with FFMPEG,\n",
    "            use at your own risk.\n",
    "        quality: Default is 5. Uses variable bit rate. Highest quality is 10,\n",
    "            lowest is 0.  Set to None to prevent variable bitrate flags to\n",
    "            FFMPEG so you can manually specify them using output_params\n",
    "            instead. Specifying a fixed bitrate using ‘bitrate’ disables\n",
    "            this parameter.\n",
    "    \"\"\"\n",
    "    assert 0 <= quality <= 10\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    video_name = video_name.replace(\" \", \"_\").replace(\"\\n\", \"_\") + \".mp4\"\n",
    "\n",
    "    audio_clips = []\n",
    "    multiplier = 0.5\n",
    "    for i, audio in enumerate(audios):\n",
    "        audio_clip = AudioArrayClip(audio.T[:int(sr * 1 / fps)] * multiplier, fps=sr)\n",
    "        audio_clip = audio_clip.set_start(1 / fps * i)\n",
    "        audio_clips.append(audio_clip)\n",
    "    composite_audio_clip = CompositeAudioClip(audio_clips)\n",
    "    video_clip = mpy.ImageSequenceClip(images, fps=fps)\n",
    "    video_with_new_audio = video_clip.set_audio(composite_audio_clip)\n",
    "    video_with_new_audio.write_videofile(os.path.join(output_dir, video_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.TASK_CONFIG.SIMULATOR.AUDIO.RIR_SAMPLING_RATE # 16000 for mp3d datset\n",
    "config.TASK_CONFIG.SIMULATOR.VIEW_CHANGE_FPS # 10 for mp3d by default ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_video_with_audio(\n",
    "    images=ep_rgb_observations,\n",
    "    output_dir=\"/tmp/ss-videos\",\n",
    "    video_name=\"ss_video_dgb\",\n",
    "    audios=ep_audiogoal_observations,\n",
    "    sr=config.TASK_CONFIG.SIMULATOR.AUDIO.RIR_SAMPLING_RATE, # 16000 for mp3d dataset\n",
    "    fps=config.TASK_CONFIG.SIMULATOR.VIEW_CHANGE_FPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing videoe with audio passing to tensorboard\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch as th\n",
    "writer = SummaryWriter(\"/tmp/ss-videos/tbtest\")\n",
    "vid_tensor = np.array([[obs[\"rgb\"] for obs in ep_observations]]) # [1, N, H, W, C]\n",
    "# vid_tensor.shape\n",
    "vid_tensor = th.Tensor(vid_tensor).permute(0, 1, 4, 3, 2)\n",
    "vid_tensor.shape\n",
    "for gstep in range(10):\n",
    "    # writer.add_video(tag=\"video\", vid_tensor=vid_tensor, global_step=gstep, fps=10)\n",
    "    writer.add_video(\"audio_video\", \"/tmp/ss-videos/ss_video_dgb.mp4\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with WANDB and audio video logging\n",
    "import wandb\n",
    "wandb.init(project=\"ss-hab-test\", entity=\"dosssman\", settings=wandb.Settings(start_method='thread'))\n",
    "wandb.log({\"audio_video\": wandb.Video(\n",
    "                data_or_path=\"/tmp/ss-videos/ss_video_dgb.mp4\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ss-hab-headless')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00027260fa4f823a31ae38ab2393c0dcefb86013ebf0fa3f15d0752952fe71d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
