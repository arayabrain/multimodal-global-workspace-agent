{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 1\n",
    "H = 8\n",
    "H_GW = 4\n",
    "\n",
    "vis_feats = th.randn([1, H]); vis_feats.shape\n",
    "aud_feats = th.randn([1, H]); aud_feats.shape\n",
    "gw = th.randn([1, H_GW]); gw.shape\n",
    "if H != H_GW:\n",
    "  assert H > H_GW, f\"Invalid vis-aud: {H} and GW: {H_GW} shape combination\"\n",
    "  # Pad the reduced global workspace\n",
    "  gw = th.cat([\n",
    "    gw,\n",
    "    gw.new_zeros([B, H - H_GW])\n",
    "  ], dim=-1)\n",
    "  # print(f\"Post cat dim: {gw.shape}\")\n",
    "\n",
    "  # Generate the key_padding_mask\n",
    "  # NOTE: according to doc of attn_mask field in Pytorch, a True / 1 value mean\n",
    "  # that the position is not allowed to attend to\n",
    "  attn_mask = gw.new_zeros([B, 3, H]).bool()\n",
    "  attn_mask[:, 2, H_GW:] = True\n",
    "\n",
    "  # attn_mask.shape; attn_mask.float()\n",
    "\n",
    "q = th.cat([\n",
    "  vis_feats[:, None, :],\n",
    "  aud_feats[:, None, :],\n",
    "  gw[:, None, :]\n",
    "], dim=1); q.shape\n",
    "\n",
    "kv = th.cat([\n",
    "  vis_feats[:, None, :],\n",
    "  aud_feats[:, None, :],\n",
    "  gw[:, None, :]\n",
    "], dim=1); kv.shape\n",
    "kv = th.cat([\n",
    "  kv,\n",
    "  kv.new_zeros([1, 1, H])\n",
    "], dim=1); kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = nn.MultiheadAttention(\n",
    "  H,\n",
    "  1,\n",
    "  dropout=0.0,\n",
    "  add_zero_attn=False,\n",
    "  batch_first=True,\n",
    "  kdim=H,\n",
    "  vdim=H\n",
    ")\n",
    "\n",
    "# key_padding_mask = th.ones([3, H]); key_padding_mask.shape\n",
    "attn_values, attn_weights = mha(q, kv, kv); attn_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, kv.shape, kv.mT.shape\n",
    "A = (q @ kv.mT * H ** -0.5).softmax(dim=-1); A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = (A @ kv); B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1607, -0.1777, -0.0145,  0.0072, -0.2937,  0.1897, -0.0494,\n",
       "           -0.3013],\n",
       "          [-0.1219, -0.1638,  0.0980, -0.0303, -0.2723,  0.1179, -0.0071,\n",
       "           -0.2830],\n",
       "          [-0.1423, -0.2004,  0.1438, -0.0354, -0.3020,  0.1045,  0.0021,\n",
       "           -0.3100]]], grad_fn=<TransposeBackward0>),\n",
       " tensor([[[-0.6238,  0.4845, -0.1870,  0.2597, -0.6642, -0.6949,  1.1371,\n",
       "           -0.1859],\n",
       "          [-1.3688,  0.0429, -0.2187,  0.2357, -0.4113, -0.1630,  1.0966,\n",
       "            0.9588],\n",
       "          [-1.0530,  0.0263, -0.0991, -0.0427, -0.2852, -0.1889,  0.6492,\n",
       "            0.3603]]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_values, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
