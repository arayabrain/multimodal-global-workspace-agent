{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseExecution(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"{layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "from typing import Dict, Iterable, Callable\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "\n",
    "        for layer_id in layers:\n",
    "            layer = dict([*self.model.named_modules()])[layer_id]\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_id))\n",
    "\n",
    "    def save_outputs_hook(self, layer_id: str) -> Callable:\n",
    "        def fn(_, __, output):\n",
    "            self._features[layer_id] = output\n",
    "        return fn\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        _ = self.model(x)\n",
    "        return self._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ca.ln_q -> torch.Size([3, 8, 64])\n",
      "ca.ln_kv -> torch.Size([3, 2, 512])\n",
      "ca.ff_self.0 -> torch.Size([3, 8, 64])\n",
      "ca.ff_self.1 -> torch.Size([3, 8, 64])\n",
      "ca.ff_self.2 -> torch.Size([3, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "# from perceiver_gwt_gwwm import Perceiver_GWT_GWWM\n",
    "from perceiver_gwt_gwwm import SelfAttention, CrossAttention\n",
    "class Perceiver_GWT_GWWM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        input_dim,\n",
    "        latent_type = \"randn\",\n",
    "        latent_learned = True,\n",
    "        num_latents = 8,\n",
    "        latent_dim = 64,\n",
    "        cross_heads = 1, # LucidRains's implm. uses 1 by default\n",
    "        latent_heads = 4, # LucidRains's implm. uses 8 by default\n",
    "        # cross_dim_head = 64,\n",
    "        # latent_dim_head = 64,\n",
    "        # self_per_cross_attn = 1, # Number of self attention blocks per cross attn.\n",
    "        # Modality embeddings realted\n",
    "        hidden_size = 512, # Dim of the visual / audio encoder outputs\n",
    "        mod_embed = 0, # Dimensio of learned modality embeddings\n",
    "        use_sa = False,\n",
    "        ca_prev_latents = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_latents = num_latents # N\n",
    "        self.latent_dim = latent_dim # D\n",
    "        self.latent_type = latent_type\n",
    "        self.latent_learned = latent_learned\n",
    "\n",
    "        self.mod_embed = mod_embed\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_sa = use_sa\n",
    "        self.ca_prev_latents = ca_prev_latents\n",
    "\n",
    "        # Cross Attention\n",
    "        if self.ca_prev_latents:\n",
    "            assert num_latents * latent_dim == input_dim, \\\n",
    "                f\"input_dim=={input_dim} and num_latents * latent_dim=={num_latents * latent_dim} must match\"\n",
    "        self.ca = CrossAttention(latent_dim, input_dim + mod_embed,\n",
    "            n_heads=cross_heads, skip_q=True) # If not skipping, usually blows\n",
    "        # Self Attention\n",
    "        if self.use_sa:\n",
    "            self.sa = SelfAttention(latent_dim, n_heads=latent_heads)\n",
    "        # self.decoder = CrossAttention(self.h_size, self.s_size, skip_q=True)\n",
    "\n",
    "        # Modality embedding\n",
    "        if self.mod_embed:\n",
    "            self.modality_embeddings = nn.Parameter(torch.randn(1, 2 + int(ca_prev_latents), mod_embed))\n",
    "        \n",
    "        # Latent vector, supposedly equivalent to an RNN's hidden state\n",
    "        if latent_type == \"randn\":\n",
    "            self.latents = torch.randn(1, num_latents, latent_dim)\n",
    "            # As per original paper\n",
    "            with th.no_grad():\n",
    "                self.latents.normal_(0.0, 0.02).clamp_(-2.0,2.0)\n",
    "        elif latent_type == \"zeros\":\n",
    "            self.latents = torch.zeros(1, num_latents, latent_dim)\n",
    "        \n",
    "        self.latents = nn.Parameter(self.latents, requires_grad=latent_learned)\n",
    "\n",
    "        # Hooking\n",
    "\n",
    "\n",
    "    def seq_forward(self, data, prev_latents, masks):\n",
    "        # TODO: a more optimal method to process sequences of same length together ?\n",
    "        x_list, latents_list = [], []\n",
    "\n",
    "        T_B, feat_dim = data.shape\n",
    "        B = prev_latents.shape[0]\n",
    "        T = T_B // B # TODO: assert that T * B == T_B exactly\n",
    "        latents = prev_latents.clone()\n",
    "\n",
    "        data = data.reshape(T, B, feat_dim)\n",
    "        masks = masks.reshape(T, B, 1)\n",
    "\n",
    "        for t in range(T):\n",
    "            x, latents = self.single_forward(data[t], latents, masks[t])\n",
    "\n",
    "            x_list.append(x.clone())\n",
    "            latents_list.append(latents.clone())\n",
    "        \n",
    "        x_list = th.stack(x_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, feat_dim]\n",
    "        latents_list = th.stack(latents_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, num_latents, latent_dim]\n",
    "\n",
    "        return x_list, latents_list\n",
    "\n",
    "    def single_forward(self, data, prev_latents, masks):\n",
    "        b = data.shape[0] # Batch size\n",
    "\n",
    "        if data.dim() == 2:\n",
    "            data = data.reshape(b, 2, self.hidden_size) # [B,1024] -> [B,2,512]\n",
    "        \n",
    "        if self.ca_prev_latents:\n",
    "            # NOTE: flattened latents dim must equal dim of audio, vision features\n",
    "            data = th.cat([data, prev_latents.flatten(start_dim=1)[:, None, :]], dim=1) # [B, 2, H] and [B, 1, L * D] -> [B, 3, H == L * D]\n",
    "\n",
    "        if self.mod_embed:\n",
    "            data = th.cat([data, self.modality_embeddings.repeat(b, 1, 1)], dim=2)\n",
    "        \n",
    "        # If the current step is the start of a new episode,\n",
    "        # the the mask will contain 0\n",
    "        prev_latents = masks[:, :, None] * prev_latents + \\\n",
    "            (1. - masks[:, :, None]) * self.latents.repeat(b, 1, 1)\n",
    "\n",
    "        x = prev_latents\n",
    "        \n",
    "        # Cross Attention\n",
    "        x, _ = self.ca(x, data) # x: [B, N * D], x_weights: [B, ???]\n",
    "\n",
    "        # Self Attention\n",
    "        if self.use_sa:\n",
    "            x, _ = self.sa(x) # x: [B, N * D]\n",
    "        \n",
    "        return x.flatten(start_dim=1), x\n",
    "\n",
    "    def forward(self, data, prev_latents, masks):\n",
    "        \"\"\"\n",
    "            - data: observation features [NUM_ENVS, feat_dim] or [NUM_ENVS, NUM_STEPS, feat_dim]\n",
    "            - prev_latents: previous latents [B, num_latents, latent_dim]\n",
    "            - masks: not Perceiver mask, but end-of-episode signaling mask\n",
    "                - shape of [NUM_ENVS, 1] if single step forward\n",
    "                - shape of [NUM_ENVS, NUM_STEPS, 1] if sequence forward\n",
    "        \"\"\"\n",
    "        if data.size(0) == prev_latents.size(0):\n",
    "            return self.single_forward(data, prev_latents, masks)\n",
    "        else:\n",
    "            return self.seq_forward(data, prev_latents, masks)\n",
    "\n",
    "state_encoder = Perceiver_GWT_GWWM(\n",
    "            input_dim = 512,\n",
    "            # latent_type = \"rand\"\n",
    "            # latent_learned = config.pgwt_latent_learned,\n",
    "            num_latents = 8,\n",
    "            latent_dim = 64,\n",
    "            cross_heads = 1,\n",
    "            latent_heads = 4,\n",
    "            # cross_dim_head = config.pgwt_cross_dim_head, # Default: 64\n",
    "            # latent_dim_head = config.pgwt_latent_dim_head, # Default: 64\n",
    "            # self_per_cross_attn = 1,\n",
    "            # Modality embedding related\n",
    "            mod_embed = 0,\n",
    "            hidden_size = 512,\n",
    "            use_sa = False,\n",
    "            ca_prev_latents = False\n",
    "        ); # list(state_encoder.named_modules())\n",
    "\n",
    "# for i, nm in enumerate(state_encoder.named_modules()):\n",
    "#     if i: # Skips the first '' that tcontains the whole model\n",
    "#         print(nm) # tuple: (name, nn.Module)\n",
    "    # break \n",
    "# print(state_encoder)\n",
    "_state_encoder = FeatureExtractor(state_encoder, layers=[\n",
    "    # CrossAttentio block\n",
    "    # \"ca\",\n",
    "        \"ca.ln_q\",\n",
    "        \"ca.ln_kv\",\n",
    "        # \"ca.mha\", # attention_value, attention_weight\n",
    "        # \"ca.ff_self\",\n",
    "            \"ca.ff_self.0\", # Layer Norm\n",
    "            \"ca.ff_self.1\", # Linear\n",
    "            \"ca.ff_self.2\", # GELU\n",
    "        #     \"ca.ff_self.3\"  # Linear\n",
    "    ])\n",
    "\n",
    "prev_latents = state_encoder.latents.clone().repeat(3, 1, 1)\n",
    "obs_feats = th.zeros(3, 512 * 2)\n",
    "# obs_feats.shape, prev_latents.shape\n",
    "state_feats, next_latents = state_encoder(obs_feats, prev_latents, th.ones([3, 1]))\n",
    "print(\"\"); print(\"\")\n",
    "for k, v in _state_encoder._features.items():\n",
    "    # print(k, len(v))\n",
    "    # break\n",
    "    # print(f\"{k} -> {[vv.shape for vv in v]}\")\n",
    "    print(f\"{k} -> {v.shape if isinstance(v, th.Tensor) else [vv.shape for vv in v]}\")\n",
    "# state_feats.shape, next_latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype the hooking of a complete agent, distinguish between the various parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 16:44:30,257 Initializing dataset AudioNav\n",
      "2022-09-12 16:44:30,266 Initializing dataset AudioNav\n",
      "2022-09-12 16:45:37,825 initializing sim SoundSpacesSim\n",
      "2022-09-12 16:45:38,082 Initializing task AudioNav\n"
     ]
    }
   ],
   "source": [
    "# PPO GRU Agent\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic\n",
    "\n",
    "config_path = \"env_configs/audiogoal_depth_nocont.yaml\"\n",
    "env_config = get_config(config_paths=config_path)\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "env_config.USE_SYNC_VECENV = True\n",
    "# env_config.USE_VECENV = False\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "env_config.freeze()\n",
    "# print(env_config)\n",
    "\n",
    "# Environment instantiation\n",
    "envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "single_observation_space = envs.observation_spaces[0]\n",
    "single_action_space = envs.action_spaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual_encoder.cnn.0 -> [torch.Size([32, 31, 31]), torch.Size([32, 31, 31])]\n",
      "visual_encoder.cnn.1 -> [torch.Size([32, 31, 31]), torch.Size([32, 31, 31])]\n",
      "visual_encoder.cnn.2 -> [torch.Size([64, 14, 14]), torch.Size([64, 14, 14])]\n",
      "visual_encoder.cnn.3 -> [torch.Size([64, 14, 14]), torch.Size([64, 14, 14])]\n",
      "visual_encoder.cnn.4 -> [torch.Size([64, 6, 6]), torch.Size([64, 6, 6])]\n",
      "visual_encoder.cnn.5 -> [torch.Size([2304]), torch.Size([2304])]\n",
      "visual_encoder.cnn.6 -> [torch.Size([512]), torch.Size([512])]\n",
      "visual_encoder.cnn.7 -> [torch.Size([512]), torch.Size([512])]\n",
      "audio_encoder.cnn.0 -> [torch.Size([32, 31, 11]), torch.Size([32, 31, 11])]\n",
      "audio_encoder.cnn.1 -> [torch.Size([32, 31, 11]), torch.Size([32, 31, 11])]\n",
      "audio_encoder.cnn.2 -> [torch.Size([64, 15, 5]), torch.Size([64, 15, 5])]\n",
      "audio_encoder.cnn.3 -> [torch.Size([64, 15, 5]), torch.Size([64, 15, 5])]\n",
      "audio_encoder.cnn.4 -> [torch.Size([64, 13, 3]), torch.Size([64, 13, 3])]\n",
      "audio_encoder.cnn.5 -> [torch.Size([2496]), torch.Size([2496])]\n",
      "audio_encoder.cnn.6 -> [torch.Size([512]), torch.Size([512])]\n",
      "audio_encoder.cnn.7 -> [torch.Size([512]), torch.Size([512])]\n",
      "action_distribution.linear -> [torch.Size([4]), torch.Size([4])]\n",
      "critic.fc -> [torch.Size([1]), torch.Size([1])]\n",
      "state_encoder -> [torch.Size([2, 512]), torch.Size([1, 2, 512])]\n"
     ]
    }
   ],
   "source": [
    "# PPO GRU Agent\n",
    "\n",
    "# region: inspect the strcuture of the PPO GRU agent\n",
    "# agent = ActorCritic(single_observation_space, \n",
    "#                     single_action_space,\n",
    "#                     512, extra_rgb=False); agent\n",
    "\n",
    "# for i, nm in enumerate(agent.named_modules()):\n",
    "#     if i: # Skips the first '' that contains the whole model\n",
    "#         print(nm) # tuple: (name, nn.Module)\n",
    "\n",
    "# List of layers of which to capture the intermediate outputs\n",
    "layer_names = [\n",
    "    *[f\"visual_encoder.cnn.{i}\" for i in range(8)], # Shared arch. for GRU and PGWT\n",
    "    *[f\"audio_encoder.cnn.{i}\" for i in range(8)], # Shared arch. for GRU and PGWT\n",
    "    \"action_distribution.linear\", # Shared arch. for any type of agent\n",
    "    \"critic.fc\", # Shared arch. for any type of agent\n",
    "    \n",
    "    \"state_encoder\", # Either GRU or PGWT based. Shape different based on nature of cell though.\n",
    "]\n",
    "# layer_names\n",
    "# endregion: inspect the strcuture of the PPO GRU agent\n",
    "\n",
    "# region: Prototyping the PPO GRU agent with layer output recording\n",
    "agent = ActorCritic(single_observation_space, \n",
    "                    single_action_space,\n",
    "                    512, extra_rgb=False,\n",
    "                    analysis_layers=layer_names)\n",
    "                    # analysis_layers=[])\n",
    "\n",
    "batch_size = 2\n",
    "prev_states = th.zeros(1, batch_size, 512)\n",
    "masks = th.ones(batch_size, 1)\n",
    "obs_dict = { k: th.randn([batch_size, *v.shape]) for k,v in single_observation_space.items()}\n",
    "# for k, v in obs_dict.items():\n",
    "#     print(k, v.shape)\n",
    "\n",
    "actions, action_logprobs, dist_ent, values, latents = agent.act(obs_dict, prev_states, masks)\n",
    "actions.shape, action_logprobs.shape, dist_ent.shape, values.shape, latents.shape\n",
    "\n",
    "for k, v in agent._features.items():\n",
    "    print(f\"{k} -> {[vv.shape for vv in v] if isinstance(v[0], th.Tensor) else v[0].shape}\")\n",
    "# [v.shape for v in agent._features[\"state_encoder\"]]\n",
    "# endregion: Prototyping the PPO GRU agent with layer output recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO PGWT Agent\n",
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "from configurator import get_arg_dict, generate_args\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/audiogoal_depth_nocont.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.2), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"deep-etho\",\n",
    "                    \"perceiver-gwt-gwwm\", \"perceiver-gwt-attgru\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, False, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# single_observation_space, single_action_space\n",
    "pgwt_agent = Perceiver_GWT_GWWM_ActorCritic(single_observation_space, \n",
    "                    single_action_space,\n",
    "                    args, extra_rgb=False); pgwt_agent\n",
    "\n",
    "# for i, nm in enumerate(pgwt_agent.named_modules()):\n",
    "#     if i: # Skips the first '' that contains the whole model\n",
    "#         print(nm) # tuple: (name, nn.Module)\n",
    "\n",
    "# List of layers of which to capture the intermediate outputs\n",
    "layer_names = [\n",
    "    *[f\"visual_encoder.cnn.{i}\" for i in range(8)], # Shared arch. for GRU and PGWT\n",
    "    *[f\"audio_encoder.cnn.{i}\" for i in range(8)], # Shared arch. for GRU and PGWT\n",
    "    \"action_distribution.linear\", # Shared arch. for any type of agent\n",
    "    \"critic.fc\", # Shared arch. for any type of agent\n",
    "\n",
    "    \"state_encoder\", # Either GRU or PGWT based. Shape different based on nature of cell though.\n",
    "    ## PGWT GWWM specific\n",
    "    \"state_encoder.ca\", # This will have the output of the residual conn. self.ff_self(attention_value) + attention_value\n",
    "    \"state_encoder.ca.mha\", # Note: output here is tuple (attention_value, attention_weight)\n",
    "    \"state_encoder.ca.ln_q\",\n",
    "    \"state_encoder.ca.ln_kv\",\n",
    "    \"state_encoder.ca.ff_self\", # No residual connection output\n",
    "    *[f\"state_encoder.ca.ff_self.{i}\" for i in range(4)], # [LayerNorm, Linear, GELU, Linear]\n",
    "    ## TODO: add support for the SA layers too\n",
    "]\n",
    "layer_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
