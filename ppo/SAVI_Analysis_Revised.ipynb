{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import apex\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "import rsatoolbox\n",
    "from torchinfo import summary\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "# plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "import models\n",
    "from models import GW_Actor, GRU_Actor\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline\n",
    "\n",
    "from ss_baselines.common.utils import plot_top_down_map\n",
    "\n",
    "\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"white\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General config related\n",
    "from configurator import get_arg_dict, generate_args\n",
    "\n",
    "# Env config related\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "\t# General hyper parameters\n",
    "\tget_arg_dict(\"seed\", int, 111),\n",
    "\tget_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "\n",
    "\t# Behavior cloning gexperiment config\n",
    "\tget_arg_dict(\"dataset-path\", str, \"SAVI_Oracle_Dataset_v0\"),\n",
    "\n",
    "\t# SS env config\n",
    "\tget_arg_dict(\"config-path\", str, \"env_configs/savi/savi_ss1_rgb_spectro.yaml\"),\n",
    "\n",
    "\t# PPO Hyper parameters\n",
    "\tget_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "\tget_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "\tget_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "\tget_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "\tget_arg_dict(\"gamma\", float, 0.99),\n",
    "\tget_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "\tget_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "\tget_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "\tget_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "\tget_arg_dict(\"ent-coef\", float, 0.2), # Entropy loss coef; 0.2 in SS baselines\n",
    "\tget_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "\tget_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "\tget_arg_dict(\"target-kl\", float, None),\n",
    "\tget_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "\tget_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "\t## Agent network params\n",
    "\tget_arg_dict(\"agent-type\", str, \"gw\", metatype=\"choice\",\n",
    "\t\tchoices=[\"gw\", \"gru\"]),\n",
    "\tget_arg_dict(\"gru-type\", str, \"layernorm\", metatype=\"choice\",\n",
    "\t\t\t\t\tchoices=[\"default\", \"layernorm\"]),\n",
    "\tget_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features\n",
    "\n",
    "\t## BC related hyper parameters\n",
    "\tget_arg_dict(\"batch-chunk-length\", int, 0), # For gradient accumulation\n",
    "\tget_arg_dict(\"dataset-ce-weights\", bool, False, metatype=\"bool\"), # If True, will read CEL weights based on action dist. from the 'dataset_statistics.bz2' file.\n",
    "\tget_arg_dict(\"ce-weights\", float, None, metatype=\"list\"), # Weights for the Cross Entropy loss\n",
    "\n",
    "\t## GW Agent with custom attention, recurrent encoder and null inputs\n",
    "\tget_arg_dict(\"gw-size\", int, 512), # Dim of the GW vector\n",
    "\tget_arg_dict(\"recenc-use-gw\", bool, True, metatype=\"bool\"), # Use GW at Recur. Enc. level\n",
    "\tget_arg_dict(\"recenc-gw-detach\", bool, True, metatype=\"bool\"), # When using GW at Recurrent Encoder level, whether to detach the grads or not\n",
    "\tget_arg_dict(\"gw-use-null\", bool, True, metatype=\"bool\"), # Use Null at CrossAtt level\n",
    "\tget_arg_dict(\"gw-cross-heads\", int, 1), # num_heads of the CrossAttn\n",
    "\n",
    "\t# Eval protocol\n",
    "\tget_arg_dict(\"eval\", bool, True, metatype=\"bool\"),\n",
    "\tget_arg_dict(\"eval-every\", int, int(1.5e4)), # Every X frames || steps sampled\n",
    "\tget_arg_dict(\"eval-n-episodes\", int, 5),\n",
    "\n",
    "\t# Logging params\n",
    "\t# NOTE: Video logging expensive\n",
    "\tget_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "\tget_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "\tget_arg_dict(\"save-model-every\", int, int(5e5)), # Every X frames || steps sampled\n",
    "\tget_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "\tget_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "\tget_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "\tenv_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "\tenv_config = get_config(config_paths=args.config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate obs / act space based on args and env_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict(audiogoal:Box(-3.4028235e+38, 3.4028235e+38, (2, 16000), float32), rgb:Box(0, 255, (128, 128, 3), uint8), spectrogram:Box(-3.4028235e+38, 3.4028235e+38, (65, 26, 2), float32)),\n",
       " Discrete(4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "env_config.USE_SYNC_VECENV = True\n",
    "# env_config.USE_VECENV = False\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "env_config.freeze()\n",
    "# print(env_config)\n",
    "\n",
    "# Environment instantiation\n",
    "# envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "# Dummy environment spaces\n",
    "\n",
    "# TODO: add dyanmicallly set single_observation_space so that RGB and RGBD based variants\n",
    "# can be evaluated at thet same time\n",
    "from gym import spaces\n",
    "single_action_space = spaces.Discrete(4)\n",
    "single_observation_space = spaces.Dict({\n",
    "    \"rgb\": spaces.Box(shape=[128,128,3], low=0, high=255, dtype=np.uint8),\n",
    "    # \"depth\": spaces.Box(shape=[128,128,1], low=0, high=255, dtype=np.uint8),\n",
    "    \"audiogoal\": spaces.Box(shape=[2,16000], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32),\n",
    "    \"spectrogram\": spaces.Box(shape=[65,26,2], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32)\n",
    "})\n",
    "# single_observation_space = envs.observation_spaces[0]\n",
    "# single_action_space = envs.action_spaces[0]\n",
    "\n",
    "single_observation_space, single_action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Category-Scene-Trajs, Scene-Category-Trajs, and Dataset's metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads data for analysis, as well as dataset's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of categories C: 6 | # of scenes: 56\n",
      "TARGET_CATEGORY_DICT: {'chair': 0, 'picture': 1, 'table': 2, 'cushion': 3, 'cabinet': 4, 'plant': 5}\n",
      "TARGET_SCENE_DICT: {'gTV8FGcVJC9': 0, '5LpN3gDmAk7': 1, 'vyrNrziPKCB': 2, 'b8cTxDM8gDG': 3, 'Vvot9Ly1tCj': 4, 'rPc6DW4iMge': 5, 'PuKPg4mmafe': 6, '759xd9YjKW5': 7, 'ZMojNkEp431': 8, 'VzqfbhrpDEA': 9, 'ac26ZMwG7aT': 10, 'D7N2EKCX4Sj': 11, 'E9uDoFAP3SH': 12, 'S9hNv5qa7GM': 13, '5q7pvUzZiYa': 14, 'kEZ7cmS4wCh': 15, 'VFuaQ6m2Qom': 16, '7y3sRwLe3Va': 17, 'p5wJjkQkbXX': 18, 'V2XKFyX4ASd': 19, 'VVfe2KiqLaN': 20, 'mJXqzFtmKg4': 21, 'SN83YJsR3w2': 22, 'EDJbREhghzL': 23, 'PX4nDJXEHrG': 24, 'JmbYfDe2QKZ': 25, 'r1Q1Z4BcV1o': 26, 'aayBHfsNo7d': 27, 'r47D5H71a5s': 28, 'pRbA3pwrgk9': 29, 'Pm6F8kyY3z2': 30, 'sKLMLpTHeUy': 31, 'GdvgFV5R1Z5': 32, 'e9zR4mvMWw7': 33, 'JeFG25nYj2p': 34, 'B6ByNegPMKs': 35, 'uNb9QFRL6hY': 36, 'cV4RVeZvu5T': 37, 'D7G3Y4RVNrH': 38, 'XcA2TqTSSAj': 39, 'ur6pFq6Qu1A': 40, '29hnd4uzFmX': 41, 's8pcmisQ38h': 42, 'qoiz87JEwZ2': 43, 'ULsKaCPVFJR': 44, '1pXnuDYAj8r': 45, 'VLzqgDo317F': 46, 'YmJkqBEsHnH': 47, 'sT4fr6TAbpF': 48, 'jh4fc5c5qoQ': 49, '8WUmhLawc2A': 50, '1LXtFkjw3qL': 51, '17DRP5sb8fy': 52, '82sE5b5pLXE': 53, 'JF19kD82Mey': 54, 'Uxmj2M2itWa': 55}\n",
      "\n",
      "chair:\n",
      "\tgTV8FGcVJC9: [28, 10, 46, 9, 6]\n",
      "\tb8cTxDM8gDG: [15, 19, 26, 20, 8]\n",
      "\tD7N2EKCX4Sj: [19, 36, 12, 40, 33]\n",
      "\tVvot9Ly1tCj: [18, 21, 23, 16, 27]\n",
      "\tvyrNrziPKCB: [20, 24, 10, 14, 28]\n",
      "\n",
      "picture:\n",
      "\tgTV8FGcVJC9: [10, 12, 16, 20, 17]\n",
      "\tD7N2EKCX4Sj: [46, 31, 53, 39, 43]\n",
      "\tvyrNrziPKCB: [12, 38, 18, 14, 15]\n",
      "\tVvot9Ly1tCj: [24, 51, 31, 41, 35]\n",
      "\tb8cTxDM8gDG: [33, 12, 13, 20, 11]\n",
      "\n",
      "table:\n",
      "\tvyrNrziPKCB: [22, 22, 54, 57, 12]\n",
      "\tb8cTxDM8gDG: [17, 9, 16, 25, 14]\n",
      "\tD7N2EKCX4Sj: [6, 31, 24, 28, 17]\n",
      "\tVvot9Ly1tCj: [23, 41, 14, 34, 32]\n",
      "\tgTV8FGcVJC9: [16, 13, 15, 18, 15]\n",
      "\n",
      "cushion:\n",
      "\tb8cTxDM8gDG: [7, 14, 22, 12, 8]\n",
      "\tVvot9Ly1tCj: [36, 32, 47, 36, 46]\n",
      "\tvyrNrziPKCB: [62, 18, 21, 42, 54]\n",
      "\tgTV8FGcVJC9: [10, 11, 27, 7, 13]\n",
      "\tD7N2EKCX4Sj: [21, 12, 14, 14, 20]\n",
      "\n",
      "cabinet:\n",
      "\tgTV8FGcVJC9: [13, 13, 18, 47, 13]\n",
      "\tvyrNrziPKCB: [30, 22, 22, 40, 47]\n",
      "\tb8cTxDM8gDG: [10, 14, 21, 24, 16]\n",
      "\tVvot9Ly1tCj: [9, 35, 42, 39, 28]\n",
      "\tD7N2EKCX4Sj: [21, 34, 14, 40, 10]\n",
      "\n",
      "plant:\n",
      "\tgTV8FGcVJC9: [6, 13, 42, 6, 41]\n",
      "\tVvot9Ly1tCj: [16, 47, 23, 11, 26]\n",
      "\tb8cTxDM8gDG: [6, 8, 13, 11, 8]\n",
      "\tvyrNrziPKCB: [36, 21, 59, 34, 34]\n",
      "\tD7N2EKCX4Sj: [27, 31, 37, 26, 47]\n",
      "\n",
      "gTV8FGcVJC9\n",
      "\tchair: [28, 10, 46, 9, 6]\n",
      "\tpicture: [10, 12, 16, 20, 17]\n",
      "\ttable: [16, 13, 15, 18, 15]\n",
      "\tcushion: [10, 11, 27, 7, 13]\n",
      "\tcabinet: [13, 13, 18, 47, 13]\n",
      "\tplant: [6, 13, 42, 6, 41]\n",
      "\n",
      "b8cTxDM8gDG\n",
      "\tchair: [15, 19, 26, 20, 8]\n",
      "\tpicture: [33, 12, 13, 20, 11]\n",
      "\ttable: [17, 9, 16, 25, 14]\n",
      "\tcushion: [7, 14, 22, 12, 8]\n",
      "\tcabinet: [10, 14, 21, 24, 16]\n",
      "\tplant: [6, 8, 13, 11, 8]\n",
      "\n",
      "D7N2EKCX4Sj\n",
      "\tchair: [19, 36, 12, 40, 33]\n",
      "\tpicture: [46, 31, 53, 39, 43]\n",
      "\ttable: [6, 31, 24, 28, 17]\n",
      "\tcushion: [21, 12, 14, 14, 20]\n",
      "\tcabinet: [21, 34, 14, 40, 10]\n",
      "\tplant: [27, 31, 37, 26, 47]\n",
      "\n",
      "Vvot9Ly1tCj\n",
      "\tchair: [18, 21, 23, 16, 27]\n",
      "\tpicture: [24, 51, 31, 41, 35]\n",
      "\ttable: [23, 41, 14, 34, 32]\n",
      "\tcushion: [36, 32, 47, 36, 46]\n",
      "\tcabinet: [9, 35, 42, 39, 28]\n",
      "\tplant: [16, 47, 23, 11, 26]\n",
      "\n",
      "vyrNrziPKCB\n",
      "\tchair: [20, 24, 10, 14, 28]\n",
      "\tpicture: [12, 38, 18, 14, 15]\n",
      "\ttable: [22, 22, 54, 57, 12]\n",
      "\tcushion: [62, 18, 21, 42, 54]\n",
      "\tcabinet: [30, 22, 22, 40, 47]\n",
      "\tplant: [36, 21, 59, 34, 34]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify file name\n",
    "analysis_trajs_filename = \"cats_scenes_trajs_C_6_M_5_N_5__2023_06_01_10_41.bz2\"\n",
    "\n",
    "# Read the filtred trajectories data\n",
    "## Default format is {cat -> { scenes -> traj: []}}\n",
    "with open(analysis_trajs_filename, \"rb\") as f:\n",
    "    cats_scenes_trajs_dict = cpkl.load(f)\n",
    "\n",
    "## Compute the equivalent scenes cat trajs format\n",
    "## {scenes -> { cat -> trajs: []}}\n",
    "scenes_cats_trajs_dict = {}\n",
    "for cat, cat_scenes_trajs in cats_scenes_trajs_dict.items():\n",
    "    for scene, scenes_trajs in cat_scenes_trajs.items():\n",
    "        if scene not in scenes_cats_trajs_dict.keys():\n",
    "            scenes_cats_trajs_dict[scene] = {}\n",
    "        \n",
    "        scenes_cats_trajs_dict[scene][cat] = scenes_trajs\n",
    "\n",
    "# Generic: load the dataset statistics\n",
    "## Compute action coefficient for CEL of BC\n",
    "dataset_stats_filepath = f\"{args.dataset_path}/dataset_statistics.bz2\"\n",
    "# Override dataset statistics if the file already exists\n",
    "if os.path.exists(dataset_stats_filepath):\n",
    "    with open(dataset_stats_filepath, \"rb\") as f:\n",
    "        dataset_statistics = cpkl.load(f)\n",
    "\n",
    "# Extract some global metadata\n",
    "# TARGET_SCENE_LIST = list(cats_scenes_trajs_dict[list(cats_scenes_trajs_dict.keys())[0]].keys())\n",
    "TARGET_SCENE_LIST = list(dataset_statistics[\"scene_counts\"].keys())\n",
    "TARGET_SCENE_DICT = {scene: i for i, scene in enumerate(TARGET_SCENE_LIST)}\n",
    "TARGET_CATEGORY_LIST = list(cats_scenes_trajs_dict.keys())\n",
    "TARGET_CATEGORY_DICT = {cat: i for i, cat in enumerate(TARGET_CATEGORY_LIST)}\n",
    "\n",
    "from soundspaces.mp3d_utils import CATEGORY_INDEX_MAPPING\n",
    "def get_category_name(idx):\n",
    "    assert idx >= 0 and idx <=20, f\"Invalid category index number: {idx}\"\n",
    "\n",
    "    for catname, catidx in CATEGORY_INDEX_MAPPING.items():\n",
    "        if catidx == idx:\n",
    "            return catname\n",
    "\n",
    "def get_sceneid_by_idx(scene_idx):\n",
    "    for k, v in TARGET_SCENE_DICT.items():\n",
    "        if v == scene_idx:\n",
    "            return k\n",
    "\n",
    "C = len(TARGET_CATEGORY_LIST) # C: total number of categories\n",
    "M = len(TARGET_SCENE_LIST) # M: total number of rooms, assuming all categories has N trajs for a same set of scenes.\n",
    "\n",
    "print(f\"# of categories C: {C} | # of scenes: {M}\")\n",
    "print(f\"TARGET_CATEGORY_DICT: {TARGET_CATEGORY_DICT}\")\n",
    "print(f\"TARGET_SCENE_DICT: {TARGET_SCENE_DICT}\")\n",
    "print(\"\")\n",
    "\n",
    "# for catname, cat_scenes_trajs in cats_scenes_trajs_dict.items():\n",
    "#     print(f\"Cat: {catname}; Scenes: {[k for k in cat_scenes_trajs.keys()]}\")\n",
    "\n",
    "# Basic check of the scene -> categories fileted trajectories\n",
    "# for scene, scenes_cat_trajs in scenes_cats_trajs_dict.items():\n",
    "#     print(f\"Scene: {scene}; Cats: {[k for k in scenes_cat_trajs.keys()]}\")\n",
    "\n",
    "# More detailed breakdown of the trajectories per categories then scenes\n",
    "for catname, cat_scenes_trajs in cats_scenes_trajs_dict.items():\n",
    "    print(f\"{catname}:\")\n",
    "    for scene, scene_trajs in cat_scenes_trajs.items():\n",
    "        traj_lengths = [len(traj_data[\"edd\"][\"done_list\"]) for traj_data in scene_trajs]\n",
    "        print(f\"\\t{scene}: {traj_lengths}\")\n",
    "    print(\"\")\n",
    "\n",
    "# More detailed breakdown of the trajectories per categories then scenes\n",
    "for scene, scene_cats_trajs in scenes_cats_trajs_dict.items():\n",
    "    print(f\"{scene}\")\n",
    "    for cat, cat_trajs in scene_cats_trajs.items():\n",
    "        traj_lengths = [len(traj_data[\"edd\"][\"done_list\"]) for traj_data in cat_trajs]\n",
    "        print(f\"\\t{cat}: {traj_lengths}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers to extract traj. data based on \"category\", \"scene\", etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region: Categories -> Scenes\n",
    "## cats_scenes_trajs_dict: dictionary structured as: {category: {scene: [traj_data]}}\n",
    "# TODO: add support for the device in case tensors are returned\n",
    "def get_traj_data_by_category_scene_trajIdx(trajs_dicts, category, scene, trajIdx=0, tensorize=False, device=\"cpu\"):\n",
    "    # Get a single trajectory specified by idx, for a specificed category and scene\n",
    "    # TODO: maybe fix the \"depth\" dimension here directly ?\n",
    "    obs_list_dict = trajs_dicts[category][scene][trajIdx][\"edd\"][\"obs_list\"]\n",
    "    done_list = trajs_dicts[category][scene][trajIdx][\"edd\"][\"done_list\"]\n",
    "\n",
    "    obs_dict_list = []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    T = len(obs_list_dict[\"rgb\"])\n",
    "    for t in range(T):\n",
    "        obs_dict_list.append({k: v[t] for k, v in obs_list_dict.items()})\n",
    "        target_scene_idx_list.append(TARGET_SCENE_DICT[scene])\n",
    "        target_category_idx_list.append(CATEGORY_INDEX_MAPPING[category])\n",
    "\n",
    "    # Tensorize if required\n",
    "    if tensorize:\n",
    "        done_list__th = []\n",
    "        obs_dict_list__th = []\n",
    "\n",
    "        for t, (obs_dict, done) in enumerate(zip(obs_dict_list, done_list)):\n",
    "            # done_list__th.append(th.Tensor(np.array([done])[None, :]))\n",
    "            done_list__th.append(th.Tensor(np.array([done])).to(device)) # TODO: make sure that the deprecation warning stops showing up. Or always stay on current Torch version.\n",
    "            tmp_dict = {}\n",
    "            for k, v in obs_dict.items():\n",
    "                if k == \"depth\":\n",
    "                    v = np.array(v)[:, :, None] # From (H, W) -> (H, W, 1)\n",
    "                tmp_dict[k] = th.Tensor(v)[None, :].to(device)\n",
    "            \n",
    "            obs_dict_list__th.append(tmp_dict)\n",
    "        \n",
    "        return obs_dict_list__th, done_list__th, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "def get_traj_data_by_category_scene(trajs_dicts, category, scene, max_scenes=0, tensorize=False, device=\"cpu\"):\n",
    "    # Get all trajectories for a specific category and scene\n",
    "    obs_dict_list, done_list = [], []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    N_SCENES = len(trajs_dicts[category][scene])\n",
    "    res_n_scenes = N_SCENES if max_scenes <= 0 else max_scenes\n",
    "\n",
    "    for i in range(N_SCENES):\n",
    "        traj_obs_dict_list, traj_done_list, target_scene_idxes, target_category_idxes = \\\n",
    "            get_traj_data_by_category_scene_trajIdx(trajs_dicts, category, scene, i, tensorize=tensorize, device=device)\n",
    "\n",
    "        obs_dict_list.extend(traj_obs_dict_list)\n",
    "        done_list.extend(traj_done_list)\n",
    "        target_scene_idx_list.extend(target_scene_idxes)\n",
    "        target_category_idx_list.extend(target_category_idxes)\n",
    "\n",
    "        traj_length = len(traj_done_list)\n",
    "        # print(f\"Selected traj of length: {traj_length}\")\n",
    "        if i >= res_n_scenes - 1:\n",
    "            break\n",
    "\n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "def get_traj_data_by_category(trajs_dicts, category, max_scenes=0, tensorize=False, device=\"cpu\"):\n",
    "    # Get all trajectories for a specific category, across all scenes and all trajectories\n",
    "    obs_dict_list, done_list =[], []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    for scene in trajs_dicts[category].keys():\n",
    "        scene_obs_dict_list, scene_done_list, target_scene_idxes, target_category_idxes = \\\n",
    "            get_traj_data_by_category_scene(trajs_dicts, category, scene, max_scenes=max_scenes, tensorize=tensorize, device=device)\n",
    "\n",
    "        obs_dict_list.extend(scene_obs_dict_list)\n",
    "        done_list.extend(scene_done_list)\n",
    "        target_scene_idx_list.extend(target_scene_idxes)\n",
    "        target_category_idx_list.extend(target_category_idxes)\n",
    "    \n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "def get_all_traj_data_by_category(trajs_dicts, tensorize=False, device=\"cpu\"):\n",
    "    # Get all trajectories for a specific category, across all scenes and all trajectories\n",
    "    obs_dict_list, done_list =[], []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    for cat in trajs_dicts.keys():\n",
    "        cat_scene_obs_dict_list, cat_scene_done_list, cat_target_scene_idxes, cat_target_category_idxes = \\\n",
    "            get_traj_data_by_category(trajs_dicts, cat, tensorize=tensorize, device=device)\n",
    "\n",
    "        obs_dict_list.extend(cat_scene_obs_dict_list)\n",
    "        done_list.extend(cat_scene_done_list)\n",
    "        target_scene_idx_list.extend(cat_target_scene_idxes)\n",
    "        target_category_idx_list.extend(cat_target_category_idxes)\n",
    "    \n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "# endregion: Categories -> Scenes\n",
    "\n",
    "\n",
    "# region: Scenes -> Categories\n",
    "# TODO: add \"return\" for target categories and scenes label\n",
    "## scenes_cats_trajs_dict: dictionary structured as: {scene: {category: [traj-data]}}\n",
    "def get_traj_data_by_scene_category_trajIdx(trajs_dicts, scene, category, trajIdx=0, tensorize=False, device=\"cpu\"):\n",
    "    # Get a single trajectory specified by idx, for a specificed category and scene\n",
    "    # TODO: maybe fix the \"depth\" dimension here directly ?\n",
    "    obs_list_dict = trajs_dicts[scene][category][trajIdx][\"edd\"][\"obs_list\"]\n",
    "    done_list = trajs_dicts[scene][category][trajIdx][\"edd\"][\"done_list\"]\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    obs_dict_list = []\n",
    "    T = len(obs_list_dict[\"rgb\"])\n",
    "    for t in range(T):\n",
    "        obs_dict_list.append({k: v[t] for k, v in obs_list_dict.items()})\n",
    "        target_scene_idx_list.append(TARGET_SCENE_DICT[scene])\n",
    "        target_category_idx_list.append(CATEGORY_INDEX_MAPPING[category])\n",
    "\n",
    "    # Tensorize if required\n",
    "    if tensorize:\n",
    "        done_list__th = []\n",
    "        obs_dict_list__th = []\n",
    "\n",
    "        for t, (obs_dict, done) in enumerate(zip(obs_dict_list, done_list)):\n",
    "            # done_list__th.append(th.Tensor(np.array([done])[None, :]))\n",
    "            done_list__th.append(th.Tensor(np.array([done])).to(device)) # TODO: make sure that the deprecation warning stops showing up. Or always stay on current Torch version.\n",
    "            tmp_dict = {}\n",
    "            for k, v in obs_dict.items():\n",
    "                if k == \"depth\":\n",
    "                    v = np.array(v)[:, :, None] # From (H, W) -> (H, W, 1)\n",
    "                tmp_dict[k] = th.Tensor(v)[None, :].to(device)\n",
    "            \n",
    "            obs_dict_list__th.append(tmp_dict)\n",
    "        \n",
    "        return obs_dict_list__th, done_list__th, target_scene_idx_list, target_category_idx_list\n",
    "        \n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "def get_traj_data_by_scene_category(trajs_dicts, scene, category, tensorize=False, device=\"cpu\"):\n",
    "    # Get all trajectories for a specific category and scene\n",
    "    obs_dict_list, done_list = [], []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    for i in range(len(trajs_dicts[scene][category])):\n",
    "        traj_obs_dict_list, traj_done_list, target_scene_idxes, target_category_idxes = \\\n",
    "            get_traj_data_by_scene_category_trajIdx(trajs_dicts, scene, category, i, tensorize=tensorize, device=device)\n",
    "\n",
    "        obs_dict_list.extend(traj_obs_dict_list)\n",
    "        done_list.extend(traj_done_list)\n",
    "        target_scene_idx_list.extend(target_scene_idxes)\n",
    "        target_category_idx_list.extend(target_category_idxes)\n",
    "\n",
    "        traj_length = len(traj_done_list)\n",
    "        # print(f\"Selected traj of length: {traj_length}\")\n",
    "\n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "def get_traj_data_by_scene(trajs_dicts, scene, tensorize=False, device=\"cpu\"):\n",
    "    # Get all trajectories for a specific category, across all scenes and all trajectories\n",
    "    obs_dict_list, done_list =[], []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "    \n",
    "    for cat in trajs_dicts[scene].keys():\n",
    "        cat_obs_dict_list, cat_done_list, target_scene_idxes, target_category_idxes = \\\n",
    "            get_traj_data_by_scene_category(trajs_dicts, scene, cat, tensorize=tensorize, device=device)\n",
    "\n",
    "        obs_dict_list.extend(cat_obs_dict_list)\n",
    "        done_list.extend(cat_done_list)\n",
    "        target_scene_idx_list.extend(target_scene_idxes)\n",
    "        target_category_idx_list.extend(target_category_idxes)\n",
    "    \n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "\n",
    "def get_all_traj_data_by_scene(trajs_dicts, tensorize=False, device=\"cpu\"):\n",
    "    # Get all trajectories for a specific category, across all scenes and all trajectories\n",
    "    obs_dict_list, done_list =[], []\n",
    "    target_scene_idx_list, target_category_idx_list = [], []\n",
    "\n",
    "    for scene in trajs_dicts.keys():\n",
    "        # Too rushing / lazy to change the names of the temporary list of obs\n",
    "        cat_scene_obs_dict_list, cat_scene_done_list, cat_target_scene_idxes, cat_target_category_idxes = \\\n",
    "            get_traj_data_by_category(trajs_dicts, scene, tensorize=tensorize, device=device)\n",
    "\n",
    "        obs_dict_list.extend(cat_scene_obs_dict_list)\n",
    "        done_list.extend(cat_scene_done_list)\n",
    "        target_scene_idx_list.extend(cat_target_scene_idxes)\n",
    "        target_category_idx_list.extend(cat_target_category_idxes)\n",
    "    \n",
    "    return obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list\n",
    "# endregion: Scenes -> Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GRU Agent: 'ppo_bc__sweep_gru_512'\n",
      "  Feat size: 512\n",
      "  GW size: 512\n",
      "Loaded GW Agent: 'ppo_bc__sweep_gw_64'\n",
      "  Feat size: 512\n",
      "  GW size: 64\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained agent\n",
    "MODEL_VARIANTS_TO_STATEDICT_PATH = {\n",
    "    ## GRU\n",
    "    # region: SAVI BC GRUv3 variants: rec enc gw3 detach\n",
    "    \"ppo_bc__sweep_gru_512\": {\n",
    "        \"pretty_name\": \"GRU 1 (Sweep)\",\n",
    "        \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc-revised-sweep/\"\n",
    "            \"ppo_bc_seed_42__2024_02_05_18_30_00_569723.musashi\"\n",
    "            # \"/models/ppo_agent.19995001.ckpt.pth\",\n",
    "            \"/models/ppo_agent.10000500.ckpt.pth\",\n",
    "        # TODO: prending probes\n",
    "        \"probe_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc-probing/\"\n",
    "            \"ppo_bc__savi_ss1_rgb_spectro__gruv3__gw_detach__usenull__grulynrm__entcoef_0.2__no_cew__n_mb_50__prb_dpth_2_seed_111__2023_11_16_16_08_52_068321.conan\"\n",
    "    },\n",
    "    # endregion: SAVI BC GRUv3 variants: rec enc gw3 detach\n",
    "\n",
    "    ## GWTv3 H=512\n",
    "    # region: SAVI BC GWTv3 variants: rec enc gw3 detach; CA uses null\n",
    "    \"ppo_bc__sweep_gw_64\": {\n",
    "        \"pretty_name\": \"GW 1 (Sweep)\",\n",
    "        \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc-revised-sweep/\"\n",
    "            \"ppo_bc_seed_42__2024_01_23_15_44_57_777702.musashi\"\n",
    "            \"/models/ppo_agent.10000500.ckpt.pth\",\n",
    "        # TODO: prending probes\n",
    "        \"probe_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc-probing/\"\n",
    "            \"ppo_bc__savi_ss1_rgb_spectro__gwtv3__gw_detach__usenull__grulynrm__entcoef_0.2__no_cew__n_mb_50__prb_dpth_2_seed_111__2023_11_14_18_38_49_687853.musashi\"\n",
    "    },\n",
    "    # endregion: SAVI BC GWTv3 variants: rec enc gw3 detach; CA uses null\n",
    "}\n",
    "\n",
    "# dev = th.device(\"cpu\")\n",
    "dev = th.device(\"cuda\")\n",
    "\n",
    "# 'variant named' indexed 'torch agent'\n",
    "MODEL_VARIANTS_TO_AGENTMODEL = {}\n",
    "\n",
    "for k, v in MODEL_VARIANTS_TO_STATEDICT_PATH.items():\n",
    "    args_copy = copy.copy(args)\n",
    "    tmp_args = copy.copy(args)\n",
    "\n",
    "    # Detected the  gw_size\n",
    "    for gw_size in [32, 64, 128, 256, 512]:\n",
    "        if k.__contains__(f\"{gw_size}\"):\n",
    "            tmp_args.gw_size = gw_size\n",
    "            break\n",
    "\n",
    "    # Override args depending on the model in use\n",
    "    if k.__contains__(\"gru\"):\n",
    "        print(f\"Loaded GRU Agent: '{k}'\")\n",
    "        print(f\"  Feat size: {tmp_args.hidden_size}\")\n",
    "        print(f\"  GW size: {tmp_args.gw_size}\")\n",
    "\n",
    "        agent = GRU_Actor(single_observation_space, single_action_space, tmp_args,\n",
    "            analysis_layers=models.GWTAGENT_DEFAULT_ANALYSIS_LAYER_NAMES)\n",
    "        # print(agent)\n",
    "    elif k.__contains__(\"gw\"):\n",
    "        print(f\"Loaded GW Agent: '{k}'\")\n",
    "        print(f\"  Feat size: {tmp_args.hidden_size}\")\n",
    "        print(f\"  GW size: {tmp_args.gw_size}\")\n",
    "\n",
    "        agent = GW_Actor(single_observation_space, single_action_space, tmp_args,\n",
    "            analysis_layers=models.GWTAGENT_DEFAULT_ANALYSIS_LAYER_NAMES + [\"state_encoder.ca.mha\"])\n",
    "        # print(agent)\n",
    "\n",
    "    agent.eval()\n",
    "    # Load the model weights\n",
    "    # TODO: add map location device to use CPU only ?\n",
    "    if v[\"state_dict_path\"] != \"\":\n",
    "        agent_state_dict = th.load(v[\"state_dict_path\"], map_location=dev)\n",
    "        agent.load_state_dict(agent_state_dict)\n",
    "    agent = agent.to(dev)\n",
    "\n",
    "    MODEL_VARIANTS_TO_AGENTMODEL[k] = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GW_Actor(\n",
      "  (visual_encoder): RecurrentVisualEncoder(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Flatten()\n",
      "    )\n",
      "    (rnn): GRUCell(\n",
      "      (_layer): Linear(in_features=2880, out_features=1536, bias=True)\n",
      "      (_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (audio_encoder): RecurrentAudioEncoder(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(2, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Flatten()\n",
      "    )\n",
      "    (rnn): GRUCell(\n",
      "      (_layer): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "      (_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (state_encoder): GWStateEncoder(\n",
      "    (ca): CrossAttention(\n",
      "      (proj_vis): Linear(in_features=512, out_features=64, bias=False)\n",
      "      (proj_aud): Linear(in_features=512, out_features=64, bias=False)\n",
      "      (ln_q): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_k): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_v): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (rnn): GRUCell(\n",
      "      (_layer): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (action_distribution): CategoricalNet(\n",
      "    (linear): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process agents features, retain the relevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the subset of cached features that will be analyzed\n",
    "# CACHE_DIRNAME = \"cats_scenes_trajs_C_6_M_5_N_5__2023_06_01_10_41__ablation_agents_features__cache\"\n",
    "# FEATURES_CACHE_DIRNAME = f\"cached_data/features/cats_scenes_trajs_C_6_M_5_N_5__2023_06_01_10_41\"\n",
    "\n",
    "CATEGORIES_OF_INTEREST = [\n",
    "    \"chair\",\n",
    "    # \"picture\",\n",
    "    # \"cabinet\",\n",
    "    # \"plant\",\n",
    "    # \"cushion\",\n",
    "    # \"table\"\n",
    "]\n",
    "SCENES_OF_INTEREST = [\n",
    "    \"gTV8FGcVJC9\",\n",
    "    # \"D7N2EKCX4Sj\",\n",
    "    # \"b8cTxDM8gDG\",\n",
    "    # \"Vvot9Ly1tCj\",\n",
    "    # \"vyrNrziPKCB\"\n",
    "] # ['gTV8FGcVJC9', 'b8cTxDM8gDG', 'D7N2EKCX4Sj', 'Vvot9Ly1tCj', 'vyrNrziPKCB']\n",
    "TRAJ_INDICES = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4\n",
    "] # Only select trajectories in indices of interest\n",
    "ABLATIONS_OF_INTEREST = [\"default\", \"vision\", \"audio\", \"zeros\"]\n",
    "LAYERS_OF_INTEREST = [\n",
    "    \"visual_encoder.rnn\", \"audio_encoder.rnn\",\n",
    "    \"state_encoder\",\n",
    "    \"state_encoder.ca.mha\", \"state_encoder.ca\"\n",
    "]\n",
    "AGENTS_OF_INTEREST = [\n",
    "    ## GRU H=512\n",
    "    \"ppo_bc__sweep_gru_512\",\n",
    "    \n",
    "    ## GWTv3 H=64\n",
    "    \"ppo_bc__sweep_gw_64\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect visual, audio, state features for all agent variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair:\n",
      "  gTV8FGcVJC9: [28, 10, 46, 9, 6]\n",
      "    Traj 0: Length: 28\n",
      "        Model variant: ppo_bc__sweep_gru_512\n",
      "          Detected GW size: 512\n",
      "        Model variant: ppo_bc__sweep_gw_64\n",
      "          Detected GW size: 64\n",
      "    Traj 1: Length: 10\n",
      "        Model variant: ppo_bc__sweep_gru_512\n",
      "          Detected GW size: 512\n",
      "        Model variant: ppo_bc__sweep_gw_64\n",
      "          Detected GW size: 64\n",
      "    Traj 2: Length: 46\n",
      "        Model variant: ppo_bc__sweep_gru_512\n",
      "          Detected GW size: 512\n",
      "        Model variant: ppo_bc__sweep_gw_64\n",
      "          Detected GW size: 64\n",
      "    Traj 3: Length: 9\n",
      "        Model variant: ppo_bc__sweep_gru_512\n",
      "          Detected GW size: 512\n",
      "        Model variant: ppo_bc__sweep_gw_64\n",
      "          Detected GW size: 64\n",
      "    Traj 4: Length: 6\n",
      "        Model variant: ppo_bc__sweep_gru_512\n",
      "          Detected GW size: 512\n",
      "        Model variant: ppo_bc__sweep_gw_64\n",
      "          Detected GW size: 64\n"
     ]
    }
   ],
   "source": [
    "# Collect the relevant features from the PGWT variants\n",
    "CAT_SCENE_TRAJS_FEATURES = {}\n",
    "\n",
    "## Helper for cleaning up and preparing the recorded intermediate features\n",
    "def process_analysis_feats_raw__occ_variant(raw_dict):\n",
    "    result_dict = {}\n",
    "\n",
    "    for k, v in raw_dict.items():\n",
    "        if isinstance(v[0], th.Tensor):\n",
    "            new_v = th.stack(v, dim=0).cpu()\n",
    "        elif isinstance(v[0], tuple):\n",
    "            new_v = None # TODO\n",
    "            n_elements = len(v[0])\n",
    "            elements = [[] for _ in range(n_elements)]\n",
    "            for j in range(n_elements):\n",
    "                for i in range(len(v)):\n",
    "                    elements[j].append(v[i][j])\n",
    "            \n",
    "            new_v = [th.stack(vv, dim=0).cpu() for vv in elements]\n",
    "        else:\n",
    "            raise Exception(f\"Unhandled type: {v[0].__class__}\")\n",
    "\n",
    "        result_dict[k] = new_v\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "for catname, cat_scenes_trajs in cats_scenes_trajs_dict.items():\n",
    "    if catname not in CATEGORIES_OF_INTEREST:\n",
    "        continue\n",
    "\n",
    "    if catname not in CAT_SCENE_TRAJS_FEATURES.keys():\n",
    "        CAT_SCENE_TRAJS_FEATURES[catname] = {}\n",
    "    print(f\"{catname}:\")\n",
    "\n",
    "    for scene, scene_trajs in cat_scenes_trajs.items():\n",
    "        if scene not in SCENES_OF_INTEREST:\n",
    "            continue\n",
    "\n",
    "        traj_lengths = [len(traj_data[\"edd\"][\"done_list\"]) for traj_data in scene_trajs]\n",
    "        print(f\"  {scene}: {traj_lengths}\")\n",
    "\n",
    "        if scene not in CAT_SCENE_TRAJS_FEATURES[catname].keys():\n",
    "            CAT_SCENE_TRAJS_FEATURES[catname][scene] = {}\n",
    "        \n",
    "        for traj_idx, traj_data in enumerate(scene_trajs):\n",
    "            if traj_idx not in TRAJ_INDICES:\n",
    "                continue\n",
    "            \n",
    "            if traj_idx not in CAT_SCENE_TRAJS_FEATURES[catname][scene].keys():\n",
    "                CAT_SCENE_TRAJS_FEATURES[catname][scene][traj_idx] = {}\n",
    "\n",
    "            # Load the data, perform ablations if necessary\n",
    "            obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list = \\\n",
    "                get_traj_data_by_category_scene_trajIdx(cats_scenes_trajs_dict, catname, scene, traj_idx, tensorize=True, device=dev)\n",
    "            \n",
    "            ep_length = len(obs_dict_list)\n",
    "            print(f\"    Traj {traj_idx}: Length: {ep_length}\")\n",
    "\n",
    "            # TODO: verify\n",
    "            CAT_SCENE_TRAJS_FEATURES[catname][scene][traj_idx][\"target_scene_idx_list\"] = target_scene_idx_list\n",
    "            CAT_SCENE_TRAJS_FEATURES[catname][scene][traj_idx][\"target_category_idx_list\"] = target_category_idx_list\n",
    "\n",
    "            for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "                if agent_variant not in AGENTS_OF_INTEREST:\n",
    "                    continue\n",
    "                print(f\"        Model variant: {agent_variant}\")\n",
    "\n",
    "                agent_raw_features = {}\n",
    "                # Init the agent_rnn_state depending on the variant used\n",
    "                H = 512\n",
    "                for gw_size in [32, 64, 128, 256]:\n",
    "                    if agent_variant.__contains__(f\"{gw_size}\"):\n",
    "                        H = gw_size\n",
    "\n",
    "                print(f\"          Detected GW size: {H}\")\n",
    "\n",
    "                agent_rnn_state = th.zeros((1, H), device=dev)\n",
    "                modality_features = {\n",
    "                    \"audio\": agent_rnn_state.new_zeros([1, 512]),\n",
    "                    \"visual\": agent_rnn_state.new_zeros([1, 512])\n",
    "                }\n",
    "                \n",
    "                for t, (obs_th, done_th) in enumerate(zip(obs_dict_list, done_list)):\n",
    "                    \n",
    "                    masks = 1. - done_th[:, None]\n",
    "                    with th.no_grad():\n",
    "                        _, _, _, _, \\\n",
    "                        _, agent_rnn_state, modality_features = \\\n",
    "                            agent_model.act(obs_th, agent_rnn_state, masks=masks, \n",
    "                                modality_features=modality_features)\n",
    "\n",
    "                    # Collecting intermediate layers results\n",
    "                    for k, v in agent_model._features.items():\n",
    "                        if k not in LAYERS_OF_INTEREST:\n",
    "                            continue # Skip irrelevant layers\n",
    "                        if k not in list(agent_raw_features.keys()):\n",
    "                            agent_raw_features[k] = []\n",
    "                        agent_raw_features[k].append((v[0].cpu(), v[1].cpu()) if isinstance(v, tuple) else v.cpu())\n",
    "                \n",
    "                agent_layers_features = process_analysis_feats_raw__occ_variant(agent_raw_features)\n",
    "                del agent_raw_features\n",
    "\n",
    "                CAT_SCENE_TRAJS_FEATURES[catname][scene][traj_idx][agent_variant] = \\\n",
    "                    agent_layers_features\n",
    "                \n",
    "                # Caching features: TODO\n",
    "\n",
    "            del obs_dict_list, done_list, target_scene_idx_list, target_category_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_encoder', 'visual_encoder.rnn', 'audio_encoder.rnn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(CAT_SCENE_TRAJS_FEATURES.keys())\n",
    "list(CAT_SCENE_TRAJS_FEATURES[\"chair\"][\"gTV8FGcVJC9\"][0][\"ppo_bc__sweep_gru_512\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAT_SCENE_TRAJS_FEATURES[\"chair\"][\"gTV8FGcVJC9\"][0][\"ppo_bc__sweep_gru_512\"][\"state_encoder\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAT_SCENE_TRAJS_FEATURES[\"chair\"][\"gTV8FGcVJC9\"][0][\"ppo_bc__sweep_gru_512\"][\"visual_encoder.rnn\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAT_SCENE_TRAJS_FEATURES[\"chair\"][\"gTV8FGcVJC9\"][0][\"ppo_bc__sweep_gru_512\"][\"audio_encoder.rnn\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair | gTV8FGcVJC9 | 0 | Length: 28\n",
      "chair | gTV8FGcVJC9 | 1 | Length: 10\n",
      "chair | gTV8FGcVJC9 | 2 | Length: 46\n",
      "chair | gTV8FGcVJC9 | 3 | Length: 9\n",
      "chair | gTV8FGcVJC9 | 4 | Length: 6\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "\n",
    "colpal = sns.color_palette(\"tab10\")\n",
    "MOD_AXIS_TO_NAME = {0: \"Audio\", 1: \"Vision\", 2: r\"Prev. state $h_{t-1}$\", 3: \"Null\"}\n",
    "MOD_AXIS_TO_COLOR = { 0: colpal[2], 1: colpal[1], 2: colpal[0], 3: colpal[7]}\n",
    "\n",
    "# N_TRAJS = 4\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "TRAJS_OF_INTEREST_DICT = [\n",
    "  *[{\"category\": \"chair\", \"scene\": \"gTV8FGcVJC9\", \"traj_idx\": i} for i in range(5)]\n",
    "  # *[{\"category\": \"picture\", \"scene\": \"gTV8FGcVJC9\", \"traj_idx\": i} for i in range(5)] # No sound cutoffs \n",
    "  # *[{\"category\": \"cushion\", \"scene\": \"gTV8FGcVJC9\", \"traj_idx\": i} for i in range(5)]\n",
    "  # *[{\"category\": \"plant\", \"scene\": \"gTV8FGcVJC9\", \"traj_idx\": i} for i in range(5)]\n",
    "  # *[{\"category\": \"cabinet\", \"scene\": \"gTV8FGcVJC9\", \"traj_idx\": i} for i in range(5)]\n",
    "  # *[{\"category\": \"table\", \"scene\": \"gTV8FGcVJC9\", \"traj_idx\": i} for i in range(5)]\n",
    "]\n",
    "N_TRAJS = len(TRAJS_OF_INTEREST_DICT)\n",
    "fig, axes = plt.subplots(3, N_TRAJS, figsize=(N_TRAJS * 1.5, 3 * 1.25))\n",
    "\n",
    "\n",
    "AGENT_GROUPS_OF_INTEREST = {\n",
    "\n",
    "  \"GW H=64\": {\n",
    "    42: \"ppo_bc__sweep_gw_64\",\n",
    "  },\n",
    "  # },\n",
    "\n",
    "}\n",
    "\n",
    "# TODO: functionalize\n",
    "for ridx in range(3):\n",
    "  for cidx in range(N_TRAJS):\n",
    "    # Tweak the ylimit for all plots\n",
    "    axes[ridx, cidx].set_ylim(-0.05, 1.05)\n",
    "\n",
    "    if ridx == 0:\n",
    "      axes[ridx, cidx].xaxis.set_label_position(\"top\")\n",
    "      if cidx == 2:\n",
    "        axes[ridx, cidx].set_xlabel(f\"Trajectory\\n{cidx+1}\", fontsize=10)\n",
    "      else:\n",
    "        axes[ridx, cidx].set_xlabel(f\"{cidx+1}\", fontsize=10)\n",
    "    if cidx > 0:\n",
    "      axes[ridx, cidx].yaxis.set_major_locator(ticker.NullLocator())\n",
    "    if ridx < 2:\n",
    "      axes[ridx, cidx].xaxis.set_major_locator(ticker.NullLocator())\n",
    "    else:\n",
    "      axes[ridx, cidx].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "      if cidx == 2:\n",
    "        axes[ridx, cidx].set_xlabel(\"Time steps\", fontsize=9)\n",
    "\n",
    "# Plots labelling\n",
    "# ylabel_fontsize = 10\n",
    "axes[0, 0].set_ylabel(\"Audio\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Query\\n\\nVision\", fontsize=11)\n",
    "axes[2, 0].set_ylabel(\"GW\", fontsize=11)\n",
    "\n",
    "# Create custom legend handles and labels\n",
    "attn_weights_legend_handles = [\n",
    "    Line2D([0], [0], color=colpal[color], label=label)\n",
    "    for color, label in zip(\n",
    "      [0, 1, 2, 7],\n",
    "      [\"GW\", \"Vision\", \"Audio\", \"Null\"]\n",
    "    )\n",
    "]\n",
    "bottom_row_x_MultipleLocator_bases = []\n",
    "\n",
    "# Add legend\n",
    "# plt.legend(handles=probe_acc_legend_handles)\n",
    "\n",
    "# Iterate over traj data, plot if of interest\n",
    "trajint_idx = 0\n",
    "\n",
    "# TODO: rework the plotting to satisfy:\n",
    "# 1. Iterate over the trajectories of interest\n",
    "# 2. Group agent runs by seed, give them a name\n",
    "for trajint_idx, trajint_data in enumerate(TRAJS_OF_INTEREST_DICT):\n",
    "  cat = trajint_data[\"category\"]\n",
    "  scene = trajint_data[\"scene\"]\n",
    "  traj_idx = trajint_data[\"traj_idx\"]\n",
    "\n",
    "  # Load the current trajectory's data\n",
    "  # TODO: could count this during the processing phase somehow ?\n",
    "  obs_dict_list, _, _, _ = \\\n",
    "      get_traj_data_by_category_scene_trajIdx(cats_scenes_trajs_dict, cat, scene, traj_idx, tensorize=False)\n",
    "  \n",
    "  T = len(CAT_SCENE_TRAJS_FEATURES[cat][scene][traj_idx][\"target_category_idx_list\"])\n",
    "\n",
    "  if 0 < T <= 15:\n",
    "    bottom_row_x_MultipleLocator_bases.append(5)\n",
    "  elif T <= 50:\n",
    "    bottom_row_x_MultipleLocator_bases.append(10)\n",
    "  \n",
    "  mute_step = 0\n",
    "  spectrogram_obs_list = np.array([d[\"spectrogram\"] for d in obs_dict_list])\n",
    "  for t in range(T):\n",
    "    if spectrogram_obs_list[t].sum() >= 1e-6:\n",
    "      mute_step += 1\n",
    "    else:\n",
    "      break\n",
    "  mute_step = min(mute_step, T-1)\n",
    "\n",
    "  print(f\"{cat} | {scene} | {traj_idx} | Length: {T}\")\n",
    "\n",
    "  # Plot the probe accuracies\n",
    "  xs = np.arange(T)\n",
    "\n",
    "  ## Shading the non silent region in gray\n",
    "  [axes[i, trajint_idx].fill_between([0, mute_step+1], 0, [1.1, 1.1], color=\"gray\", alpha=0.125) for i in [0, 1, 2]]\n",
    "  \n",
    "  for agent_group, agent_group_data in AGENT_GROUPS_OF_INTEREST.items():\n",
    "    for exp_seed, exp_name in agent_group_data.items():\n",
    "\n",
    "      ## Fix the length of the x axis\n",
    "      [axes[i, trajint_idx].set_xlim([0, T-1]) for i in range(3)]\n",
    "\n",
    "      if exp_name.__contains__(\"gwtv3\"):\n",
    "        # Plot the attention weights for the GWTv3 variant\n",
    "        att_weights = CAT_SCENE_TRAJS_FEATURES[cat][scene][traj_idx][exp_name][\"state_encoder.ca.mha\"][1][:, 0, :, :]\n",
    "\n",
    "        for mod_idx in range(3):\n",
    "          for mod_axis in range(4):\n",
    "            mod_idx_offset = mod_idx-2\n",
    "            axes[mod_idx, trajint_idx].plot(xs, att_weights[:, mod_idx_offset, mod_axis], \n",
    "              linewidth=1.25,\n",
    "              label=MOD_AXIS_TO_NAME[mod_axis],\n",
    "              color=MOD_AXIS_TO_COLOR[mod_axis],\n",
    "              alpha=0.7)\n",
    "\n",
    "# Set legend using dashed lines before doing the dots / scater\n",
    "# axes[0, 0].legend(handles=probe_acc_legend_handles)\n",
    "# axes[2, N_TRAJS-1].legend(handles=attn_weights_legend_handles)\n",
    "# axes[2, 3].legend(handles=attn_weights_legend_handles, ncol=len(attn_weights_legend_handles), bbox_to_anchor=(0,0))\n",
    "for cidx in range(len(TRAJS_OF_INTEREST_DICT)):\n",
    "  axes[2, cidx].xaxis.set_major_locator(MultipleLocator(base=bottom_row_x_MultipleLocator_bases[cidx]))\n",
    "  if cidx == 2:\n",
    "      axes[1, cidx].set_xlabel(\"Time steps\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.08, hspace=0.08)\n",
    "fig.legend(handles=attn_weights_legend_handles, ncol=len(attn_weights_legend_handles), bbox_to_anchor=(0.782, 0.01))\n",
    "# fig.supxlabel(\"Trajectory\", va=\"top\")\n",
    "# fig.supylabel(\"Query\", ha=)\n",
    "# fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# fig.savefig(\"GWAgent_H_64_Seed_111_AttentionWeights.pdf\", bbox_inches=\"tight\")\n",
    "# fig.savefig(\"GWAgent_H_64_Seed_222_AttentionWeights.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# fig.savefig(\"GWAgent_H_512_Seed_111_AttentionWeights.pdf\", bbox_inches=\"tight\")\n",
    "# fig.savefig(\"GWAgent_H_512_Seed_222_AttentionWeights.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
