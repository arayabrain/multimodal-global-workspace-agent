{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# Custom PPO implementation with Soundspaces 2.0\n",
    "# Borrows from \n",
    "## - CleanRL's PPO LSTM: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_lstm.py\n",
    "## - SoundSpaces AudioNav Baselines: https://github.com/facebookresearch/sound-spaces/tree/main/ss_baselines/av_nav\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import tools\n",
    "from configurator import generate_args, get_arg_dict\n",
    "from th_logger import TBXLogger as TBLogger\n",
    "\n",
    "# Env deps: Soundspaces and Habitat\n",
    "from habitat.datasets import make_dataset\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "\n",
    "# Custom ActorCritic agent for PPO\n",
    "from models import ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "# Tensorize current observation, store to rollout data\n",
    "def tensorize_obs_dict(obs, device, observations=None, rollout_step=None):\n",
    "    obs_th = {}\n",
    "    for obs_field, _ in obs[0].items():\n",
    "        v_th = th.Tensor(np.array([step_obs[obs_field] for step_obs in obs], dtype=np.float32)).to(device)\n",
    "        obs_th[obs_field] = v_th\n",
    "        # Special case when doing the rollout, also stores the \n",
    "        if observations is not None:\n",
    "            observations[obs_field][rollout_step] = v_th\n",
    "    \n",
    "    return obs_th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 12:40:55.375565: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Logdir: ./logs/_seed_111__2022_08_05_12_40_55_178162.musashi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Environment config\n",
    "# TODO: Override some of the config elements through arg parse ?\n",
    "env_config = get_config(\n",
    "    config_paths=\"env_configs/audiogoal_rgb.yaml\",\n",
    "    # run_type=\"train\"\n",
    ")\n",
    "\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyepr parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 10_000_000),\n",
    "    \n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 2), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.2), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "\n",
    "    # Logging params\n",
    "    get_arg_dict(\"save-videos\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e4)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\"), # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Experiment logger\n",
    "tblogger = TBLogger(exp_name=args.exp_name, args=args)\n",
    "print(f\"# Logdir: {tblogger.logdir}\")\n",
    "# should_eval = tools.Every(args.eval_every)\n",
    "should_log_sampling_stats = tools.Every(args.log_sampling_stats_every)\n",
    "should_log_training_stats = tools.Every(args.log_training_stats_every)\n",
    "\n",
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "th.cuda.manual_seed_all(args.seed)\n",
    "th.backends.cudnn.deterministic = args.torch_deterministic\n",
    "th.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "# Set device as GPU\n",
    "device = tools.get_device(args)\n",
    "\n",
    "# env_config.TASK_CONFIG.TASK.MEASUREMENTS.append(\"TOP_DOWN_MAP\") # Note: can we add audio sensory info fields here too ?\n",
    "# NOTE: when evaluating, if we use the same dataset split as the training mode, then evaluation will not be fair to the baseline\n",
    "# env_config.TASK_CONFIG.DATASET.SPLIT = config.EVAL.SPLIT\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "env_config.NUM_PROCESSES = 2 # args.num_envs # Corresponds to number of envs, makes script startup faster for debugs\n",
    "## JUPYTER DEBUG\n",
    "env_config.USE_VECENV = False\n",
    "env_config.USE_SYNC_VECENV = False\n",
    "env_config.freeze()\n",
    "# print(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_TASK_CONFIG_PATH: env_configs/base_audiogoal.yaml\n",
      "CHECKPOINT_FOLDER: data/models/output/data\n",
      "CHECKPOINT_INTERVAL: 50\n",
      "CMD_TRAILING_OPTS: []\n",
      "CONTINUOUS: True\n",
      "DEBUG: False\n",
      "DISPLAY_RESOLUTION: 128\n",
      "ENV_NAME: AudioNavRLEnv\n",
      "EVAL:\n",
      "  SPLIT: val\n",
      "  USE_CKPT_CONFIG: True\n",
      "EVAL_CKPT_PATH_DIR: data/models/output/data\n",
      "EXTRA_RGB: False\n",
      "FOLLOW_SHORTEST_PATH: False\n",
      "LOG_FILE: data/models/output/train.log\n",
      "LOG_INTERVAL: 10\n",
      "NUM_PROCESSES: 2\n",
      "NUM_UPDATES: 40000\n",
      "RL:\n",
      "  DISTANCE_REWARD_SCALE: 1.0\n",
      "  PPO:\n",
      "    clip_param: 0.1\n",
      "    entropy_coef: 0.2\n",
      "    eps: 1e-05\n",
      "    gamma: 0.99\n",
      "    hidden_size: 512\n",
      "    lr: 0.00025\n",
      "    max_grad_norm: 0.5\n",
      "    num_mini_batch: 1\n",
      "    num_steps: 150\n",
      "    ppo_epoch: 4\n",
      "    reward_window_size: 50\n",
      "    tau: 0.95\n",
      "    use_gae: True\n",
      "    use_linear_clip_decay: True\n",
      "    use_linear_lr_decay: True\n",
      "    value_loss_coef: 0.5\n",
      "  SLACK_REWARD: -0.01\n",
      "  SUCCESS_REWARD: 10.0\n",
      "  TIME_DIFF: False\n",
      "  WITH_DISTANCE_REWARD: True\n",
      "  WITH_TIME_PENALTY: True\n",
      "SEED: 0\n",
      "SENSORS: ['RGB_SENSOR']\n",
      "SIMULATOR_GPU_ID: 0\n",
      "TASK_CONFIG:\n",
      "  DATASET:\n",
      "    CONTENT_SCENES: ['*']\n",
      "    CONTINUOUS: True\n",
      "    DATA_PATH: data/datasets/audionav/mp3d/{version}/{split}/{split}.json.gz\n",
      "    SCENES_DIR: data/scene_datasets/mp3d\n",
      "    SPLIT: train_telephone\n",
      "    TYPE: AudioNav\n",
      "    VERSION: v1\n",
      "  ENVIRONMENT:\n",
      "    ITERATOR_OPTIONS:\n",
      "      CYCLE: True\n",
      "      GROUP_BY_SCENE: True\n",
      "      MAX_SCENE_REPEAT_EPISODES: -1\n",
      "      MAX_SCENE_REPEAT_STEPS: 10000\n",
      "      NUM_EPISODE_SAMPLE: -1\n",
      "      SHUFFLE: True\n",
      "      STEP_REPETITION_RANGE: 0.2\n",
      "    MAX_EPISODE_SECONDS: 10000000\n",
      "    MAX_EPISODE_STEPS: 500\n",
      "  GYM:\n",
      "    ACHIEVED_GOAL_KEYS: []\n",
      "    ACTION_KEYS: None\n",
      "    AUTO_NAME: \n",
      "    CLASS_NAME: RearrangeRLEnv\n",
      "    DESIRED_GOAL_KEYS: []\n",
      "    FIX_INFO_DICT: True\n",
      "    OBS_KEYS: None\n",
      "  PYROBOT:\n",
      "    BASE_CONTROLLER: proportional\n",
      "    BASE_PLANNER: none\n",
      "    BUMP_SENSOR:\n",
      "      TYPE: PyRobotBumpSensor\n",
      "    DEPTH_SENSOR:\n",
      "      CENTER_CROP: False\n",
      "      HEIGHT: 480\n",
      "      MAX_DEPTH: 5.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      TYPE: PyRobotDepthSensor\n",
      "      WIDTH: 640\n",
      "    LOCOBOT:\n",
      "      ACTIONS: ['BASE_ACTIONS', 'CAMERA_ACTIONS']\n",
      "      BASE_ACTIONS: ['go_to_relative', 'go_to_absolute']\n",
      "      CAMERA_ACTIONS: ['set_pan', 'set_tilt', 'set_pan_tilt']\n",
      "    RGB_SENSOR:\n",
      "      CENTER_CROP: False\n",
      "      HEIGHT: 480\n",
      "      TYPE: PyRobotRGBSensor\n",
      "      WIDTH: 640\n",
      "    ROBOT: locobot\n",
      "    ROBOTS: ['locobot']\n",
      "    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'BUMP_SENSOR']\n",
      "  SEED: 100\n",
      "  SIMULATOR:\n",
      "    ACTION_SPACE_CONFIG: v0\n",
      "    AC_FREQ_RATIO: 4\n",
      "    ADDITIONAL_OBJECT_PATHS: []\n",
      "    AGENTS: ['AGENT_0']\n",
      "    AGENT_0:\n",
      "      HEIGHT: 1.5\n",
      "      IS_SET_START_STATE: False\n",
      "      RADIUS: 0.1\n",
      "      SENSORS: ['RGB_SENSOR']\n",
      "      START_POSITION: [0, 0, 0]\n",
      "      START_ROTATION: [0, 0, 0, 1]\n",
      "    ARM_DEPTH_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      MAX_DEPTH: 10.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimDepthSensor\n",
      "      UUID: robot_arm_depth\n",
      "      WIDTH: 640\n",
      "    ARM_REST: [0.6, 0.0, 0.9]\n",
      "    ARM_RGB_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimRGBSensor\n",
      "      UUID: robot_arm_rgb\n",
      "      WIDTH: 640\n",
      "    AUDIO:\n",
      "      BINAURAL_RIR_DIR: data/binaural_rirs\n",
      "      CROSSFADE: True\n",
      "      EVERLASTING: True\n",
      "      GRAPH_FILE: graph.pkl\n",
      "      HAS_DISTRACTOR_SOUND: False\n",
      "      IR_TIME: 1.0\n",
      "      METADATA_DIR: data/metadata\n",
      "      POINTS_FILE: points.txt\n",
      "      RIR_SAMPLING_RATE: 16000\n",
      "      SCENE: \n",
      "      SOURCE_SOUND_DIR: data/sounds/1s_all\n",
      "    AUTO_SLEEP: False\n",
      "    CONCUR_RENDER: False\n",
      "    CONTINUOUS_VIEW_CHANGE: False\n",
      "    CREATE_RENDERER: False\n",
      "    CTRL_FREQ: 120.0\n",
      "    DEBUG_RENDER: False\n",
      "    DEBUG_RENDER_GOAL: True\n",
      "    DEFAULT_AGENT_ID: 0\n",
      "    DEPTH_SENSOR:\n",
      "      HEIGHT: 128\n",
      "      HFOV: 90\n",
      "      MAX_DEPTH: 10.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimDepthSensor\n",
      "      WIDTH: 128\n",
      "    EE_LINK_NAME: None\n",
      "    EQUIRECT_DEPTH_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      MAX_DEPTH: 10.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      TYPE: HabitatSimEquirectangularDepthSensor\n",
      "      WIDTH: 640\n",
      "    EQUIRECT_RGB_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      TYPE: HabitatSimEquirectangularRGBSensor\n",
      "      WIDTH: 640\n",
      "    EQUIRECT_SEMANTIC_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      TYPE: HabitatSimEquirectangularSemanticSensor\n",
      "      WIDTH: 640\n",
      "    FISHEYE_DEPTH_SENSOR:\n",
      "      ALPHA: 0.57\n",
      "      FOCAL_LENGTH: [364.84, 364.86]\n",
      "      HEIGHT: 480\n",
      "      MAX_DEPTH: 10.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      PRINCIPAL_POINT_OFFSET: None\n",
      "      SENSOR_MODEL_TYPE: DOUBLE_SPHERE\n",
      "      TYPE: HabitatSimFisheyeDepthSensor\n",
      "      WIDTH: 640\n",
      "      XI: -0.27\n",
      "    FISHEYE_RGB_SENSOR:\n",
      "      ALPHA: 0.57\n",
      "      FOCAL_LENGTH: [364.84, 364.86]\n",
      "      HEIGHT: 640\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      PRINCIPAL_POINT_OFFSET: None\n",
      "      SENSOR_MODEL_TYPE: DOUBLE_SPHERE\n",
      "      TYPE: HabitatSimFisheyeRGBSensor\n",
      "      WIDTH: 640\n",
      "      XI: -0.27\n",
      "    FISHEYE_SEMANTIC_SENSOR:\n",
      "      ALPHA: 0.57\n",
      "      FOCAL_LENGTH: [364.84, 364.86]\n",
      "      HEIGHT: 640\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      PRINCIPAL_POINT_OFFSET: None\n",
      "      SENSOR_MODEL_TYPE: DOUBLE_SPHERE\n",
      "      TYPE: HabitatSimFisheyeSemanticSensor\n",
      "      WIDTH: 640\n",
      "      XI: -0.27\n",
      "    FORWARD_STEP_SIZE: 0.25\n",
      "    GRASP_IMPULSE: 1000.0\n",
      "    GRID_SIZE: 1.0\n",
      "    HABITAT_SIM_V0:\n",
      "      ALLOW_SLIDING: True\n",
      "      ENABLE_PHYSICS: False\n",
      "      FRUSTUM_CULLING: True\n",
      "      GPU_DEVICE_ID: 0\n",
      "      GPU_GPU: False\n",
      "      LEAVE_CONTEXT_WITH_BACKGROUND_RENDERER: False\n",
      "      PHYSICS_CONFIG_FILE: ./data/default.physics_config.json\n",
      "    HEAD_DEPTH_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      MAX_DEPTH: 10.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimDepthSensor\n",
      "      UUID: robot_head_depth\n",
      "      WIDTH: 640\n",
      "    HEAD_RGB_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimRGBSensor\n",
      "      UUID: robot_head_rgb\n",
      "      WIDTH: 640\n",
      "    HOLD_THRESH: 0.09\n",
      "    IK_ARM_URDF: data/robots/hab_fetch/robots/fetch_onlyarm.urdf\n",
      "    LAG_OBSERVATIONS: 0\n",
      "    LOAD_OBJS: False\n",
      "    NEEDS_MARKERS: True\n",
      "    REQUIRES_TEXTURES: True\n",
      "    RGB_SENSOR:\n",
      "      HEIGHT: 128\n",
      "      HFOV: 90\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimRGBSensor\n",
      "      WIDTH: 128\n",
      "    ROBOT_JOINT_START_NOISE: 0.0\n",
      "    ROBOT_TYPE: FetchRobot\n",
      "    ROBOT_URDF: data/robots/hab_fetch/robots/hab_fetch.urdf\n",
      "    SCENE: data/scene_datasets/habitat-test-scenes/van-gogh-room.glb\n",
      "    SCENE_DATASET: mp3d\n",
      "    SCENE_OBSERVATION_DIR: data/scene_observations\n",
      "    SEED: 100\n",
      "    SEMANTIC_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimSemanticSensor\n",
      "      WIDTH: 640\n",
      "    STEP_PHYSICS: True\n",
      "    STEP_TIME: 0.25\n",
      "    THIRD_DEPTH_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      MAX_DEPTH: 10.0\n",
      "      MIN_DEPTH: 0.0\n",
      "      NORMALIZE_DEPTH: True\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimDepthSensor\n",
      "      UUID: robot_third_rgb\n",
      "      WIDTH: 640\n",
      "    THIRD_RGB_SENSOR:\n",
      "      HEIGHT: 480\n",
      "      HFOV: 90\n",
      "      ORIENTATION: [0.0, 0.0, 0.0]\n",
      "      POSITION: [0, 1.25, 0]\n",
      "      SENSOR_SUBTYPE: PINHOLE\n",
      "      TYPE: HabitatSimRGBSensor\n",
      "      UUID: robot_third_rgb\n",
      "      WIDTH: 640\n",
      "    TILT_ANGLE: 15\n",
      "    TURN_ANGLE: 10\n",
      "    TYPE: ContinuousSoundSpacesSim\n",
      "    UPDATE_ROBOT: True\n",
      "    USE_RENDERED_OBSERVATIONS: False\n",
      "    USE_SYNC_VECENV: False\n",
      "    VIEW_CHANGE_FPS: 10\n",
      "  TASK:\n",
      "    ABS_GOAL_SENSOR:\n",
      "      DIMENSIONALITY: 3\n",
      "      GOAL_FORMAT: CARTESIAN\n",
      "      TYPE: AbsGoalSensor\n",
      "    ABS_TARGET_START_SENSOR:\n",
      "      DIMENSIONALITY: 3\n",
      "      GOAL_FORMAT: CARTESIAN\n",
      "      TYPE: AbsTargetStartSensor\n",
      "    ACTIONS:\n",
      "      ANSWER:\n",
      "        TYPE: AnswerAction\n",
      "      ARM_ACTION:\n",
      "        ARM_CONTROLLER: ArmRelPosAction\n",
      "        ARM_JOINT_DIMENSIONALITY: 7\n",
      "        DELTA_POS_LIMIT: 0.0125\n",
      "        DISABLE_GRIP: False\n",
      "        EE_CTRL_LIM: 0.015\n",
      "        GRASP_THRESH_DIST: 0.15\n",
      "        GRIP_CONTROLLER: None\n",
      "        ORACLE_GRASP: False\n",
      "        RENDER_EE_TARGET: False\n",
      "        SHOULD_CLIP: False\n",
      "        TYPE: ArmAction\n",
      "      BASE_VELOCITY:\n",
      "        ALLOW_BACK: True\n",
      "        ALLOW_DYN_SLIDE: True\n",
      "        ANG_SPEED: 12.0\n",
      "        END_ON_STOP: False\n",
      "        LIN_SPEED: 12.0\n",
      "        MIN_ABS_ANG_SPEED: 1.0\n",
      "        MIN_ABS_LIN_SPEED: 1.0\n",
      "        TYPE: BaseVelAction\n",
      "      EMPTY:\n",
      "        TYPE: EmptyAction\n",
      "      LOOK_DOWN:\n",
      "        TYPE: LookDownAction\n",
      "      LOOK_UP:\n",
      "        TYPE: LookUpAction\n",
      "      MOVE_FORWARD:\n",
      "        TYPE: MoveForwardAction\n",
      "      REARRANGE_STOP:\n",
      "        TYPE: RearrangeStopAction\n",
      "      STOP:\n",
      "        TYPE: StopAction\n",
      "      TELEPORT:\n",
      "        TYPE: TeleportAction\n",
      "      TURN_LEFT:\n",
      "        TYPE: TurnLeftAction\n",
      "      TURN_RIGHT:\n",
      "        TYPE: TurnRightAction\n",
      "      VELOCITY_CONTROL:\n",
      "        ANG_VEL_RANGE: [-10.0, 10.0]\n",
      "        LIN_VEL_RANGE: [0.0, 0.25]\n",
      "        MIN_ABS_ANG_SPEED: 1.0\n",
      "        MIN_ABS_LIN_SPEED: 0.025\n",
      "        TIME_STEP: 1.0\n",
      "        TYPE: VelocityAction\n",
      "    ANSWER_ACCURACY:\n",
      "      TYPE: AnswerAccuracy\n",
      "    ART_JOINT_SENSOR:\n",
      "      TYPE: ArtJointSensor\n",
      "    ART_JOINT_SENSOR_NO_VEL:\n",
      "      TYPE: ArtJointSensorNoVel\n",
      "    ART_OBJ_AT_DESIRED_STATE:\n",
      "      SUCCESS_DIST_THRESHOLD: 0.05\n",
      "      TYPE: ArtObjAtDesiredState\n",
      "      USE_ABSOLUTE_DISTANCE: True\n",
      "    ART_OBJ_REWARD:\n",
      "      ART_AT_DESIRED_STATE_REWARD: 5.0\n",
      "      ART_DIST_REWARD: 10.0\n",
      "      CONSTRAINT_VIOLATE_PEN: 10.0\n",
      "      DIST_REWARD: 1.0\n",
      "      EE_DIST_REWARD: 10.0\n",
      "      FORCE_END_PEN: 10.0\n",
      "      FORCE_PEN: 0.0\n",
      "      GRASP_REWARD: 0.0\n",
      "      MARKER_DIST_REWARD: 0.0\n",
      "      MAX_FORCE_PEN: 1.0\n",
      "      TYPE: ArtObjReward\n",
      "      WRONG_GRASP_END: False\n",
      "      WRONG_GRASP_PEN: 5.0\n",
      "    ART_OBJ_STATE:\n",
      "      TYPE: ArtObjState\n",
      "    ART_OBJ_SUCCESS:\n",
      "      REST_DIST_THRESHOLD: 0.15\n",
      "      TYPE: ArtObjSuccess\n",
      "    ART_SUCC_THRESH: 0.15\n",
      "    AUDIOGOAL_SENSOR:\n",
      "      TYPE: AudioGoalSensor\n",
      "    BAD_CALLED_TERMINATE:\n",
      "      BAD_TERM_PEN: 0.0\n",
      "      DECAY_BAD_TERM: False\n",
      "      TYPE: BadCalledTerminate\n",
      "    BASE_ANGLE_NOISE: 0.15\n",
      "    BASE_NOISE: 0.05\n",
      "    COLLISIONS:\n",
      "      TYPE: Collisions\n",
      "    COMPASS_SENSOR:\n",
      "      TYPE: CompassSensor\n",
      "    COMPOSITE_BAD_CALLED_TERMINATE:\n",
      "      TYPE: CompositeBadCalledTerminate\n",
      "    COMPOSITE_NODE_IDX:\n",
      "      TYPE: CompositeNodeIdx\n",
      "    COMPOSITE_REWARD:\n",
      "      STAGE_COMPLETE_REWARD: 10.0\n",
      "      SUCCESS_REWARD: 10.0\n",
      "      TYPE: CompositeReward\n",
      "    COMPOSITE_SUCCESS:\n",
      "      TYPE: CompositeSuccess\n",
      "    CONSTRAINT_VIOLATION_ENDS_EPISODE: True\n",
      "    CORRECT_ANSWER:\n",
      "      TYPE: CorrectAnswer\n",
      "    COUNT_OBJ_COLLISIONS: True\n",
      "    COUNT_ROBOT_OBJ_COLLS: False\n",
      "    DEBUG_GOAL_POINT: True\n",
      "    DEBUG_SKIP_TO_NODE: -1\n",
      "    DESIRED_RESTING_POSITION: []\n",
      "    DID_PICK_OBJECT:\n",
      "      TYPE: DidPickObjectMeasure\n",
      "    DID_VIOLATE_HOLD_CONSTRAINT:\n",
      "      TYPE: DidViolateHoldConstraintMeasure\n",
      "    DISTANCE_TO_GOAL:\n",
      "      DISTANCE_TO: POINT\n",
      "      TYPE: DistanceToGoal\n",
      "    DIST_TO_GOAL:\n",
      "      TYPE: DistToGoal\n",
      "    DIST_TO_NAV_GOAL:\n",
      "      TYPE: DistToNavGoalSensor\n",
      "    DOES_WANT_TERMINATE:\n",
      "      TYPE: DoesWantTerminate\n",
      "    EASY_INIT: False\n",
      "    EE_DIST_TO_MARKER:\n",
      "      TYPE: EndEffectorDistToMarker\n",
      "    EE_EXCLUDE_REGION: 0.0\n",
      "    EE_SAMPLE_FACTOR: 0.2\n",
      "    END_EFFECTOR_SENSOR:\n",
      "      TYPE: EEPositionSensor\n",
      "    END_EFFECTOR_TO_OBJECT_DISTANCE:\n",
      "      TYPE: EndEffectorToObjectDistance\n",
      "    END_EFFECTOR_TO_REST_DISTANCE:\n",
      "      TYPE: EndEffectorToRestDistance\n",
      "    END_ON_SUCCESS: False\n",
      "    EPISODE_INFO:\n",
      "      TYPE: EpisodeInfo\n",
      "    FILTER_NAV_TO_TASKS: []\n",
      "    FORCE_REGENERATE: False\n",
      "    FORCE_TERMINATE:\n",
      "      MAX_ACCUM_FORCE: -1.0\n",
      "      TYPE: ForceTerminate\n",
      "    GOAL_SENSOR:\n",
      "      DIMENSIONALITY: 3\n",
      "      GOAL_FORMAT: CARTESIAN\n",
      "      TYPE: GoalSensor\n",
      "    GOAL_SENSOR_UUID: spectrogram\n",
      "    GPS_SENSOR:\n",
      "      DIMENSIONALITY: 2\n",
      "      TYPE: GPSSensor\n",
      "    HEADING_SENSOR:\n",
      "      TYPE: HeadingSensor\n",
      "    IMAGEGOAL_SENSOR:\n",
      "      TYPE: ImageGoalSensor\n",
      "    INSTRUCTION_SENSOR:\n",
      "      TYPE: InstructionSensor\n",
      "    INSTRUCTION_SENSOR_UUID: instruction\n",
      "    IS_HOLDING_SENSOR:\n",
      "      TYPE: IsHoldingSensor\n",
      "    JOINT_MAX_IMPULSE: -1.0\n",
      "    JOINT_SENSOR:\n",
      "      DIMENSIONALITY: 7\n",
      "      TYPE: JointSensor\n",
      "    JOINT_VELOCITY_SENSOR:\n",
      "      DIMENSIONALITY: 7\n",
      "      TYPE: JointVelocitySensor\n",
      "    LIMIT_TASK_LEN_SCALING: 0.0\n",
      "    LIMIT_TASK_NODE: -1\n",
      "    LOCALIZATION_SENSOR:\n",
      "      TYPE: LocalizationSensor\n",
      "    MARKER_REL_POS_SENSOR:\n",
      "      TYPE: MarkerRelPosSensor\n",
      "    MAX_COLLISIONS: -1.0\n",
      "    MEASUREMENTS: ['DISTANCE_TO_GOAL', 'NORMALIZED_DISTANCE_TO_GOAL', 'SUCCESS', 'SPL', 'SOFT_SPL', 'NUM_ACTION', 'SUCCESS_WEIGHTED_BY_NUM_ACTION', 'TOP_DOWN_MAP']\n",
      "    MOVE_OBJECTS_REWARD:\n",
      "      CONSTRAINT_VIOLATE_PEN: 10.0\n",
      "      DIST_REWARD: 1.0\n",
      "      FORCE_END_PEN: 10.0\n",
      "      FORCE_PEN: 0.001\n",
      "      MAX_FORCE_PEN: 1.0\n",
      "      PICK_REWARD: 1.0\n",
      "      SINGLE_REARRANGE_REWARD: 1.0\n",
      "      SUCCESS_DIST: 0.15\n",
      "      TYPE: MoveObjectsReward\n",
      "    MUST_LOOK_AT_TARG: True\n",
      "    NAV_GOAL_SENSOR:\n",
      "      TYPE: NavGoalSensor\n",
      "    NAV_TO_POS_SUCC:\n",
      "      SUCCESS_DISTANCE: 0.2\n",
      "      TYPE: NavToPosSucc\n",
      "    NAV_TO_SKILL_SENSOR:\n",
      "      NUM_SKILLS: 8\n",
      "      TYPE: NavToSkillSensor\n",
      "    NORMALIZED_DISTANCE_TO_GOAL:\n",
      "      TYPE: NormalizedDistanceToGoal\n",
      "    NUM_ACTION:\n",
      "      TYPE: NA\n",
      "    NUM_STEPS:\n",
      "      TYPE: NumStepsMeasure\n",
      "    OBJECTGOAL_SENSOR:\n",
      "      GOAL_SPEC: TASK_CATEGORY_ID\n",
      "      GOAL_SPEC_MAX_VAL: 50\n",
      "      TYPE: ObjectGoalSensor\n",
      "    OBJECT_IN_HAND_SAMPLE_PROB: 0.167\n",
      "    OBJECT_SENSOR:\n",
      "      DIMENSIONALITY: 3\n",
      "      GOAL_FORMAT: CARTESIAN\n",
      "      TYPE: TargetCurrentSensor\n",
      "    OBJECT_TO_GOAL_DISTANCE:\n",
      "      TYPE: ObjectToGoalDistance\n",
      "    OBJ_AT_GOAL:\n",
      "      SUCC_THRESH: 0.15\n",
      "      TYPE: ObjAtGoal\n",
      "    OBJ_SUCC_THRESH: 0.3\n",
      "    ORACLE_ACTION_SENSOR:\n",
      "      TYPE: OracleActionSensor\n",
      "    ORACLE_NAV_ACTION_SENSOR:\n",
      "      TYPE: OracleNavigationActionSensor\n",
      "    PDDL_DOMAIN_DEF: configs/tasks/rearrange/pddl/replica_cad_domain.yaml\n",
      "    PICK_REWARD:\n",
      "      COLLISION_PENALTY: 0.0\n",
      "      COLL_PEN: 1.0\n",
      "      CONSTRAINT_VIOLATE_PEN: 10.0\n",
      "      DIST_REWARD: 20.0\n",
      "      DROP_OBJ_SHOULD_END: False\n",
      "      DROP_PEN: 5.0\n",
      "      FORCE_END_PEN: 10.0\n",
      "      FORCE_PEN: 0.001\n",
      "      MAX_ACCUM_FORCE: 5000.0\n",
      "      MAX_FORCE_PEN: 1.0\n",
      "      PICK_REWARD: 20.0\n",
      "      ROBOT_OBJ_COLLISION_PENALTY: 0.0\n",
      "      ROBOT_OBJ_COLL_PEN: 0.0\n",
      "      SUCC_REWARD: 10.0\n",
      "      TYPE: RearrangePickReward\n",
      "      USE_DIFF: True\n",
      "      WRONG_PICK_PEN: 5.0\n",
      "      WRONG_PICK_SHOULD_END: False\n",
      "    PICK_SUCCESS:\n",
      "      EE_RESTING_SUCCESS_THRESHOLD: 0.15\n",
      "      TYPE: RearrangePickSuccess\n",
      "    PLACE_REWARD:\n",
      "      CONSTRAINT_VIOLATE_PEN: 10.0\n",
      "      DIST_REWARD: 20.0\n",
      "      DROP_PEN: 5.0\n",
      "      FORCE_END_PEN: 10.0\n",
      "      FORCE_PEN: 0.001\n",
      "      MAX_FORCE_PEN: 1.0\n",
      "      PLACE_REWARD: 20.0\n",
      "      SUCC_REWARD: 10.0\n",
      "      TYPE: PlaceReward\n",
      "      USE_DIFF: True\n",
      "      WRONG_DROP_SHOULD_END: False\n",
      "    PLACE_SUCCESS:\n",
      "      EE_RESTING_SUCCESS_THRESHOLD: 0.15\n",
      "      TYPE: PlaceSuccess\n",
      "    POINTGOAL_SENSOR:\n",
      "      DIMENSIONALITY: 2\n",
      "      GOAL_FORMAT: POLAR\n",
      "      TYPE: PointGoalSensor\n",
      "    POINTGOAL_WITH_GPS_COMPASS_SENSOR:\n",
      "      DIMENSIONALITY: 2\n",
      "      GOAL_FORMAT: POLAR\n",
      "      TYPE: PointGoalWithGPSCompassSensor\n",
      "    POSSIBLE_ACTIONS: ['STOP', 'MOVE_FORWARD', 'TURN_LEFT', 'TURN_RIGHT']\n",
      "    PROXIMITY_SENSOR:\n",
      "      MAX_DETECTION_RADIUS: 2.0\n",
      "      TYPE: ProximitySensor\n",
      "    QUESTION_SENSOR:\n",
      "      TYPE: QuestionSensor\n",
      "    REARRANGE_NAV_TO_OBJ_REWARD:\n",
      "      ANGLE_DIST_REWARD: 1.0\n",
      "      CONSTRAINT_VIOLATE_PEN: 10.0\n",
      "      DIST_REWARD: 10.0\n",
      "      FORCE_END_PEN: 10.0\n",
      "      FORCE_PEN: 0.0\n",
      "      MAX_FORCE_PEN: 1.0\n",
      "      SHOULD_REWARD_TURN: True\n",
      "      TURN_REWARD_DIST: 0.1\n",
      "      TYPE: NavToObjReward\n",
      "    REARRANGE_NAV_TO_OBJ_SUCCESS:\n",
      "      HEURISTIC_STOP: False\n",
      "      MUST_CALL_STOP: True\n",
      "      MUST_LOOK_AT_TARG: True\n",
      "      SUCCESS_ANGLE_DIST: 0.15\n",
      "      TYPE: NavToObjSuccess\n",
      "    REARRANGE_REACH_REWARD:\n",
      "      DIFF_REWARD: True\n",
      "      SCALE: 1.0\n",
      "      SPARSE_REWARD: False\n",
      "      TYPE: RearrangeReachReward\n",
      "    REARRANGE_REACH_SUCCESS:\n",
      "      SUCC_THRESH: 0.2\n",
      "      TYPE: RearrangeReachSuccess\n",
      "    RELATIVE_RESTING_POS_SENSOR:\n",
      "      TYPE: RelativeRestingPositionSensor\n",
      "    RENDER_TARGET: True\n",
      "    RESTING_POS_SENSOR:\n",
      "      TYPE: RestingPositionSensor\n",
      "    REWARD_MEASURE: distance_to_goal\n",
      "    REWARD_MEASUREMENT: \n",
      "    ROBOT_COLLS:\n",
      "      TYPE: RobotCollisions\n",
      "    ROBOT_FORCE:\n",
      "      MIN_FORCE: 20.0\n",
      "      TYPE: RobotForce\n",
      "    ROT_DIST_TO_GOAL:\n",
      "      TYPE: RotDistToGoal\n",
      "    SENSORS: ['SPECTROGRAM_SENSOR']\n",
      "    SETTLE_STEPS: 5\n",
      "    SHOULD_ENFORCE_TARGET_WITHIN_REACH: False\n",
      "    SHOULD_SAVE_TO_CACHE: True\n",
      "    SINGLE_EVAL_NODE: -1\n",
      "    SKIP_NODES: ['move_obj']\n",
      "    SLACK_REWARD: -0.01\n",
      "    SOFT_SPL:\n",
      "      TYPE: SoftSPL\n",
      "    SPAWN_REGION_SCALE: 0.2\n",
      "    SPECTROGRAM_SENSOR:\n",
      "      TYPE: SpectrogramSensor\n",
      "    SPL:\n",
      "      TYPE: SPL\n",
      "    SUCCESS:\n",
      "      SUCCESS_DISTANCE: 1.0\n",
      "      TYPE: Success\n",
      "    SUCCESS_DISTANCE: 1.0\n",
      "    SUCCESS_MEASURE: spl\n",
      "    SUCCESS_MEASUREMENT: \n",
      "    SUCCESS_REWARD: 2.5\n",
      "    SUCCESS_STATE: 0.0\n",
      "    SUCCESS_WEIGHTED_BY_NUM_ACTION:\n",
      "      TYPE: SNA\n",
      "    SUCCESS_WHEN_SILENT:\n",
      "      TYPE: SWS\n",
      "    TARGET_GOAL_GPS_COMPASS_SENSOR:\n",
      "      TYPE: TargetGoalGpsCompassSensor\n",
      "    TARGET_START_GPS_COMPASS_SENSOR:\n",
      "      TYPE: TargetStartGpsCompassSensor\n",
      "    TARGET_START_POINT_GOAL_SENSOR:\n",
      "      TYPE: TargetOrGoalStartPointGoalSensor\n",
      "    TARGET_START_SENSOR:\n",
      "      DIMENSIONALITY: 3\n",
      "      GOAL_FORMAT: CARTESIAN\n",
      "      TYPE: TargetStartSensor\n",
      "    TASK_SPEC: nav_pick\n",
      "    TASK_SPEC_BASE_PATH: configs/tasks/rearrange/pddl/\n",
      "    TOP_DOWN_MAP:\n",
      "      DRAW_BORDER: True\n",
      "      DRAW_GOAL_AABBS: True\n",
      "      DRAW_GOAL_POSITIONS: True\n",
      "      DRAW_SHORTEST_PATH: True\n",
      "      DRAW_SOURCE: True\n",
      "      DRAW_SOURCE_AND_TARGET: True\n",
      "      DRAW_VIEW_POINTS: True\n",
      "      FOG_OF_WAR:\n",
      "        DRAW: True\n",
      "        FOV: 90\n",
      "        VISIBILITY_DIST: 5.0\n",
      "      MAP_PADDING: 3\n",
      "      MAP_RESOLUTION: 500\n",
      "      MAX_EPISODE_STEPS: 1000\n",
      "      TYPE: TopDownMap\n",
      "    TYPE: AudioNav\n",
      "    USE_MARKER_T: True\n",
      "    USING_SUBTASKS: False\n",
      "TENSORBOARD_DIR: data/models/output/tb\n",
      "TEST_EPISODE_COUNT: 2\n",
      "TORCH_GPU_ID: 0\n",
      "TRAINER_NAME: AVNavTrainer\n",
      "USE_LAST_CKPT: False\n",
      "USE_SYNC_VECENV: False\n",
      "USE_VECENV: False\n",
      "VIDEO_DIR: data/models/output/video_dir\n",
      "VIDEO_OPTION: []\n",
      "VISUALIZATION_OPTION: []\n"
     ]
    }
   ],
   "source": [
    "# print(env_config.TASK_CONFIG.TASK.SENSORS)\n",
    "print(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 14:55:38,798 Initializing dataset AudioNav\n",
      "2022-07-27 14:55:38,866 Initializing dataset AudioNav\n",
      "2022-07-27 14:55:38,870 Initializing dataset AudioNav\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### JUPYTER DEBUG NOTE: Thsi step takes a lot of time, so better avoid running it too much.\n",
    "# Environment instantiation\n",
    "envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "single_observation_space = envs.observation_spaces[0]\n",
    "single_action_space = envs.action_spaces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tried to write to process 0 but the last write has not been read",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/rousslan/random/rl/ss-hab-headless-py39/ppo/ppo_proto.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab-headless-py39/ppo/ppo_proto.ipynb#ch0000005vscode-remote?line=28'>29</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab-headless-py39/ppo/ppo_proto.ipynb#ch0000005vscode-remote?line=29'>30</a>\u001b[0m num_updates \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mtotal_steps \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m args\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab-headless-py39/ppo/ppo_proto.ipynb#ch0000005vscode-remote?line=31'>32</a>\u001b[0m obs \u001b[39m=\u001b[39m envs\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab-headless-py39/ppo/ppo_proto.ipynb#ch0000005vscode-remote?line=33'>34</a>\u001b[0m done \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mnum_envs)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab-headless-py39/ppo/ppo_proto.ipynb#ch0000005vscode-remote?line=34'>35</a>\u001b[0m done_th \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mTensor(done)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/random/rl/ss-hab-headless-py39/habitat-lab/habitat/core/vector_env.py:381\u001b[0m, in \u001b[0;36mVectorEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Reset all the vectorized environments\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \n\u001b[1;32m    378\u001b[0m \u001b[39m:return: list of outputs from the reset method of envs.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39mfor\u001b[39;00m write_fn \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_write_fns:\n\u001b[0;32m--> 381\u001b[0m     write_fn((RESET_COMMAND, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    382\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    383\u001b[0m \u001b[39mfor\u001b[39;00m read_fn \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_read_fns:\n",
      "File \u001b[0;32m~/random/rl/ss-hab-headless-py39/habitat-lab/habitat/core/vector_env.py:114\u001b[0m, in \u001b[0;36m_WriteWrapper.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_wrapper\u001b[39m.\u001b[39mis_waiting:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTried to write to process \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_wrapper\u001b[39m.\u001b[39mrank\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m but the last write has not been read\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_fn(data)\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_wrapper\u001b[39m.\u001b[39mis_waiting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tried to write to process 0 but the last write has not been read"
     ]
    }
   ],
   "source": [
    "# TODO: make the ActorCritic components parameterizable through comand line ?\n",
    "agent = ActorCritic(single_observation_space, single_action_space, 512).to(device)\n",
    "optimizer = th.optim.Adam(agent.parameters(), lr=args.lr, eps=1e-5)\n",
    "\n",
    "# Rollout storage setup\n",
    "observations = {\n",
    "    \"rgb\": th.zeros((args.num_steps, args.num_envs) + single_observation_space[\"rgb\"].shape, device=device),\n",
    "    \"spectrogram\": th.zeros((args.num_steps, args.num_envs) + single_observation_space[\"spectrogram\"].shape, device=device),\n",
    "    \"audiogoal\": th.zeros((args.num_steps, args.num_envs) + single_observation_space[\"audiogoal\"].shape, device=device)\n",
    "}\n",
    "actions = th.zeros((args.num_steps, args.num_envs), dtype=th.int64, device=device)\n",
    "logprobs = th.zeros((args.num_steps, args.num_envs), device=device)\n",
    "rewards = th.zeros((args.num_steps, args.num_envs), device=device)\n",
    "dones = th.zeros((args.num_steps, args.num_envs), device=device)\n",
    "values = th.zeros((args.num_steps, args.num_envs), device=device)\n",
    "\n",
    "# Variables to track episode reward\n",
    "current_episode_reward = th.zeros(envs.num_envs, 1)\n",
    "running_episode_stats = dict(\n",
    "    count=th.zeros(envs.num_envs, 1),\n",
    "    reward=th.zeros(envs.num_envs, 1),\n",
    ")\n",
    "latest_successes = deque([], env_config.RL.PPO.reward_window_size)\n",
    "window_episode_stats = defaultdict(\n",
    "    lambda: deque(maxlen=env_config.RL.PPO.reward_window_size)\n",
    ")\n",
    "\n",
    "# Training start\n",
    "start_time = time.time()\n",
    "num_updates = args.total_steps // args.batch_size\n",
    "\n",
    "obs = envs.reset()\n",
    "\n",
    "done = [False for _ in range(args.num_envs)]\n",
    "done_th = th.Tensor(done).to(device)\n",
    "masks = 1. - done_th[:, None]\n",
    "\n",
    "init_hidden_state = th.zeros((1, args.num_envs, args.hidden_size), device=device)\n",
    "rnn_hidden_state = init_hidden_state.clone()\n",
    "\n",
    "for global_step in range(1, args.total_steps+1, args.num_steps * args.num_envs):\n",
    "\n",
    "    for rollout_step in range(args.num_steps):\n",
    "        # NOTE: the following line tensorize and also appends data to the rollout storage\n",
    "        obs_th = tensorize_obs_dict(obs, device, observations, rollout_step)\n",
    "        dones[rollout_step] = done_th\n",
    "\n",
    "        # Sample action\n",
    "        with th.no_grad():\n",
    "            action, action_logprobs, _, value, rnn_hidden_state = \\\n",
    "                agent.act(obs_th, rnn_hidden_state, masks=masks)\n",
    "            values[rollout_step] = value.flatten()\n",
    "        actions[rollout_step] = action.squeeze(-1) # actions: [T, B] but action: [B, 1]\n",
    "        logprobs[rollout_step] = action_logprobs.sum(-1)\n",
    "\n",
    "        outputs = envs.step([a[0].item() for a in action])\n",
    "        obs, reward, done, info = [list(x) for x in zip(*outputs)]\n",
    "        reward_th = th.Tensor(np.array(reward, dtype=np.float32)).to(device)\n",
    "        rewards[rollout_step] = reward_th\n",
    "        \n",
    "        ## This is done to update the masks that will be used to track episodic return. Anyway to make this more efficient ?\n",
    "        done_th = th.Tensor(done).to(device)\n",
    "        masks = 1. - done_th[:, None]\n",
    "\n",
    "        # Tracking episode return\n",
    "        # TODO: keep this on GPU for more efficiency ? We log less than we update, so ...\n",
    "        current_episode_reward += reward_th[:, None].to(current_episode_reward.device)\n",
    "        running_episode_stats[\"reward\"] += (1 - masks.to(current_episode_reward.device)) * current_episode_reward\n",
    "        running_episode_stats[\"count\"] += (1. - masks.to(current_episode_reward.device))\n",
    "        \n",
    "        if should_log_sampling_stats(global_step) and (True in done):\n",
    "            # TODO: additional metrics logging, log the video and other stats of\n",
    "            info_stats = {\n",
    "                \"global_step\": global_step,\n",
    "                \"duration\": time.time() - start_time,\n",
    "                \"fps\": tblogger.track_duration(\"fps\", global_step),\n",
    "                \"env_step_duration\": tblogger.track_duration(\"fps_inv\", global_step, inverse=True),\n",
    "                \"model_updates_per_sec\": tblogger.track_duration(\"model_updates\",\n",
    "                    num_updates),\n",
    "                \"model_update_step_duration\": tblogger.track_duration(\"model_updates_inv\",\n",
    "                    num_updates, inverse=True)\n",
    "            }\n",
    "            tblogger.log_stats(info_stats, global_step, \"info\")\n",
    "\n",
    "            # TODO: extract the success rate and other variables\n",
    "            episode_stats = {\n",
    "                # This seems to accumulate episodic return over all episodes, not very representative of ongoingtraining.\n",
    "                # \"episode_return\": (running_episode_stats[\"reward\"].sum() / done_th.sum().item()).item(),\n",
    "                \"episode_return\": (current_episode_reward.sum() / done_th.sum()).item(),\n",
    "                \"episode_count\": running_episode_stats[\"count\"].sum().item()\n",
    "            }\n",
    "            tblogger.log_stats(episode_stats, global_step, \"metrics\")\n",
    "        \n",
    "        # Resets the episodic return tracker\n",
    "        current_episode_reward *= masks.to(current_episode_reward.device)\n",
    "\n",
    "    # Prepare for PPO update phase\n",
    "    ## Bootstrap value if not done\n",
    "    with th.no_grad():\n",
    "        obs_th = tensorize_obs_dict(obs, device)\n",
    "        done_th = th.Tensor(done).to(device)\n",
    "        value = agent.get_value(obs_th, rnn_hidden_state, masks=1.-done_th[:, None]).flatten()\n",
    "        # By default, use GAE\n",
    "        advantages = th.zeros_like(rewards)\n",
    "        lastgaelam = 0.\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - done_th\n",
    "                nextvalues = value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # Form batch data of dim [ NUM_ENVS * NUM_STEPS, ...]\n",
    "    b_observations = {}\n",
    "    for k, v in observations.items():\n",
    "        b_observations[k] = th.flatten(v, start_dim=0, end_dim=1)\n",
    "    # b_observations = observations.reshape()\n",
    "    b_logprobs = logprobs.reshape(-1) # From [B, T] -> [B * T]\n",
    "    b_actions = actions.reshape(-1) # From [B, T] -> [B * T]\n",
    "    b_dones = dones.reshape(-1) # From [B, T] -> [B * T]\n",
    "    b_advantages = advantages.reshape(-1) # From [B, T] -> [B * T]\n",
    "    b_returns = returns.reshape(-1) # From [B, T] -> [B * T]\n",
    "    b_values = values.reshape(-1) # From [B, T] -> [B * T]\n",
    "    \n",
    "    # PPO Update Phase: actor and critic network updates\n",
    "    assert args.num_envs % args.num_minibatches == 0\n",
    "    envsperbatch = args.num_envs // args.num_minibatches\n",
    "    envinds = np.arange(args.num_envs)\n",
    "    flatinds = np.arange(args.batch_size).reshape(args.num_steps, args.num_envs)\n",
    "    clipfracs = []\n",
    "\n",
    "    for _ in range(args.update_epochs):\n",
    "        np.random.shuffle(envinds)\n",
    "        # Why minibatch ? Some empirical evidence that using smaller batch around 32 or 64\n",
    "        # are generally better. Also, LeCun.\n",
    "        for start in range(0, args.num_envs, envsperbatch):\n",
    "            end = start + envsperbatch\n",
    "            mbenvinds = envinds[start:end]\n",
    "            mb_inds = flatinds[:, mbenvinds].ravel()  # be really careful about the index\n",
    "\n",
    "            # Make a minibatch of observation dict\n",
    "            mb_observations = {k: v[mb_inds] for k, v in b_observations.items()}\n",
    "\n",
    "            _, newlogprob, entropy, newvalue, _ = \\\n",
    "                agent.act(\n",
    "                    mb_observations, init_hidden_state[:, mbenvinds],\n",
    "                    masks=1-b_dones[mb_inds], actions=b_actions[mb_inds])\n",
    "\n",
    "            newlogprob = newlogprob.sum(-1) # From [B * T, 1] -> [B * T]\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with th.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * th.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = th.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + th.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = th.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        num_updates += 1\n",
    "        \n",
    "        if args.target_kl is not None:\n",
    "            if approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    if num_updates > 0 and should_log_training_stats(num_updates):\n",
    "        train_stats = {\n",
    "            \"value_loss\": v_loss.item(),\n",
    "            \"policy_loss\": pg_loss.item(),\n",
    "            \"entropy\": entropy_loss.item(),\n",
    "            \"old_approx_kl\": old_approx_kl.item(),\n",
    "            \"approx_kl\": approx_kl.item(),\n",
    "            \"clipfrac\": np.mean(clipfracs),\n",
    "            \"explained_variance\": explained_var\n",
    "        }\n",
    "        tblogger.log_stats(train_stats, global_step, prefix=\"train\")\n",
    "    \n",
    "# Clean up\n",
    "envs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
