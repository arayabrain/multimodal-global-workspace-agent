{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "import rsatoolbox\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "# General config related\n",
    "from configurator import get_arg_dict, generate_args\n",
    "\n",
    "# Env config related\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/audiogoal_rgb_nocont.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 1), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.2), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"deep-etho\",\n",
    "                    \"perceiver-gwt-gwwm\", \"perceiver-gwt-attgru\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, True, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "# endregion: Generating additional hyparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "env_config.USE_SYNC_VECENV = True\n",
    "# env_config.USE_VECENV = False\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "env_config.freeze()\n",
    "# print(env_config)\n",
    "\n",
    "# Environment instantiation\n",
    "# envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "# Dummy environment spaces\n",
    "from gym import spaces\n",
    "single_action_space = spaces.Discrete(4)\n",
    "single_observation_space = spaces.Dict({\n",
    "    \"rgb\": spaces.Box(shape=[128,128,3], low=0, high=255, dtype=np.uint8),\n",
    "    \"depth\": spaces.Box(shape=[128,128,1], low=0, high=255, dtype=np.uint8),\n",
    "    \"audiogoal\": spaces.Box(shape=[2,16000], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32),\n",
    "    \"spectrogram\": spaces.Box(shape=[65,26,2], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32)\n",
    "})\n",
    "# single_observation_space = envs.observation_spaces[0]\n",
    "# single_action_space = envs.action_spaces[0]\n",
    "\n",
    "single_observation_space, single_action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO seeding for reproducibility ? Make sure that we can control the generated episode trajs ?\n",
    "\n",
    "# Loading pretrained agent\n",
    "import models\n",
    "from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic\n",
    "\n",
    "MODEL_VARIANTS_TO_STATEDICT_PATH = {\n",
    "\n",
    "    # SAVi BC Baseline variants\n",
    "    \"ppo_gru__bc__SAVi\": {\n",
    "        \"pretty_name\": \"[SAVi] PPO GRU (BC)\",\n",
    "        \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "            \"ppo_bc__savi_ss1__rgb_depth_spectro__gru_seed_111__2023_05_23_23_17_03_387659.musashi\"\n",
    "            \"/models/ppo_agent.4995001.ckpt.pth\"\n",
    "    },\n",
    "    \"ppo_pgwt__bc__SAVi\": {\n",
    "        \"pretty_name\": \"[SAVi] PPO PGWT (BC)\",\n",
    "        \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "            \"ppo_bc__savi_ss1__rgb_depth_spectro__pgwt__dpth_1_nlats_8_latdim_64_noSA_CAnheads_1_SAnheads_4_modembed_0_CAprevlats_seed_111__2023_05_23_23_17_04_044443.musashi\"\n",
    "            \"/models/ppo_agent.4995001.ckpt.pth\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# 'variant named' indexed 'torch agent'\n",
    "MODEL_VARIANTS_TO_AGENTMODEL = {}\n",
    "\n",
    "for k, v in MODEL_VARIANTS_TO_STATEDICT_PATH.items():\n",
    "    args_copy = copy.copy(args)\n",
    "    # Override args depending on the model in use\n",
    "    if k.__contains__(\"gru\"):\n",
    "        agent = ActorCritic(single_observation_space, single_action_space, args.hidden_size, extra_rgb=False,\n",
    "            analysis_layers=models.GRU_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES)\n",
    "    elif k.__contains__(\"pgwt\"):\n",
    "        agent = Perceiver_GWT_GWWM_ActorCritic(single_observation_space, single_action_space, args, extra_rgb=False,\n",
    "            analysis_layers=models.PGWT_GWWM_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES + [\"state_encoder.ca.mha\"])\n",
    "\n",
    "    agent.eval()\n",
    "    # Load the model weights\n",
    "    # TODO: add map location device to use CPU only ?\n",
    "    if v[\"state_dict_path\"] != \"\":\n",
    "        agent_state_dict = th.load(v[\"state_dict_path\"])\n",
    "        agent.load_state_dict(agent_state_dict)\n",
    "    \n",
    "    MODEL_VARIANTS_TO_AGENTMODEL[k] = agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Category-Scene-Trajs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file name\n",
    "analysis_trajs_filename = \"analysis_trajs_C_6_M_5_N_5.bz2\"\n",
    "\n",
    "# Read the filtred trajectories data\n",
    "with open(analysis_trajs_filename, \"rb\") as f:\n",
    "    analysis_trajs_dict = cpkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for catname, cat_scenes_trajs in analysis_trajs_dict.items():\n",
    "    print(f\"Cat: {catname}; Scenes: {[k for k in cat_scenes_trajs.keys()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One trajectory sample dict\n",
    "edd = analysis_trajs_dict[\"chair\"][\"7y3sRwLe3Va\"][0][\"edd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edd[\"obs_list\"][\"rgb\"]\n",
    "len(edd[\"obs_list\"][\"rgb\"])\n",
    "np.array(edd[\"obs_list\"][\"rgb\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rsatoolbox\n",
    "\n",
    "rgb_obs_list = np.array(edd[\"obs_list\"][\"rgb\"])\n",
    "rgb_obs_list_flat = rgb_obs_list.reshape(len(rgb_obs_list), -1)\n",
    "data = rsatoolbox.data.Dataset(\n",
    "    rgb_obs_list_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDM\n",
    "rdm = rsatoolbox.rdm.calc_rdm(data); rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsatoolbox.vis.rdm_plot.show_rdm(rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All trajs for the same category and same room\n",
    "def get_catname_scene__rgb_obs_list_flat(analysis_trajs_dict, catname, scene_id):\n",
    "    rgb_obs_list = []\n",
    "    for cat_scene_traj_dict in analysis_trajs_dict[catname][scene_id]:\n",
    "        edd = cat_scene_traj_dict[\"edd\"]\n",
    "        for rgb_obs in edd[\"obs_list\"][\"rgb\"]:\n",
    "            rgb_obs_list.append(rgb_obs)\n",
    "    \n",
    "    return np.array(rgb_obs_list).reshape(len(rgb_obs_list), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_7y3sRwLe3Va_alltrajs_rgb_obs_list_flat = \\\n",
    "    get_catname_scene__rgb_obs_list_flat(analysis_trajs_dict, \"chair\", \"7y3sRwLe3Va\")\n",
    "rsa_dataset = rsatoolbox.data.Dataset(chair_7y3sRwLe3Va_alltrajs_rgb_obs_list_flat)\n",
    "rdm = rsatoolbox.rdm.calc_rdm(rsa_dataset)\n",
    "\n",
    "rsatoolbox.vis.rdm_plot.show_rdm(rdm, figsize=(16, 16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
