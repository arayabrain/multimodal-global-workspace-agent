{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "# Prototyping datsaet reading for the BC experimetns\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "# General config related\n",
    "from configurator import get_arg_dict, generate_args\n",
    "\n",
    "# Env config related\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/audiogoal_rgb_nocont.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.2), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"deep-etho\",\n",
    "                    \"perceiver-gwt-gwwm\", \"perceiver-gwt-attgru\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, True, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# TODO: make it not require the environemtn instantiation\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "env_config.NUM_PROCESSES = args.num_envs # Corresponds to number of envs, makes script startup faster for debugs\n",
    "# env_config.USE_SYNC_VECENV = True\n",
    "# env_config.USE_VECENV = False\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "env_config.freeze()\n",
    "# print(env_config)\n",
    "\n",
    "# Environment instantiation\n",
    "# envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "# single_observation_space = envs.observation_spaces[0]\n",
    "# single_action_space = envs.action_spaces[0]\n",
    "\n",
    "# single_observation_space, single_action_space\n",
    "\n",
    "# # Loading pretrained agent\n",
    "# import models\n",
    "# from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR_PATH = f\"ppo_gru_dset_2022_09_21__1000000_STEPS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_list dict:\n",
      "\t rgb -> (34, 128, 128, 3); type: <class 'list'>\n",
      "\t audiogoal -> (34, 2, 16000); type: <class 'list'>\n",
      "\t spectrogram -> (34, 65, 26, 2); type: <class 'list'>\n",
      "action_list -> (34, 1); type: list\n",
      "done_list -> (34,); type: list\n",
      "reward_list -> (34,); type: list\n",
      "info_list -> (34,); type: list\n",
      "ep_length -> 34; type: int\n"
     ]
    }
   ],
   "source": [
    "ep_filenames = os.listdir(DATASET_DIR_PATH)\n",
    "len(ep_filenames)\n",
    "ep_filename = np.random.choice(ep_filenames)\n",
    "\n",
    "ep_filepath = os.path.join(DATASET_DIR_PATH, ep_filename)\n",
    "with open(ep_filepath, \"rb\") as f:\n",
    "    ep_data_dict = cpkl.load(f)\n",
    "ep_data_dict.keys()\n",
    "\n",
    "for k, v in ep_data_dict.items():\n",
    "    if isinstance(v, dict):\n",
    "        print(f\"{k} dict:\")\n",
    "        for kk, vv in v.items():\n",
    "            print(f\"\\t {kk} -> {np.shape(vv)}; type: {type(vv)}\")\n",
    "    elif isinstance(v, list):\n",
    "        print(f\"{k} -> {np.shape(v)}; type: list\")\n",
    "    elif isinstance(v, int):\n",
    "        print(f\"{k} -> {v}; type: int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "## Shape of the dave ep_data_dict:, for reference\n",
    "# obs_list dict:\n",
    "# \t rgb -> (94, 128, 128, 3)\n",
    "# \t audiogoal -> (94, 2, 16000)\n",
    "# \t spectrogram -> (94, 65, 26, 2)\n",
    "# action_list -> (94, 1)\n",
    "# done_list -> (94,)\n",
    "# reward_list -> (94,)\n",
    "# info_list -> (94,)\n",
    "# ep_length -> 94\n",
    "\n",
    "class BCIterableDataset(IterableDataset):\n",
    "    def __init__(self, dataset_path, batch_length, seed=111):\n",
    "        self.seed = seed\n",
    "        self.batch_length = batch_length\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        # Read episode filenames in the dataset path\n",
    "        self.ep_filenames = os.listdir(dataset_path)\n",
    "        print(f\"Initialized IterDset with {len(self.ep_filenames)} episodes.\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch_length = self.batch_length\n",
    "        while True:\n",
    "            # Sample epsiode data until there is enough for one trajectory\n",
    "            # Hardcoded for now, make flexible later\n",
    "            # Done later to recover the \n",
    "            obs_list = {\n",
    "                \"rgb\": np.zeros([batch_length, 128, 128, 3]),\n",
    "                \"audiogoal\": np.zeros([batch_length, 2, 16000]),\n",
    "                \"spectrogram\": np.zeros([batch_length, 65, 26, 2])\n",
    "            }\n",
    "            action_list, reward_list, done_list = \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1])\n",
    "            \n",
    "            ssf = 0 # Step affected so far\n",
    "            while ssf < batch_length:\n",
    "                idx = th.randint(len(self.ep_filenames), ())\n",
    "                print(f\"Sampled traj idx: {idx}\")\n",
    "                ep_filename = self.ep_filenames[idx]\n",
    "                ep_filepath = os.path.join(DATASET_DIR_PATH, ep_filename)\n",
    "                with open(ep_filepath, \"rb\") as f:\n",
    "                    edd = cpkl.load(f)\n",
    "\n",
    "                # Append the data to the bathc trjectory\n",
    "                rs = batch_length - ssf # Reamining steps\n",
    "                horizon = ssf + min(rs, edd[\"ep_length\"])\n",
    "\n",
    "                for k, v in edd[\"obs_list\"].items():\n",
    "                    obs_list[k][ssf:horizon] = v[:rs]\n",
    "                action_list[ssf:horizon] = edd[\"action_list\"][:rs]\n",
    "                reward_list[ssf:horizon] = np.array(edd[\"reward_list\"][:rs])[:, None]\n",
    "                done_list[ssf:horizon] = np.array(edd[\"done_list\"][:rs])[:, None]\n",
    "\n",
    "                ssf += edd[\"ep_length\"]\n",
    "\n",
    "                if ssf >= self.batch_length:\n",
    "                    break\n",
    "\n",
    "            yield obs_list, action_list, reward_list, done_list\n",
    "    \n",
    "def make_dataloader(dataset_path, batch_size, batch_length, seed=111, num_workers=4):\n",
    "    def worker_init_fn(worker_id):\n",
    "        # worker_seed = th.initial_seed() % (2 ** 32)\n",
    "        worker_seed = 133754134 + worker_id\n",
    "\n",
    "        random.seed(worker_seed)\n",
    "        np.random.seed(worker_seed)\n",
    "\n",
    "    th_seed_gen = th.Generator()\n",
    "    th_seed_gen.manual_seed(133754134 + seed)\n",
    "\n",
    "    dloader = iter(\n",
    "        DataLoader(\n",
    "            BCIterableDataset(\n",
    "                dataset_path=dataset_path, batch_length=batch_length),\n",
    "                batch_size=batch_size, num_workers=num_workers,\n",
    "                worker_init_fn=worker_init_fn, generator=th_seed_gen\n",
    "            )\n",
    "    )\n",
    "\n",
    "    return dloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized IterDset with 4000 episodes.\n",
      "Sampled traj idx: 74\n",
      "Sampled traj idx: 2985\n",
      "Sampled traj idx: 1244\n",
      "Sampled traj idx: 3641\n",
      "Sampled traj idx: 739\n",
      "Sampled traj idx: 3929\n",
      "Sampled traj idx: 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled traj idx: 3497\n",
      "Sampled traj idx: 872\n",
      "Sampled traj idx: 766\n",
      "Sampled traj idx: 3926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dloader = make_dataloader(DATASET_DIR_PATH, batch_size=1, batch_length=30)\n",
    "for _ in range(2):\n",
    "    obs_batch, action_batch, reward_batch, done_batch = next(dloader)\n",
    "\n",
    "# obs_batch[\"rgb\"].shape, obs_batch[\"spectrogram\"].shape\n",
    "# done_batch[0][..., 0].tolist()\n",
    "# reward_batch[0][..., 0].tolist()\n",
    "# obs_batch[\"rgb\"][0][0, 0, 0]\n",
    "# action_batch[0][..., 0].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
