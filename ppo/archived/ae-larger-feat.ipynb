{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import apex\n",
    "\n",
    "from collections import deque\n",
    "from torchinfo import summary\n",
    "\n",
    "import tools\n",
    "from configurator import generate_args, get_arg_dict\n",
    "from th_logger import TBXLogger as TBLogger\n",
    "\n",
    "# Env deps: Soundspaces and Habitat\n",
    "from habitat.datasets import make_dataset\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "from ss_baselines.common.utils import images_to_video_with_audio\n",
    "\n",
    "# Custom ActorCritic agent for PPO\n",
    "from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic\n",
    "\n",
    "# Dataset utils\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "    \n",
    "    # Behavior cloning gexperiment config\n",
    "    get_arg_dict(\"dataset-path\", str, \"SAVI_Oracle_Dataset_v0\"),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/savi/savi_ss1.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.0), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"perceiver-gwt-gwwm\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, False, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    ## Special BC\n",
    "    get_arg_dict(\"prev-actions\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"burn-in\", int, 0), # Steps used to init the latent state for RNN component\n",
    "    get_arg_dict(\"batch-chunk-length\", int, 0), # For gradient accumulation\n",
    "    get_arg_dict(\"dataset-ce-weights\", bool, True, metatype=\"bool\"), # If True, will read CEL weights based on action dist. from the 'dataset_statistics.bz2' file.\n",
    "    get_arg_dict(\"ce-weights\", float, None, metatype=\"list\"), # Weights for the Cross Entropy loss\n",
    "\n",
    "    ## SSL Support\n",
    "    get_arg_dict(\"obs-center\", bool, False, metatype=\"bool\"), # Centers the rgb_observations' range to [-0.5,0.5]\n",
    "    get_arg_dict(\"ssl-tasks\", str, None, metatype=\"list\"), # Expects something like [\"rec-rgb-vis\", \"rec-depth\", \"rec-spectr\"]\n",
    "    get_arg_dict(\"ssl-task-coefs\", float, None, metatype=\"list\"), # For each ssl-task, specifies the loss coeff. during computation\n",
    "\n",
    "    # Eval protocol\n",
    "    get_arg_dict(\"eval\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"eval-every\", int, int(1.5e4)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"eval-n-episodes\", int, 5),\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Gradient accumulation support\n",
    "if args.batch_chunk_length == 0:\n",
    "    args.batch_chunk_length = args.num_envs\n",
    "\n",
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "th.cuda.manual_seed_all(args.seed)\n",
    "th.backends.cudnn.deterministic = args.torch_deterministic\n",
    "# th.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "# Set device as GPU\n",
    "device = tools.get_device(args) if (not args.cpu and th.cuda.is_available()) else th.device(\"cpu\")\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "## Override default seed\n",
    "env_config.SEED = env_config.TASK_CONFIG.SEED = env_config.TASK_CONFIG.SIMULATOR.SEED = args.seed\n",
    "\n",
    "env_config.TASK_CONFIG.SIMULATOR.USE_RENDERED_OBSERVATIONS = False\n",
    "# For smoother video, set CONTINUOUS_VIEW_CHANGE to True, and get the additional frames in obs_dict[\"intermediate\"]\n",
    "env_config.TASK_CONFIG.SIMULATOR.CONTINUOUS_VIEW_CHANGE = False\n",
    "\n",
    "env_config.TASK_CONFIG.SIMULATOR.RGB_SENSOR.WIDTH = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.RGB_SENSOR.HEIGHT = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.DEPTH_SENSOR.WIDTH = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.DEPTH_SENSOR.HEIGHT = 256\n",
    "\n",
    "# NOTE: using less environments for eval to save up system memory -> run more experiment at the same time\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "agent_extra_rgb = False\n",
    "if args.save_videos:\n",
    "    # For RGB video sensors\n",
    "    if \"RGB_SENSOR\" not in env_config.SENSORS:\n",
    "        env_config.SENSORS.append(\"RGB_SENSOR\")\n",
    "        # Indicates to the agent that RGB obs should not be used as observational inputs\n",
    "        agent_extra_rgb = True\n",
    "    # For Waveform to generate audio over the videos\n",
    "    if \"AUDIOGOAL_SENSOR\" not in env_config.TASK_CONFIG.TASK.SENSORS:\n",
    "        env_config.TASK_CONFIG.TASK.SENSORS.append(\"AUDIOGOAL_SENSOR\")\n",
    "# Add support for TOP_DOWN_MAP\n",
    "# NOTE: it seems to induce \"'DummySimulator' object has no attribute 'pathfinder'\" error\n",
    "# If top down map really needed, probably have to run the env without pre-rendered observations ?\n",
    "# env_config.TASK_CONFIG.TASK.MEASUREMENTS.append(\"TOP_DOWN_MAP\")\n",
    "\n",
    "env_config.freeze()\n",
    "\n",
    "# Environment instantiation\n",
    "if args.eval:\n",
    "    # In case there is need for eval, instantiate some environments\n",
    "    envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "    single_observation_space = envs.observation_spaces[0]\n",
    "    single_action_space = envs.action_spaces[0]\n",
    "else:\n",
    "    # Otherwise, just use dummy obs. and act. spaces for agent structure init\n",
    "    from gym import spaces\n",
    "    single_action_space = spaces.Discrete(4)\n",
    "    single_observation_space = spaces.Dict({\n",
    "        # \"rgb\": spaces.Box(shape=[128,128,3], low=0, high=255, dtype=np.uint8),\n",
    "        # \"depth\": spaces.Box(shape=[128,128,1], low=0, high=255, dtype=np.uint8),\n",
    "        \"audiogoal\": spaces.Box(shape=[2,16000], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32),\n",
    "        \"spectrogram\": spaces.Box(shape=[65,26,2], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32)\n",
    "    })\n",
    "\n",
    "# Override the observation space for \"rgb\" and \"depth\" from (256,256,C) to (128,128,C)\n",
    "from gym import spaces\n",
    "if \"RGB_SENSOR\" in env_config.SENSORS:\n",
    "    single_observation_space[\"rgb\"] = spaces.Box(shape=[128,128,3], low=0, high=255, dtype=np.uint8)\n",
    "if \"DEPTH_SENSOR\" in env_config.SENSORS:\n",
    "    single_observation_space[\"rgb\"] = spaces.Box(shape=[128,128,1], low=0, high=255, dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(audiogoal:Box(-3.4028235e+38, 3.4028235e+38, (2, 16000), float32), spectrogram:Box(-3.4028235e+38, 3.4028235e+38, (65, 26, 2), float32), rgb:Box(0, 255, (128, 128, 1), uint8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualCNN4(nn.Module):\n",
    "    def __init__(self, obsjervation_space, output_size, extra_rgb, obs_center=False):\n",
    "        super().__init__()\n",
    "        if \"rgb\" in observation_space.spaces and not extra_rgb:\n",
    "            self._n_input_rgb = observation_space.spaces[\"rgb\"].shape[2]\n",
    "        else:\n",
    "            self._n_input_rgb = 0\n",
    "\n",
    "        if \"depth\" in observation_space.spaces:\n",
    "            self._n_input_depth = observation_space.spaces[\"depth\"].shape[2]\n",
    "        else:\n",
    "            self._n_input_depth = 0\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.obs_center = obs_center\n",
    "\n",
    "        # kernel size for different CNN layers\n",
    "        self._cnn_layers_kernel_size = [(8, 8), (4, 4), (3, 3)]\n",
    "\n",
    "        # strides for different CNN layers\n",
    "        self._cnn_layers_stride = [(4, 4), (2, 2), (2, 2)]\n",
    "\n",
    "        if self._n_input_rgb > 0:\n",
    "            cnn_dims = np.array(\n",
    "                observation_space.spaces[\"rgb\"].shape[:2], dtype=np.float32\n",
    "            )\n",
    "        elif self._n_input_depth > 0:\n",
    "            cnn_dims = np.array(\n",
    "                observation_space.spaces[\"depth\"].shape[:2], dtype=np.float32\n",
    "            )\n",
    "\n",
    "        if self.is_blind:\n",
    "            self.cnn = nn.Sequential()\n",
    "        else:\n",
    "            for kernel_size, stride in zip(\n",
    "                self._cnn_layers_kernel_size, self._cnn_layers_stride\n",
    "            ):\n",
    "                self.cnn_dims = cnn_dims = conv_output_dim(\n",
    "                    dimension=cnn_dims,\n",
    "                    padding=np.array([0, 0], dtype=np.float32),\n",
    "                    dilation=np.array([1, 1], dtype=np.float32),\n",
    "                    kernel_size=np.array(kernel_size, dtype=np.float32),\n",
    "                    stride=np.array(stride, dtype=np.float32),\n",
    "                )\n",
    "\n",
    "            self.cnn = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self._n_input_rgb + self._n_input_depth,\n",
    "                    out_channels=32,\n",
    "                    kernel_size=self._cnn_layers_kernel_size[0],\n",
    "                    stride=self._cnn_layers_stride[0],\n",
    "                ),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(\n",
    "                    in_channels=32,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self._cnn_layers_kernel_size[1],\n",
    "                    stride=self._cnn_layers_stride[1],\n",
    "                ),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self._cnn_layers_kernel_size[2],\n",
    "                    stride=self._cnn_layers_stride[2],\n",
    "                ),\n",
    "                #  nn.ReLU(True),\n",
    "                Flatten(),\n",
    "                nn.Linear(64 * cnn_dims[0] * cnn_dims[1], 2048),\n",
    "                nn.ReLU(True),\n",
    "            )\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(2048, output_size),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        layer_init(self.cnn)\n",
    "\n",
    "    @property\n",
    "    def is_blind(self):\n",
    "        return self._n_input_rgb + self._n_input_depth == 0\n",
    "\n",
    "    def forward(self, observations):\n",
    "        cnn_input = []\n",
    "        if self._n_input_rgb > 0:\n",
    "            rgb_observations = observations[\"rgb\"]\n",
    "            # permute tensor to dimension [BATCH x CHANNEL x HEIGHT X WIDTH]\n",
    "            rgb_observations = rgb_observations.permute(0, 3, 1, 2)\n",
    "            rgb_observations = rgb_observations / 255.0  # normalize RGB\n",
    "            if self.obs_center:\n",
    "                rgb_observations -= 0.5\n",
    "            cnn_input.append(rgb_observations)\n",
    "\n",
    "        if self._n_input_depth > 0:\n",
    "            depth_observations = observations[\"depth\"]\n",
    "            # permute tensor to dimension [BATCH x CHANNEL x HEIGHT X WIDTH]\n",
    "            depth_observations = depth_observations.permute(0, 3, 1, 2)\n",
    "            if self.obs_center:\n",
    "                depth_observations -= 0.5\n",
    "            cnn_input.append(depth_observations)\n",
    "\n",
    "        cnn_input = th.cat(cnn_input, dim=1)\n",
    "\n",
    "        cnn_output = self.cnn(cnn_input)\n",
    "        features = self.linear(cnn_output)\n",
    "\n",
    "        return features, cnn_output\n",
    "\n",
    "encoder = VisualCNN4(single_observation_space, args.hidden_size, extra_rgb=False, obs_center=args.obs_center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
