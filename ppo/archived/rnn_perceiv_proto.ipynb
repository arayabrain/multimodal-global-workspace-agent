{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents\n",
      "ca.mha.q_proj_weight\n",
      "ca.mha.k_proj_weight\n",
      "ca.mha.v_proj_weight\n",
      "ca.mha.in_proj_bias\n",
      "ca.mha.out_proj.weight\n",
      "ca.mha.out_proj.bias\n",
      "ca.ln_q.weight\n",
      "ca.ln_q.bias\n",
      "ca.ln_kv.weight\n",
      "ca.ln_kv.bias\n",
      "ca.ff_self.0.weight\n",
      "ca.ff_self.0.bias\n",
      "ca.ff_self.1.weight\n",
      "ca.ff_self.1.bias\n",
      "ca.ff_self.3.weight\n",
      "ca.ff_self.3.bias\n",
      "sa.mha.in_proj_weight\n",
      "sa.mha.in_proj_bias\n",
      "sa.mha.out_proj.weight\n",
      "sa.mha.out_proj.bias\n",
      "sa.ln.weight\n",
      "sa.ln.bias\n",
      "sa.ff_self.0.weight\n",
      "sa.ff_self.0.bias\n",
      "sa.ff_self.1.weight\n",
      "sa.ff_self.1.bias\n",
      "sa.ff_self.3.weight\n",
      "sa.ff_self.3.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "STR_TO_ACTIVATION = {\n",
    "    \"identity\": nn.Identity(),\n",
    "    \"tanh\": nn.Tanh()\n",
    "}\n",
    "\n",
    "# Helper classes\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size, n_heads=4, ff_act=nn.Identity()):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_act = ff_act\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            h_size, n_heads, dropout=0.0, add_zero_attn=False, batch_first=True\n",
    "        )\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            ff_act\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, attn_weighting = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        # NOTE / TODO: Even with Than as final activation, the res. connection\n",
    "        # will make the final range to [-2, 2]\n",
    "        attention_value = self.ff_self(attention_value) * 0.5 + attention_value * 0.5\n",
    "        return attention_value, attn_weighting\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, q_size, kv_size, n_heads=4, ff_act=nn.Identity(), skip_q=False):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.h_size = q_size\n",
    "        self.skip_q = skip_q\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_act = ff_act\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            q_size,\n",
    "            n_heads,\n",
    "            dropout=0.0,\n",
    "            add_zero_attn=False,\n",
    "            batch_first=True,\n",
    "            kdim=kv_size,\n",
    "            vdim=kv_size,\n",
    "        )\n",
    "        self.ln_q = nn.LayerNorm([q_size])\n",
    "        self.ln_kv = nn.LayerNorm([kv_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([q_size]),\n",
    "            nn.Linear(q_size, q_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(q_size, q_size),\n",
    "            ff_act\n",
    "        )\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        q_ln = self.ln_q(q)\n",
    "        kv_ln = self.ln_kv(kv)\n",
    "        attention_value, attn_weighting = self.mha(q_ln, kv_ln, kv_ln)\n",
    "        if not self.skip_q:\n",
    "            attention_value = attention_value + q\n",
    "        # NOTE / TODO: Even with Than as final activation, the res. connection\n",
    "        # will make the final range to [-2, 2]\n",
    "        attention_value = self.ff_self(attention_value) * 0.5 + attention_value * 0.5\n",
    "        return attention_value, attn_weighting\n",
    "\n",
    "# Main class(es)\n",
    "class Perceiver_GWT_GWWM_WeightNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        input_dim,\n",
    "        latent_type = \"randn\",\n",
    "        latent_learned = True,\n",
    "        num_latents = 8,\n",
    "        latent_dim = 64,\n",
    "        cross_heads = 1, # LucidRains's implm. uses 1 by default\n",
    "        latent_heads = 4, # LucidRains's implm. uses 8 by default\n",
    "        # cross_dim_head = 64,\n",
    "        # latent_dim_head = 64,\n",
    "        # self_per_cross_attn = 1, # Number of self attention blocks per cross attn.\n",
    "        # Modality embeddings realted\n",
    "        hidden_size = 512, # Dim of the visual / audio encoder outputs\n",
    "        mod_embed = 0, # Dimensio of learned modality embeddings\n",
    "        use_sa = True,\n",
    "        ca_ff_act = \"identity\",\n",
    "        sa_ff_act = \"identity\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_latents = num_latents # N\n",
    "        self.latent_dim = latent_dim # D\n",
    "        self.latent_type = latent_type\n",
    "        self.latent_learned = latent_learned\n",
    "\n",
    "        self.mod_embed = mod_embed\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_sa = use_sa\n",
    "        self.ca_ff_act = ca_ff_act\n",
    "        self.sa_ff_act = sa_ff_act\n",
    "\n",
    "        # Cross Attention\n",
    "        self.ca = CrossAttention(num_latents * latent_dim, input_dim + mod_embed * 2,\n",
    "            n_heads=cross_heads, skip_q=False, ff_act=STR_TO_ACTIVATION[ca_ff_act]) # skip_q if not using SA\n",
    "        # Self Attention\n",
    "        if self.use_sa:\n",
    "            self.sa = SelfAttention(num_latents * latent_dim, n_heads=latent_heads,\n",
    "                ff_act=STR_TO_ACTIVATION[ca_ff_act])\n",
    "        # self.decoder = CrossAttention(self.h_size, self.s_size, skip_q=True)\n",
    "\n",
    "        # Modality embedding\n",
    "        if self.mod_embed:\n",
    "            self.modality_embeddings = nn.Parameter(0.1 * torch.randn(1, mod_embed * 2))\n",
    "        \n",
    "        # Latent vector, supposedly equivalent to an RNN's hidden state\n",
    "        if latent_type == \"randn\":\n",
    "            self.latents = torch.randn(num_latents, latent_dim)\n",
    "            # As per original paper\n",
    "            with th.no_grad():\n",
    "                self.latents.normal_(0.0, 0.02).clamp_(-2.0,2.0)\n",
    "        elif latent_type == \"zeros\":\n",
    "            self.latents = torch.zeros(num_latents, latent_dim)\n",
    "        \n",
    "        self.latents = nn.Parameter(self.latents, requires_grad=latent_learned)\n",
    "\n",
    "    def seq_forward(self, data, prev_latents, masks):\n",
    "        # TODO: a more optimal method to process sequences of same length together ?\n",
    "        x_list, latents_list = [], []\n",
    "\n",
    "        B_T, feat_dim = data.shape\n",
    "        B = prev_latents.shape[0]\n",
    "        T = B_T // B # TODO: assert that B * T == B_T\n",
    "        latents = prev_latents.clone()\n",
    "\n",
    "        data = data.reshape(B, T, feat_dim)\n",
    "        masks = masks.reshape(B, T, 1)\n",
    "\n",
    "        for t in range(T):\n",
    "            x, latents = self.single_forward(data[:, t], latents, masks[:, t])\n",
    "\n",
    "            x_list.append(x)\n",
    "            latents_list.append(latents)\n",
    "        \n",
    "        # TODO: debug\n",
    "        x_list = th.stack(x_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, feat_dim]\n",
    "        latents_list = th.stack(latents_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, num_latents, latent_dim]\n",
    "\n",
    "        return x_list, latents_list\n",
    "\n",
    "    def single_forward(self, data, prev_latents, masks):\n",
    "        if self.mod_embed:\n",
    "            b = data.shape[0]\n",
    "            data = th.cat([\n",
    "                data[:, :self.hidden_size], self.modality_embeddings[:, :self.mod_embed].repeat(b, 1), # Audio feats and embeddings\n",
    "                data[:, self.hidden_size:], self.modality_embeddings[:, self.mod_embed:].repeat(b, 1), # Video feats and embeddings\n",
    "            ], dim=-1)\n",
    "\n",
    "        b, device, dtype = data.shape[0], data.device, data.dtype\n",
    "        \n",
    "        # If the current step is the start of a new episode,\n",
    "        # the the mask will contain 0\n",
    "        prev_latents = masks[:, :, None] * prev_latents + \\\n",
    "            (1. - masks[:, :, None]) * repeat(self.latents, 'n d -> b n d', b = b)\n",
    "\n",
    "        x = prev_latents.flatten(start_dim=1) # [B, N, D] -> [B, N * D]\n",
    "        \n",
    "        # Cross Attention\n",
    "        x, _ = self.ca(x, data) # x: [B, N * D], x_weights: [B, ???]\n",
    "\n",
    "        # Self Attention\n",
    "        if self.use_sa:\n",
    "            x, _ = self.sa(x) # x: [B, N * D]\n",
    "\n",
    "        return x, x.view(b, self.num_latents, self.latent_dim)\n",
    "\n",
    "    def forward(self, data, prev_latents, masks):\n",
    "        \"\"\"\n",
    "            - data: observation features [NUM_ENVS, feat_dim] or [NUM_ENVS, NUM_STEPS, feat_dim]\n",
    "            - prev_latents: previous latents [B, num_latents, latent_dim]\n",
    "            - masks: not Perceiver mask, but end-of-episode signaling mask\n",
    "                - shape of [NUM_ENVS, 1] if single step forward\n",
    "                - shape of [NUM_ENVS, NUM_STEPS, 1] if sequence forward\n",
    "        \"\"\"\n",
    "        if data.size(0) == prev_latents.size(0):\n",
    "            return self.single_forward(data, prev_latents, masks)\n",
    "        else:\n",
    "            return self.seq_forward(data, prev_latents, masks)\n",
    "\n",
    "class WeightNormParametrization(nn.Module):\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "model = Perceiver_GWT_GWWM(input_dim = 1024)\n",
    "for c, _ in model.named_parameters():\n",
    "    print(c)\n",
    "# for k, v in model.named_parameters():\n",
    "#     if \"weight\" in k:\n",
    "#         th.nn.utils.weight_norm(v, name=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t == 0\n",
    "prev_latents = model.latents.repeat(5, 1, 1)\n",
    "masks = th.ones([5, 1])\n",
    "obs_feat = th.randn(5, 1024)\n",
    "state_feat, latents = model(obs_feat, prev_latents, masks)\n",
    "state_feat.shape, latents.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
