{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trajectory extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify trajectory by scene -> category -> index\n",
    "# obs_dict_list, done_list = \\\n",
    "#     get_traj_data_by_scene_category_trajIdx(scenes_cats_trajs_dict, \"D7N2EKCX4Sj\", \"chair\", 0, tensorize=True)\n",
    "\n",
    "# Specify trajectories by scene, then category\n",
    "# obs_dict_list, done_list = \\\n",
    "#     get_traj_data_by_scene_category(scenes_cats_trajs_dict, \"D7N2EKCX4Sj\", \"chair\", tensorize=True)\n",
    "\n",
    "# All trajectories by scene, for all categories\n",
    "# obs_dict_list, done_list = \\\n",
    "#     get_traj_data_by_scene(scenes_cats_trajs_dict, \"D7N2EKCX4Sj\", tensorize=True)\n",
    "\n",
    "# region: D7N2EKCX4Sj, all categories, all trajectories\n",
    "# D7N2EKCX4Sj_allcats_alltrajs_obs_dict_list, \\\n",
    "# D7N2EKCX4Sj_allcats_alltrajs_done_list, \\\n",
    "# D7N2EKCX4Sj_allcats_alltrajs_target_scene_idx_list, \\\n",
    "# D7N2EKCX4Sj_allcats_alltrajs_target_category_idx_list = \\\n",
    "#         get_traj_data_by_scene(scenes_cats_trajs_dict, \"D7N2EKCX4Sj\", tensorize=True)\n",
    "\n",
    "# # Special sections will override \"obs_dict_list\" and \"done_list\"\n",
    "# obs_dict_list, done_list = \\\n",
    "#         D7N2EKCX4Sj_allcats_alltrajs_obs_dict_list, \\\n",
    "#         D7N2EKCX4Sj_allcats_alltrajs_done_list\n",
    "\n",
    "# # The following three should all be equal: one target for scene / category for each step\n",
    "# len(D7N2EKCX4Sj_allcats_alltrajs_obs_dict_list), len(D7N2EKCX4Sj_allcats_alltrajs_target_scene_idx_list), len(D7N2EKCX4Sj_allcats_alltrajs_target_category_idx_list)\n",
    "# endregion: D7N2EKCX4Sj, all categories, all trajectories\n",
    "\n",
    "\n",
    "# print(f\"done_list shape: {np.shape(done_list)}\")\n",
    "# print(len(obs_dict_list))\n",
    "\n",
    "# print(np.array(done_list).astype(np.uint8))\n",
    "# for k, v in obs_dict_list.items():\n",
    "#     print(f\"{k}: {np.shape(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process agent's features (revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Helper for cleaning up and preparing the recorded intermediate features\n",
    "# def process_analysis_feats_raw(raw_dict):\n",
    "#     result_dict = {}\n",
    "\n",
    "#     for k, v in raw_dict.items():\n",
    "#         if isinstance(v[0], th.Tensor):\n",
    "#             new_v = th.cat(v, dim=0)\n",
    "#         elif isinstance(v[0], tuple):\n",
    "#             new_v = None # TODO\n",
    "#             n_elements = len(v[0])\n",
    "#             elements = [[] for _ in range(n_elements)]\n",
    "#             for j in range(n_elements):\n",
    "#                 for i in range(len(v)):\n",
    "#                     elements[j].append(v[i][j])\n",
    "            \n",
    "#             new_v = [th.cat(vv, dim=0).cpu().numpy() for vv in elements]\n",
    "#         else:\n",
    "#             raise Exception(f\"Unhandled type: {v[0].__class__}\")\n",
    "    \n",
    "#         result_dict[k] = new_v\n",
    "    \n",
    "#     return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the collected observation samples, record the intermediate features\n",
    "# # for each agent varaint investigated\n",
    "\n",
    "# AGENT_FEATURES__RAW = {k: {} for k in MODEL_VARIANTS_TO_AGENTMODEL.keys()}\n",
    "# AGENT_RNN_HIDDEN_STATE = {}\n",
    "# for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "#     if agent_variant.__contains__(\"gru\"):\n",
    "#         AGENT_RNN_HIDDEN_STATE[agent_variant] = th.zeros((1, args.num_envs, args.hidden_size), device=dev)\n",
    "#     elif agent_variant.__contains__(\"pgwt\"):\n",
    "#         AGENT_RNN_HIDDEN_STATE[agent_variant] = agent_model.state_encoder.latents.clone()\n",
    "\n",
    "# # TODO: Add tqdm support ?\n",
    "# # for t, (obs_th, done_th) in enumerate(zip(obs_th_list, done_th_list)):\n",
    "# for t, (obs_th, done_th) in enumerate(zip(obs_dict_list, done_list)):\n",
    "#     # TODO: if multiple episodes are cated together, need to make sure that the done_th_list reflects that\n",
    "#     # other the hidden latents will not be reset\n",
    "\n",
    "#     # Feeding the same sequence of observatiosn to each type of agent\n",
    "#     masks = 1. - done_th[:, None]\n",
    "#     with th.no_grad():\n",
    "#         for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "#             # NOTE: rnn_hidden_state or pgwt's latent are already collected in \"state_encoder\" field in _features\n",
    "#             _, _, _, _, _, _, AGENT_RNN_HIDDEN_STATE[agent_variant] = \\\n",
    "#                 agent_model.act(obs_th, AGENT_RNN_HIDDEN_STATE[agent_variant], masks)\n",
    "\n",
    "#             # Collecting intermediate layers results\n",
    "#             for k, v in agent_model._features.items():\n",
    "#                 if k not in list(AGENT_FEATURES__RAW[agent_variant].keys()):\n",
    "#                     AGENT_FEATURES__RAW[agent_variant][k] = []\n",
    "#                 AGENT_FEATURES__RAW[agent_variant][k].append(v)\n",
    "\n",
    "# # After recording the intermediate layers features, process them to handle the \n",
    "# # various shape depending on the layers: for example MHA has different stored features\n",
    "# # shape than GRU network, and so on.\n",
    "\n",
    "# AGENT_FEATURES = {} # { agent_variant -> agent_feature_dict }\n",
    "# for k, v in AGENT_FEATURES__RAW.items():\n",
    "#     AGENT_FEATURES[k] = process_analysis_feats_raw(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematic plotting for each shared intermediate layers\n",
    "all_layers = []\n",
    "for v in AGENT_FEATURES.values():\n",
    "    all_layers.extend(v.keys())\n",
    "all_layers = set(all_layers); all_layers\n",
    "\n",
    "# Extract shared layers, and other groups of layers of interest\n",
    "shared_layers = []\n",
    "visual_encoder_layernames = []\n",
    "audio_encoder_layernames = []\n",
    "for x in all_layers:\n",
    "    x_found_in_all = True\n",
    "    for v in AGENT_FEATURES.values():\n",
    "        if x not in v.keys():\n",
    "            x_found_in_all = False\n",
    "            break\n",
    "    \n",
    "    if x_found_in_all:\n",
    "        shared_layers.append(x)\n",
    "    \n",
    "    if x.startswith(\"visual_encoder\"):\n",
    "        visual_encoder_layernames.append(x)\n",
    "    if x.startswith(\"audio_encoder\"):\n",
    "        audio_encoder_layernames.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache cat -> scene -> traj -> ablation type -> agent variant -> features data into independent files\n",
    "# to facilitate read into memory later\n",
    "# Keeping everything in memroy takes just too much\n",
    "CACHE_DIRNAME = \"cats_scenes_trajs_C_6_M_5_N_5__2023_06_01_10_41__ablation_agents_features__cache\"\n",
    "\n",
    "# NOTE: Uncomment for cache to disk\n",
    "# if not os.path.exists(CACHE_DIRNAME):\n",
    "#     os.mkdir(CACHE_DIRNAME)\n",
    "\n",
    "# for catname, scene_trajs_ablations_agentvariant_features_dict in CAT_SCENE_TRAJS_ABLATION_AGENTVARIANT_FEATURES.items():\n",
    "#     cat_dirname = f\"{CACHE_DIRNAME}/{catname}\"\n",
    "#     if not os.path.exists(cat_dirname):\n",
    "#         os.mkdir(cat_dirname)\n",
    "    \n",
    "#     for scene, trajs_ablations_agentvariant_features_dict in scene_trajs_ablations_agentvariant_features_dict.items():\n",
    "#         scene_dirname = f\"{cat_dirname}/{scene}\"\n",
    "#         if not os.path.exists(scene_dirname):\n",
    "#             os.mkdir(scene_dirname)\n",
    "        \n",
    "#         for traj_idx, ablations_agentvariant_features_dict in trajs_ablations_agentvariant_features_dict.items():\n",
    "#             traj_dirname = f\"{scene_dirname}/{traj_idx}\"\n",
    "#             if not os.path.exists(traj_dirname):\n",
    "#                 os.mkdir(traj_dirname)\n",
    "            \n",
    "#             for ablation_type, agentvariant_features_dict in ablations_agentvariant_features_dict.items():\n",
    "#                 if ablation_type in [\"target_category_idx_list\", \"target_scene_idx_list\"]:\n",
    "#                     # Just saves those two fields as metadata\n",
    "#                     ablation_type_filename = f\"{traj_dirname}/{ablation_type}.bz2\"\n",
    "\n",
    "#                     # TODO: control on the existence of the file ?\n",
    "#                     with open(ablation_type_filename, \"wb\") as f:\n",
    "#                         # In this case, \"agentvariant_features_dict\" is just a list\n",
    "#                         cpkl.dump(agentvariant_features_dict, f)\n",
    "                    \n",
    "#                     # This will skip metadata about the trajectory, namely the labels for probes\n",
    "#                     continue\n",
    "                \n",
    "#                 ablation_dirname = f\"{traj_dirname}/{ablation_type}\"\n",
    "#                 if not os.path.exists(ablation_dirname):\n",
    "#                     os.mkdir(ablation_dirname)\n",
    "                \n",
    "#                 for agent_variant, features_dict in agentvariant_features_dict.items():\n",
    "#                     agent_variant_dirname = f\"{ablation_dirname}/{agent_variant}\"\n",
    "#                     if not os.path.exists(agent_variant_dirname):\n",
    "#                         os.mkdir(agent_variant_dirname)\n",
    "                    \n",
    "#                     feature_dict_filename = f\"{agent_variant_dirname}/features.bz2\"\n",
    "\n",
    "#                     if not os.path.exists(feature_dict_filename):\n",
    "#                         with open(feature_dict_filename, \"wb\") as f:\n",
    "#                             cpkl.dump(features_dict, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
