{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# Helper classes\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            h_size, 4, dropout=0.0, add_zero_attn=False, batch_first=True\n",
    "        )\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, attn_weighting = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value, attn_weighting\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, q_size, kv_size, skip_q=False):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.h_size = q_size\n",
    "        self.skip_q = skip_q\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            q_size,\n",
    "            4,\n",
    "            dropout=0.0,\n",
    "            add_zero_attn=False,\n",
    "            batch_first=True,\n",
    "            kdim=kv_size,\n",
    "            vdim=kv_size,\n",
    "        )\n",
    "        self.ln_q = nn.LayerNorm([q_size])\n",
    "        self.ln_kv = nn.LayerNorm([kv_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([q_size]),\n",
    "            nn.Linear(q_size, q_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(q_size, q_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        q_ln = self.ln_q(q)\n",
    "        kv_ln = self.ln_kv(kv)\n",
    "        attention_value, attn_weighting = self.mha(q_ln, kv_ln, kv_ln)\n",
    "        if not self.skip_q:\n",
    "            attention_value = attention_value + q\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value, attn_weighting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prototest the Cross Attention\n",
    "# ca = CrossAttention(8 * 64, 1024, skip_q=False) # skip_q if not using SA\n",
    "# ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latents = repeat(th.randn(8, 64), \"n d -> b n d\", b = 5)\n",
    "# obs_feat = th.randn(5, 1024)\n",
    "# masks = th.ones(5, 1)\n",
    "# outputs = ca(latents.flatten(start_dim=1), obs_feat)\n",
    "# outputs[0].shape, outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prototest the Self Attention\n",
    "# sa = SelfAttention(8 * 64)\n",
    "# sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, _ = sa(outputs[0])\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 512]), torch.Size([5, 8, 64]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main class(es)\n",
    "class Perceiver_GWT_GWWM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        input_dim,\n",
    "        latent_type = \"randn\",\n",
    "        latent_learned = True,\n",
    "        num_latents = 8,\n",
    "        latent_dim = 64,\n",
    "        cross_heads = 1,\n",
    "        latent_heads = 8,\n",
    "        cross_dim_head = 64,\n",
    "        latent_dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        self_per_cross_attn = 1, # Number of self attention blocks per cross attn.\n",
    "        weight_tie_layers = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_latents = num_latents # N\n",
    "        self.latent_dim = latent_dim # D\n",
    "\n",
    "        # Self Attention\n",
    "        self.sa = SelfAttention(num_latents * latent_dim)\n",
    "        # Cross Attention\n",
    "        self.ca = CrossAttention(num_latents * latent_dim, input_dim, skip_q=False) # skip_q if not using SA\n",
    "        # self.decoder = CrossAttention(self.h_size, self.s_size, skip_q=True)\n",
    "\n",
    "        # Latent vector, supposedly equivalent to an RNN's hidden state\n",
    "        if latent_type == \"randn\":\n",
    "            self.latents = torch.randn(num_latents, latent_dim)\n",
    "        elif latent_type == \"zeros\":\n",
    "            self.latents = torch.zeros(num_latents, latent_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported Perceiver Latent type: {latent_type}\")\n",
    "        \n",
    "        self.latents = nn.Parameter(self.latents, requires_grad=latent_learned)\n",
    "        # Special PerceiverWorkspace GWWM project\n",
    "        with th.no_grad():\n",
    "            self.latents.normal_(0.0, 0.02).clamp_(-2.0,2.0)\n",
    "\n",
    "    def seq_forward(self, data, prev_latents, masks):\n",
    "        # TODO: a more optimal method to process sequences of same length together ?\n",
    "        x_list, latents_list = [], []\n",
    "\n",
    "        B_T, feat_dim = data.shape\n",
    "        B = prev_latents.shape[0]\n",
    "        T = B_T // B # TODO: assert that B * T == B_T\n",
    "        latents = prev_latents.clone()\n",
    "\n",
    "        data = data.reshape(B, T, feat_dim)\n",
    "        masks = masks.reshape(B, T, 1)\n",
    "\n",
    "        for t in range(T):\n",
    "            x, latents = self.single_forward(data[:, t], latents, masks[:, t])\n",
    "\n",
    "            x_list.append(x)\n",
    "            latents_list.append(latents)\n",
    "        \n",
    "        # TODO: debug\n",
    "        x_list = th.stack(x_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, feat_dim]\n",
    "        latents_list = th.stack(latents_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, num_latents, latent_dim]\n",
    "\n",
    "        return x_list, latents_list\n",
    "\n",
    "    def single_forward(self, data, prev_latents, masks):\n",
    "        b, device, dtype = data.shape[0], data.device, data.dtype\n",
    "        \n",
    "        # If the current step is the start of a new episode,\n",
    "        # the the mask will contain 0\n",
    "        prev_latents = masks[:, :, None] * prev_latents + \\\n",
    "            (1. - masks[:, :, None]) * repeat(self.latents.clone(), 'n d -> b n d', b = b)\n",
    "\n",
    "        x = prev_latents.flatten(start_dim=1) # [B, N, D] -> [B, N * D]\n",
    "        \n",
    "        # Cross Attention\n",
    "        x, _ = self.ca(x, data) # x: [B, N * D], x_weights: [B, ???]\n",
    "\n",
    "        # Self Attention\n",
    "        x, _ = self.sa(x) # x: [B, N * D]\n",
    "\n",
    "        return x, x.view(b, self.num_latents, self.latent_dim)\n",
    "\n",
    "    def forward(self, data, prev_latents, masks):\n",
    "        \"\"\"\n",
    "            - data: observation features [NUM_ENVS, feat_dim] or [NUM_ENVS, NUM_STEPS, feat_dim]\n",
    "            - prev_latents: previous latents [B, num_latents, latent_dim]\n",
    "            - masks: not Perceiver mask, but end-of-episode signaling mask\n",
    "                - shape of [NUM_ENVS, 1] if single step forward\n",
    "                - shape of [NUM_ENVS, NUM_STEPS, 1] if sequence forward\n",
    "        \"\"\"\n",
    "        if data.size(0) == prev_latents.size(0):\n",
    "            return self.single_forward(data, prev_latents, masks)\n",
    "        else:\n",
    "            return self.seq_forward(data, prev_latents, masks)\n",
    "\n",
    "state_encoder = Perceiver_GWT_GWWM(\n",
    "    input_dim=1024,\n",
    ")\n",
    "\n",
    "obs_feat = th.randn(5, 1024)\n",
    "prev_latents = repeat(state_encoder.latents, \"n d -> b n d\", b = 5)\n",
    "masks = th.ones(5, 1)\n",
    "state_feat, hidden_state = state_encoder(obs_feat, prev_latents, masks)\n",
    "state_feat.shape, hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
