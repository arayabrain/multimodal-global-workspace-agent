{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import apex\n",
    "\n",
    "from collections import deque\n",
    "from torchinfo import summary\n",
    "\n",
    "import tools\n",
    "from configurator import generate_args, get_arg_dict\n",
    "from th_logger import TBXLogger as TBLogger\n",
    "\n",
    "# Env deps: Soundspaces and Habitat\n",
    "from habitat.datasets import make_dataset\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "from ss_baselines.common.utils import images_to_video_with_audio\n",
    "\n",
    "# Custom ActorCritic agent for PPO\n",
    "from models import ActorCritic, ActorCritic_DeepEthologyVirtualRodent, \\\n",
    "    Perceiver_GWT_GWWM_ActorCritic, Perceiver_GWT_AttGRU_ActorCritic\n",
    "\n",
    "# Dataset utils\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import compress_pickle as cpkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variant will sample one single (sub) seuqence of an episode as a trajectoyr\n",
    "# and add zero paddign to the rest\n",
    "class BCIterableDataset3(IterableDataset):\n",
    "    def __init__(self, dataset_path, batch_length, seed=111):\n",
    "        self.seed = seed\n",
    "        self.batch_length = batch_length\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        # Read episode filenames in the dataset path\n",
    "        self.ep_filenames = os.listdir(dataset_path)\n",
    "        if \"dataset_statistics.bz2\" in self.ep_filenames:\n",
    "            self.ep_filenames.remove(\"dataset_statistics.bz2\")\n",
    "        \n",
    "        print(f\"Initialized IterDset with {len(self.ep_filenames)} episodes.\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch_length = self.batch_length\n",
    "        while True:\n",
    "            # Sample one episode file\n",
    "            idx = th.randint(len(self.ep_filenames), ())\n",
    "            ep_filename = self.ep_filenames[idx]\n",
    "            ep_filepath = os.path.join(self.dataset_path, ep_filename)\n",
    "            with open(ep_filepath, \"rb\") as f:\n",
    "                edd = cpkl.load(f)\n",
    "            is_success = edd[\"info_list\"][-1][\"success\"]\n",
    "            last_action = edd[\"action_list\"][-1]\n",
    "            print(f\"Sampled traj idx: {idx}; Length: {edd['ep_length']}; Success: {is_success}; Last act: {last_action}\")\n",
    "            \n",
    "            # edd_start = th.randint(0, edd[\"ep_length\"]-20, ()).item() # Sample start of sub-squence for this episode\n",
    "            # NOTE: the following sampling might not leverage long-term trajectories well.\n",
    "            edd_start = 0 # Given that we have short trajectories, just start at the beginning anyway\n",
    "            edd_end = min(edd_start + batch_length, edd[\"ep_length\"])\n",
    "            subseq_len = edd_end - edd_start\n",
    "            \n",
    "            horizon = subseq_len\n",
    "\n",
    "            obs_list = {\n",
    "                k: np.zeros([batch_length, *np.shape(v)[1:]]) for k,v in edd[\"obs_list\"].items()\n",
    "            }\n",
    "            action_list, reward_list, done_list, depad_mask_list = \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros((batch_length, 1)).astype(np.bool8)\n",
    "\n",
    "            for k, v in edd[\"obs_list\"].items():\n",
    "                obs_list[k][:horizon] = v[edd_start:edd_end]\n",
    "            # Adjust the shape of obs_list[\"depth\"] from (T, H, W) -> (T, H, W, 1))\n",
    "            obs_list[\"depth\"] = obs_list[\"depth\"][:, :, :, None]\n",
    "            action_list[:horizon] = np.array(edd[\"action_list\"][edd_start:edd_end])[:, None]\n",
    "            reward_list[:horizon] = np.array(edd[\"reward_list\"][edd_start:edd_end])[:, None]\n",
    "            done_list[:horizon] = np.array(edd[\"done_list\"][edd_start:edd_end])[:, None]\n",
    "            depad_mask_list[:horizon] = True\n",
    "\n",
    "            yield obs_list, action_list, reward_list, done_list, depad_mask_list\n",
    "    \n",
    "def make_dataloader3(dataset_path, batch_size, batch_length, seed=111, num_workers=2):\n",
    "    def worker_init_fn(worker_id):\n",
    "        # worker_seed = th.initial_seed() % (2 ** 32)\n",
    "        worker_seed = 133754134 + worker_id\n",
    "\n",
    "        random.seed(worker_seed)\n",
    "        np.random.seed(worker_seed)\n",
    "\n",
    "    th_seed_gen = th.Generator()\n",
    "    th_seed_gen.manual_seed(133754134 + seed)\n",
    "\n",
    "    dloader = iter(\n",
    "        DataLoader(\n",
    "            BCIterableDataset3(\n",
    "                dataset_path=dataset_path, batch_length=batch_length),\n",
    "                batch_size=batch_size, num_workers=num_workers,\n",
    "                worker_init_fn=worker_init_fn, generator=th_seed_gen\n",
    "            )\n",
    "    )\n",
    "\n",
    "    return dloader\n",
    "\n",
    "# Tensorize current observation, store to rollout data\n",
    "def tensorize_obs_dict(obs, device, observations=None, rollout_step=None):\n",
    "    obs_th = {}\n",
    "    for obs_field, _ in obs[0].items():\n",
    "        v_th = th.Tensor(np.array([step_obs[obs_field] for step_obs in obs], dtype=np.float32)).to(device)\n",
    "        # in SS1.0, the dcepth observations comes as [B, 128, 128, 1, 1], so fix that\n",
    "        if obs_field == \"depth\" and v_th.dim() == 5:\n",
    "            v_th = v_th.squeeze(-1)\n",
    "        obs_th[obs_field] = v_th\n",
    "        # Special case when doing the rollout, also stores the \n",
    "        if observations is not None:\n",
    "            observations[obs_field][rollout_step] = v_th\n",
    "    \n",
    "    return obs_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 15:01:42.930752: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Logdir: ./logs/_seed_111__2023_05_22_15_01_42_744417.musashi\n"
     ]
    }
   ],
   "source": [
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "    \n",
    "    # Behavior cloning gexperiment config\n",
    "    get_arg_dict(\"dataset-path\", str, \"SAVI_Oracle_Dataset_v0\"),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/savi/savi_ss1.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.0), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"deep-etho\",\n",
    "                    \"perceiver-gwt-gwwm\", \"perceiver-gwt-attgru\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, False, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    ## Special BC\n",
    "    get_arg_dict(\"prev-actions\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"burn-in\", int, 0), # Steps used to init the latent state for RNN component\n",
    "    get_arg_dict(\"batch-chunk-length\", int, 0), # For gradient accumulation\n",
    "    get_arg_dict(\"ce-weights\", float, None, metatype=\"list\"), # Weights for the Cross Entropy loss\n",
    "    get_arg_dict(\"dataset-ce-weights\", bool, True, metatype=\"bool\"),\n",
    "\n",
    "    # Eval protocol\n",
    "    get_arg_dict(\"eval\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"eval-every\", int, int(1.5e4)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"eval-n-episodes\", int, 5),\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Gradient accumulation support\n",
    "if args.batch_chunk_length == 0:\n",
    "    args.batch_chunk_length = args.num_envs\n",
    "\n",
    "# Experiment logger\n",
    "tblogger = TBLogger(exp_name=args.exp_name, args=args)\n",
    "print(f\"# Logdir: {tblogger.logdir}\")\n",
    "should_log_training_stats = tools.Every(args.log_training_stats_every)\n",
    "should_eval = tools.Every(args.eval_every)\n",
    "\n",
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "th.cuda.manual_seed_all(args.seed)\n",
    "th.backends.cudnn.deterministic = args.torch_deterministic\n",
    "# th.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "# Set device as GPU\n",
    "device = tools.get_device(args) if (not args.cpu and th.cuda.is_available()) else th.device(\"cpu\")\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "# NOTE: using less environments for eval to save up system memory -> run more experiment at thte same time\n",
    "env_config.NUM_PROCESSES = 2 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "agent_extra_rgb = False\n",
    "if args.save_videos:\n",
    "    # For RGB video sensors\n",
    "    if \"RGB_SENSOR\" not in env_config.SENSORS:\n",
    "        env_config.SENSORS.append(\"RGB_SENSOR\")\n",
    "        # Indicates to the agent that RGB obs should not be used as observational inputs\n",
    "        agent_extra_rgb = True\n",
    "    # For Waveform to generate audio over the videos\n",
    "    if \"AUDIOGOAL_SENSOR\" not in env_config.TASK_CONFIG.TASK.SENSORS:\n",
    "        env_config.TASK_CONFIG.TASK.SENSORS.append(\"AUDIOGOAL_SENSOR\")\n",
    "env_config.freeze()\n",
    "# print(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 15:01:44,242 Initializing dataset SemanticAudioNav\n",
      "2023-05-22 15:01:44,452 Initializing dataset SemanticAudioNav\n",
      "2023-05-22 15:02:05,738 initializing sim SoundSpacesSim\n",
      "2023-05-22 15:02:05,891 Initializing task SemanticAudioNav\n",
      "2023-05-22 15:02:05,970 Initializing dataset SemanticAudioNav\n",
      "2023-05-22 15:02:25,728 initializing sim SoundSpacesSim\n",
      "2023-05-22 15:02:25,874 Initializing task SemanticAudioNav\n"
     ]
    }
   ],
   "source": [
    "# Environment instantiation\n",
    "envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "single_observation_space = envs.observation_spaces[0]\n",
    "single_action_space = envs.action_spaces[0]\n",
    "\n",
    "# TODO: delete the envrionemtsn / find a more efficient method to do this\n",
    "\n",
    "# TODO: make the ActorCritic components parameterizable through comand line ?\n",
    "if args.agent_type == \"ss-default\":\n",
    "    agent = ActorCritic(single_observation_space, single_action_space,\n",
    "        args.hidden_size, extra_rgb=agent_extra_rgb, prev_actions=args.prev_actions).to(device)\n",
    "elif args.agent_type == \"perceiver-gwt-gwwm\":\n",
    "    agent = Perceiver_GWT_GWWM_ActorCritic(single_observation_space, single_action_space,\n",
    "        args, extra_rgb=agent_extra_rgb).to(device)\n",
    "elif args.agent_type == \"perceiver-gwt-attgru\":\n",
    "    agent = Perceiver_GWT_AttGRU_ActorCritic(single_observation_space, single_action_space,\n",
    "        args, extra_rgb=agent_extra_rgb).to(device)\n",
    "elif args.agent_type == \"deep-etho\":\n",
    "    raise NotImplementedError(f\"Unsupported agent-type:{args.agent_type}\")\n",
    "    # TODO: support for storing the rnn_hidden_statse, so that the policy \n",
    "    # that takes in the 'core_modules' 's rnn hidden output can also work.\n",
    "    agent = ActorCritic_DeepEthologyVirtualRodent(single_observation_space,\n",
    "            single_action_space, 512).to(device)\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unsupported agent-type:{args.agent_type}\")\n",
    "\n",
    "if not args.cpu and th.cuda.is_available():\n",
    "    # TODO: GPU only. But what if we still want to use the default pytorch one ?\n",
    "    optimizer = apex.optimizers.FusedAdam(agent.parameters(), lr=args.lr, eps=1e-5, weight_decay=args.optim_wd)\n",
    "else:\n",
    "    optimizer = th.optim.Adam(agent.parameters(), lr=args.lr, eps=1e-5, weight_decay=args.optim_wd)\n",
    "\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized IterDset with 26508 episodes.\n",
      "## Loading CEL weights from dataset:  [4.71576376051609, 0.35938861176199993, 2.2051043449115317, 1.811762617039163]\n",
      "## Manually set CEL weights from dataset:  tensor([4.7158, 0.3594, 2.2051, 1.8118], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "dloader = make_dataloader3(args.dataset_path, batch_size=args.num_envs,\n",
    "                            batch_length=args.num_steps, seed=args.seed)\n",
    "\n",
    "## Compute action coefficient for CEL of BC\n",
    "dataset_stats_filepath = f\"{args.dataset_path}/dataset_statistics.bz2\"\n",
    "# Override dataset statistics if the file already exists\n",
    "if os.path.exists(dataset_stats_filepath):\n",
    "    with open(dataset_stats_filepath, \"rb\") as f:\n",
    "        dataset_statistics = cpkl.load(f)\n",
    "\n",
    "# Reads args.ce_weights if passed\n",
    "ce_weights = args.ce_weights\n",
    "\n",
    "# In case args.dataset_ce_weights is True,\n",
    "# override the args.ce_weigths manual setting\n",
    "if args.dataset_ce_weights:\n",
    "    # TODO: make some assert on 1) the existence of the \"dataset_statistics.bz2\" file\n",
    "    # and 2) that it contains the \"action_cel_coefs\" of proper dimension\n",
    "    ce_weights = [dataset_statistics[\"action_cel_coefs\"][a] for a in range(4)]\n",
    "    print(\"## Loading CEL weights from dataset: \", ce_weights)\n",
    "\n",
    "if ce_weights is not None:\n",
    "    # TODO: assert it has same shape as action space.\n",
    "    ce_weights = th.Tensor(ce_weights).to(device)\n",
    "    print(\"## Manually set CEL weights from dataset: \", ce_weights)\n",
    "\n",
    "# Info logging\n",
    "# summary(agent)\n",
    "# print(\"\")\n",
    "# print(agent)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7158, 0.3594, 2.2051, 1.8118], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled traj idx: 22582; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5217; Length: 24; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9815; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16349; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6750; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12645; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14886; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24855; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4665; Length: 24; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6566; Length: 34; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26198; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20878; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24348; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25648; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17761; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6505; Length: 43; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11448; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24148; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20078; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5846; Length: 29; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10463; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 842; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5806; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18772; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2934; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22678; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1143; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16380; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23316; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17050; Length: 28; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7922; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3155; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8679; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5692; Length: 31; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18349; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9494; Length: 46; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1738; Length: 57; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13223; Length: 26; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14047; Length: 48; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1727; Length: 37; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11938; Length: 30; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25336; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17048; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18581; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17471; Length: 27; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5435; Length: 25; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9040; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7891; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16722; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4261; Length: 55; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14036; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3244; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24664; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 19739; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26256; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9599; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12014; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5504; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12280; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23521; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16145; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23923; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10397; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9255; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24205; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22301; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20030; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24269; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14930; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26256; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15854; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14690; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24110; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11015; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13300; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3472; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13038; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12692; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17151; Length: 34; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26425; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10576; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1970; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15392; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17966; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11910; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12182; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23251; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1317; Length: 32; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21880; Length: 30; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18981; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1535; Length: 30; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4536; Length: 34; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1871; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25675; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5256; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2963; Length: 29; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8618; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21880; Length: 30; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21535; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21464; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24138; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1033; Length: 25; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3146; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14251; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1326; Length: 32; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1292; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8095; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17617; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24116; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5675; Length: 28; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21319; Length: 28; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20685; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3410; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9708; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9734; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14761; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23076; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12203; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12492; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5912; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10208; Length: 31; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17874; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20539; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21434; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22149; Length: 35; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7943; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6809; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9368; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14080; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20413; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12080; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13570; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3046; Length: 35; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5504; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 885; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23454; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9919; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24236; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18719; Length: 24; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7224; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9271; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10043; Length: 38; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21043; Length: 32; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18118; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10870; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17215; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9411; Length: 27; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13215; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15205; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24355; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4892; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14632; Length: 35; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16064; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3370; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8537; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2930; Length: 39; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23393; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20722; Length: 76; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7174; Length: 35; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1944; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2188; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11567; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18059; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24794; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10702; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15536; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18942; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22340; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6302; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20592; Length: 18; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17650; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15418; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18118; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14311; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18055; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24752; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3150; Length: 29; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2981; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23201; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15362; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20570; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20264; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 836; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25975; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7878; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12761; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24248; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20228; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16945; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22461; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23565; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24550; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5902; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20409; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22044; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6668; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20056; Length: 36; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13228; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2993; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9979; Length: 29; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6133; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 19142; Length: 31; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24141; Length: 40; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8052; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7233; Length: 24; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5795; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21024; Length: 43; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 19387; Length: 24; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 867; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8007; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24443; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10157; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 6204; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18202; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9789; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20564; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 2783; Length: 58; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23318; Length: 41; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20681; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7162; Length: 15; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13951; Length: 26; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10869; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 23095; Length: 25; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15433; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14209; Length: 36; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4145; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13317; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18715; Length: 44; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24796; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21871; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25660; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25478; Length: 41; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22010; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26446; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17556; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7352; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4219; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12385; Length: 21; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17830; Length: 29; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20003; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25641; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18205; Length: 59; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10049; Length: 31; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11122; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 497; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22252; Length: 27; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 3301; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26193; Length: 13; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16491; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 14127; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22957; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7164; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10975; Length: 41; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18957; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8136; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 9459; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24721; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13787; Length: 20; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11902; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8502; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 24376; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 15258; Length: 8; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 11577; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 7575; Length: 37; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 17281; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 21576; Length: 7; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22305; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16253; Length: 25; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 10055; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 26114; Length: 14; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 19641; Length: 9; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 858; Length: 11; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 16748; Length: 34; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1206; Length: 48; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 18218; Length: 33; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 25440; Length: 17; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 20315; Length: 24; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 13966; Length: 19; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 4435; Length: 10; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 1016; Length: 26; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 12550; Length: 23; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22870; Length: 12; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 5704; Length: 22; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 8293; Length: 16; Success: 1.0; Last act: 0\n",
      "Sampled traj idx: 22705; Length: 22; Success: 1.0; Last act: 0\n"
     ]
    }
   ],
   "source": [
    "obs_list, action_list, _, done_list, depad_mask_list = \\\n",
    "    [ {k: th.Tensor(v).float().to(device) for k,v in b.items()} if isinstance(b, dict) else \n",
    "        b.float().to(device) for b in next(dloader)] # NOTE this will not suport \"audiogoal\" waveform audio, only rgb / depth / spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shapes of the sampled batch data, so far so good\n",
    "# action_list.shape, done_list.shape, depad_mask_list.shape, obs_list[\"rgb\"].shape, obs_list[\"depth\"].shape, obs_list[\"spectrogram\"].shape, obs_list[\"audiogoal\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_cel_coefs': {0: 4.71576376051609,\n",
      "                      1: 0.35938861176199993,\n",
      "                      2: 2.2051043449115317,\n",
      "                      3: 1.811762617039163},\n",
      " 'action_counts': {0: 26507, 1: 347815, 2: 56687, 3: 68994},\n",
      " 'action_probs': {0: 0.05301368191790849,\n",
      "                  1: 0.6956258262450425,\n",
      "                  2: 0.11337331976008144,\n",
      "                  3: 0.13798717207696753},\n",
      " 'category_counts': {'bathtub': 78,\n",
      "                     'bed': 645,\n",
      "                     'cabinet': 2388,\n",
      "                     'chair': 6903,\n",
      "                     'chest_of_drawers': 690,\n",
      "                     'clothes': 117,\n",
      "                     'counter': 849,\n",
      "                     'cushion': 1821,\n",
      "                     'fireplace': 231,\n",
      "                     'gym_equipment': 79,\n",
      "                     'picture': 3563,\n",
      "                     'plant': 1400,\n",
      "                     'seating': 813,\n",
      "                     'shower': 230,\n",
      "                     'sink': 807,\n",
      "                     'sofa': 939,\n",
      "                     'stool': 673,\n",
      "                     'table': 2981,\n",
      "                     'toilet': 272,\n",
      "                     'towel': 541,\n",
      "                     'tv_monitor': 487},\n",
      " 'scene_counts': {'1pXnuDYAj8r': 686,\n",
      "                  '29hnd4uzFmX': 223,\n",
      "                  '5LpN3gDmAk7': 896,\n",
      "                  '5q7pvUzZiYa': 377,\n",
      "                  '759xd9YjKW5': 213,\n",
      "                  '7y3sRwLe3Va': 685,\n",
      "                  '8WUmhLawc2A': 437,\n",
      "                  'B6ByNegPMKs': 817,\n",
      "                  'D7N2EKCX4Sj': 648,\n",
      "                  'E9uDoFAP3SH': 713,\n",
      "                  'EDJbREhghzL': 726,\n",
      "                  'GdvgFV5R1Z5': 185,\n",
      "                  'JF19kD82Mey': 522,\n",
      "                  'JmbYfDe2QKZ': 893,\n",
      "                  'PX4nDJXEHrG': 325,\n",
      "                  'PuKPg4mmafe': 578,\n",
      "                  'S9hNv5qa7GM': 251,\n",
      "                  'SN83YJsR3w2': 1015,\n",
      "                  'Uxmj2M2itWa': 502,\n",
      "                  'V2XKFyX4ASd': 1616,\n",
      "                  'VFuaQ6m2Qom': 630,\n",
      "                  'VLzqgDo317F': 191,\n",
      "                  'VVfe2KiqLaN': 246,\n",
      "                  'Vvot9Ly1tCj': 954,\n",
      "                  'VzqfbhrpDEA': 572,\n",
      "                  'XcA2TqTSSAj': 355,\n",
      "                  'YmJkqBEsHnH': 547,\n",
      "                  'ZMojNkEp431': 1153,\n",
      "                  'aayBHfsNo7d': 486,\n",
      "                  'ac26ZMwG7aT': 1272,\n",
      "                  'b8cTxDM8gDG': 187,\n",
      "                  'cV4RVeZvu5T': 496,\n",
      "                  'gTV8FGcVJC9': 173,\n",
      "                  'jh4fc5c5qoQ': 469,\n",
      "                  'mJXqzFtmKg4': 511,\n",
      "                  'p5wJjkQkbXX': 676,\n",
      "                  'pRbA3pwrgk9': 344,\n",
      "                  'qoiz87JEwZ2': 1284,\n",
      "                  'r1Q1Z4BcV1o': 643,\n",
      "                  'r47D5H71a5s': 353,\n",
      "                  'rPc6DW4iMge': 31,\n",
      "                  'sT4fr6TAbpF': 1087,\n",
      "                  'uNb9QFRL6hY': 683,\n",
      "                  'ur6pFq6Qu1A': 209,\n",
      "                  'vyrNrziPKCB': 647},\n",
      " 'total_episodes': 26507,\n",
      " 'total_steps': 500003}\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset steps\n",
    "from pprint import pprint\n",
    "pprint(dataset_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 1000000\n",
      "Step 4501 / 1000000\n",
      "Step 7501 / 1000000\n",
      "Step 12001 / 1000000\n",
      "Step 15001 / 1000000\n",
      "Step 19501 / 1000000\n",
      "Step 22501 / 1000000\n",
      "Step 27001 / 1000000\n",
      "Step 30001 / 1000000\n",
      "Step 34501 / 1000000\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Caught EOFError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/tmp/ipykernel_21756/998088775.py\", line 24, in __iter__\n    edd = cpkl.load(f)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n    output = uncompress_and_unpickle(\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/functools.py\", line 888, in wrapper\n    return dispatch(args[0].__class__)(*args, **kw)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n    return pickler.load(stream=compresser.get_stream(), **kwargs)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n    return pickle.load(stream, **kwargs)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/bz2.py\", line 161, in peek\n    return self._buffer.peek(n)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/_compression.py\", line 68, in readinto\n    data = self.read(len(byte_view))\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/_compression.py\", line 99, in read\n    raise EOFError(\"Compressed file ended before the \"\nEOFError: Compressed file ended before the end-of-stream marker was reached\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# NOTE: 10 * 150 as step to match the training rate of an RL Agent, irrespective of which batch size / batch length is used\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m global_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, args\u001b[39m.\u001b[39mtotal_steps \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m150\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Load batch data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     obs_list, action_list, _, done_list, depad_mask_list \u001b[39m=\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         [ {k: th\u001b[39m.\u001b[39mTensor(v)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m b\u001b[39m.\u001b[39mitems()} \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(b, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m             b\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39;49m(dloader)] \u001b[39m# NOTE this will not suport \"audiogoal\" waveform audio, only rgb / depth / spectrogram\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# NOTE: RGB are normalized in the VisualCNN module\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# PPO networks expect input of shape T,B, ... so doing the permutation here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ppo_bc_fukushu.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m obs_list\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1347\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1373\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1373\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1374\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mEOFError\u001b[0m: Caught EOFError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/tmp/ipykernel_21756/998088775.py\", line 24, in __iter__\n    edd = cpkl.load(f)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n    output = uncompress_and_unpickle(\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/functools.py\", line 888, in wrapper\n    return dispatch(args[0].__class__)(*args, **kw)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n    return pickler.load(stream=compresser.get_stream(), **kwargs)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n    return pickle.load(stream, **kwargs)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/bz2.py\", line 161, in peek\n    return self._buffer.peek(n)\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/_compression.py\", line 68, in readinto\n    data = self.read(len(byte_view))\n  File \"/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/_compression.py\", line 99, in read\n    raise EOFError(\"Compressed file ended before the \"\nEOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    }
   ],
   "source": [
    "# Training start\n",
    "start_time = time.time()\n",
    "num_updates = args.total_steps // args.batch_size # Total number of updates that will take place in this experiment\n",
    "n_updates = 0 # Progressively tracks the number of network updats\n",
    "\n",
    "# NOTE: 10 * 150 as step to match the training rate of an RL Agent, irrespective of which batch size / batch length is used\n",
    "for global_step in range(1, args.total_steps + 1, 10 * 150):\n",
    "    # Load batch data\n",
    "    obs_list, action_list, _, done_list, depad_mask_list = \\\n",
    "        [ {k: th.Tensor(v).float().to(device) for k,v in b.items()} if isinstance(b, dict) else \n",
    "            b.float().to(device) for b in next(dloader)] # NOTE this will not suport \"audiogoal\" waveform audio, only rgb / depth / spectrogram\n",
    "    \n",
    "    # NOTE: RGB are normalized in the VisualCNN module\n",
    "    # PPO networks expect input of shape T,B, ... so doing the permutation here\n",
    "    for k, v in obs_list.items():\n",
    "        if k in [\"rgb\", \"spectrogram\", \"depth\"]:\n",
    "            obs_list[k] = v.permute(1, 0, 2, 3, 4) # BTCHW -> TBCHW\n",
    "        elif k in [\"audiogoal\"]:\n",
    "            obs_list[k] = v.permute(1, 0, 2, 3) # BTCL -> TBC\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    action_list = action_list.permute(1, 0, 2)\n",
    "    done_list = done_list.permute(1, 0, 2)\n",
    "    depad_mask_list = depad_mask_list.permute(1, 0, 2)\n",
    "\n",
    "    prev_actions_list = th.zeros_like(action_list)\n",
    "    prev_actions_list[1:] = action_list[:-1]\n",
    "    prev_actions_list = F.one_hot(prev_actions_list.long()[:, :, 0], num_classes=4).float()\n",
    "    prev_actions_list[0] = prev_actions_list[0] * 0.\n",
    "\n",
    "    # PPO Update Phase: actor and critic network updates\n",
    "    # assert args.num_envs % args.num_minibatches == 0\n",
    "    # envsperbatch = args.num_envs // args.num_minibatches\n",
    "    # envinds = np.arange(args.num_envs)\n",
    "    # flatinds = np.arange(args.batch_size).reshape(args.num_envs, args.num_steps)\n",
    "\n",
    "    for _ in range(args.update_epochs):\n",
    "        # np.random.shuffle(envinds)\n",
    "        # TODO / MISSING: mini-batch updates support like in ppo_av_nav.py\n",
    "        assert args.num_envs % args.batch_chunk_length == 0, \\\n",
    "            f\"num-envs (batch-size) of {args.num_envs} should be integer divisible by {args.batch_chunk_length}\"\n",
    "        \n",
    "        # For gradient accumulation over large batches\n",
    "        n_bchunks = args.num_envs // args.batch_chunk_length\n",
    "        \n",
    "        # Reset optimizer for each chunk over the \"trajectory length\" axis\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Placeholder for tracking the actions of the agent\n",
    "        batch_traj_agent_actions = th.zeros_like(action_list).long()\n",
    "\n",
    "        for bchnk_idx in range(n_bchunks):\n",
    "            # This will be used to recompute the rnn_hidden_states when computiong the new action logprobs\n",
    "            if args.agent_type == \"ss-default\":\n",
    "                rnn_hidden_state = th.zeros((1, args.batch_chunk_length, args.hidden_size), device=device)\n",
    "            elif args.agent_type in [\"perceiver-gwt-gwwm\", \"perceiver-gwt-attgru\"]:\n",
    "                rnn_hidden_state = agent.state_encoder.latents.repeat(args.batch_chunk_length, 1, 1)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Unsupported agent-type:{args.agent_type}\")\n",
    "\n",
    "            b_chnk_start = bchnk_idx * args.batch_chunk_length\n",
    "            b_chnk_end = (bchnk_idx + 1) * args.batch_chunk_length\n",
    "\n",
    "            # NOTE: v.shape[-3:] only valid for \"rgb\", \"depth\", and \"spectrogram\"\n",
    "            obs_chunk_list = {}\n",
    "            for k,v in obs_list.items():\n",
    "                if k in [\"rgb\", \"depth\", \"spectrogram\"]:\n",
    "                    reshaped_v = v[:, b_chnk_start:b_chnk_end].reshape(-1, *v.shape[-3:])\n",
    "                elif k in [\"audiogoal\"]:\n",
    "                    reshaped_v = v[:, b_chnk_start:b_chnk_end].reshape(-1, *v.shape[-2:])\n",
    "                else:\n",
    "                    reshaped_v = v[:, b_chnk_start:b_chnk_end].reshape(-1, *v.shape[-1:])\n",
    "                \n",
    "                obs_chunk_list[k] = reshaped_v\n",
    "\n",
    "            # obs_chunk_list = {k: v[:, b_chnk_start:b_chnk_end].reshape(-1, *v.shape[(-3 if k in [\"rgb\", \"depth\", \"spectrogram\"] else -2):])\n",
    "            #                     for k, v in obs_list.items()}\n",
    "            masks_chunk_list = 1. - done_list[:, b_chnk_start:b_chnk_end].reshape(-1, 1)\n",
    "            action_target_chunk_list = action_list[:, b_chnk_start:b_chnk_end, 0].reshape(-1).long()\n",
    "            prev_actions_chunk_list = prev_actions_list[:, b_chnk_start:b_chnk_end].reshape(-1, 4)\n",
    "\n",
    "            # TODO: maybe detach the rnn_hidden_state between two chunks ?\n",
    "            actions, action_probs, _, entropies, _, _ = \\\n",
    "                agent.act(obs_chunk_list, rnn_hidden_state,\n",
    "                    masks=masks_chunk_list) #, prev_actions=prev_actions_chunk_list)\n",
    "            \n",
    "\n",
    "            bc_loss = F.cross_entropy(action_probs, action_target_chunk_list,\n",
    "                                        weight=ce_weights, reduction=\"none\")\n",
    "            bc_loss = th.masked_select(\n",
    "                bc_loss,\n",
    "                depad_mask_list[:, b_chnk_start:b_chnk_end, 0].reshape(-1).bool()\n",
    "            ).mean()\n",
    "            \n",
    "            bc_loss /= n_bchunks # Normalize accumulated grads over batch axis\n",
    "            \n",
    "            # Entropy loss\n",
    "            # TODO: consider making this decaying as training progresses\n",
    "            entropy = th.masked_select(\n",
    "                entropies,\n",
    "                depad_mask_list[:, b_chnk_start:b_chnk_end, 0].reshape(-1).bool()\n",
    "            ).mean()\n",
    "            entropy /= n_bchunks # Normalize accumulated grads over batch axis\n",
    "\n",
    "            # Backpropagate and accumulate gradients over the batch size axis\n",
    "            (bc_loss - args.ent_coef * entropy).backward()\n",
    "\n",
    "            # Temporarily save the batch chunk actions of the agent for statistics later\n",
    "            batch_traj_agent_actions[:, b_chnk_start:b_chnk_end, :] = actions.detach().view(args.num_steps, args.batch_chunk_length, 1)\n",
    "\n",
    "        grad_norms_preclip = agent.get_grad_norms()\n",
    "        if args.max_grad_norm > 0:\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        n_updates += 1\n",
    "\n",
    "    if n_updates > 0 and should_log_training_stats(n_updates):\n",
    "        print(f\"Step {global_step} / {args.total_steps}\")\n",
    "\n",
    "        # TODO: add entropy logging\n",
    "        train_stats = {\n",
    "            \"bc_loss\": bc_loss.item() * n_bchunks,\n",
    "            \"entropy\": entropy.item() * n_bchunks\n",
    "        } # * n_bchunks undoes the scaling applied during grad accum\n",
    "        \n",
    "        tblogger.log_stats(train_stats, global_step, prefix=\"train\")\n",
    "    \n",
    "        # TODO: Additional dbg stats; add if needed\n",
    "        # debug_stats = {\n",
    "        #     # Additional debug stats\n",
    "        #     \"b_state_feats_avg\": b_state_feats.flatten(start_dim=1).mean().item(),\n",
    "        #     \"init_rnn_states_avg\": init_rnn_state.flatten(start_dim=1).mean().item(),\n",
    "        # }\n",
    "        # if args.pgwt_mod_embed:\n",
    "        #     debug_stats[\"mod_embed_avg\"] = agent.state_encoder.modality_embeddings.mean().item()\n",
    "\n",
    "        # Tracking stats about the actions distribution in the batches # TODO: fix typo\n",
    "        batch_traj_final_step_idxs = (depad_mask_list.sum(dim=0)-1)[None, :, :].long().reshape(-1).tolist()\n",
    "        batch_traj_final_step_mask = th.zeros_like(depad_mask_list).long()\n",
    "        for bi, done_t in enumerate(batch_traj_final_step_idxs):\n",
    "            batch_traj_final_step_mask[done_t, bi, 0] = 1\n",
    "\n",
    "        batch_traj_final_actions = th.masked_select(\n",
    "            action_list,\n",
    "            batch_traj_final_step_mask.bool()\n",
    "        )\n",
    "        # How many '0' actions are sampled in a batch ?\n",
    "        n_zero_batch_final_actions = len(th.where(batch_traj_final_actions == 0)[0])\n",
    "        # Ratio of 0 to other actions in the batch\n",
    "        n_zero_batch_final_actions_ratio = n_zero_batch_final_actions / depad_mask_list.sum().item()\n",
    "\n",
    "\n",
    "        batch_traj_agent_final_actions = th.masked_select(\n",
    "            batch_traj_agent_actions,\n",
    "            batch_traj_final_step_mask.bool()\n",
    "        )\n",
    "        # How many '0' actions sampled by the agent given the observations ?\n",
    "        n_zero_agent_final_actions = len(th.where(batch_traj_agent_final_actions == 0)[0])\n",
    "        # Ratio of 0 to other actions in the batch\n",
    "        n_zero_agent_final_actions_ratio = n_zero_agent_final_actions / depad_mask_list.sum().item()\n",
    "\n",
    "        # Special: tracking the 'StOP' action stats across in the batch\n",
    "        tblogger.log_stats({\n",
    "            \"n_zero_batch_final_actions\": n_zero_batch_final_actions,\n",
    "            \"n_zero_batch_final_actions_ratio\": n_zero_batch_final_actions_ratio,\n",
    "            \"n_zero_agent_final_actions\": n_zero_agent_final_actions,\n",
    "            \"n_zero_agent_final_actions_ratio\": n_zero_agent_final_actions_ratio\n",
    "        }, step=global_step, prefix=\"metrics\")\n",
    "\n",
    "        # Logging grad norms\n",
    "        tblogger.log_stats(agent.get_grad_norms(), global_step, prefix=\"debug/grad_norms\")\n",
    "        tblogger.log_stats(grad_norms_preclip, global_step, prefix=\"debug/grad_norms_preclip\")\n",
    "\n",
    "        info_stats = {\n",
    "            \"global_step\": global_step,\n",
    "            \"duration\": time.time() - start_time,\n",
    "            \"fps\": tblogger.track_duration(\"fps\", global_step),\n",
    "            \"n_updates\": n_updates,\n",
    "\n",
    "            \"env_step_duration\": tblogger.track_duration(\"fps_inv\", global_step, inverse=True),\n",
    "            \"model_updates_per_sec\": tblogger.track_duration(\"model_updates\",\n",
    "                n_updates),\n",
    "            \"model_update_step_duration\": tblogger.track_duration(\"model_updates_inv\",\n",
    "                n_updates, inverse=True),\n",
    "            \"batch_real_steps_ratio\": depad_mask_list.sum().item() / np.prod(depad_mask_list.shape)\n",
    "        }\n",
    "        tblogger.log_stats(info_stats, global_step, \"info\")\n",
    "\n",
    "    # if args.eval and should_eval(global_step):\n",
    "    #     eval_window_episode_stas = eval_agent(args, envs, agent,\n",
    "    #         device=device, tblogger=tblogger,\n",
    "    #         env_config=env_config, current_step=global_step,\n",
    "    #         n_episodes=args.eval_n_episodes, save_videos=True,\n",
    "    #         is_SAVi=is_SAVi)\n",
    "    #     episode_stats = {k: np.mean(v) for k, v in eval_window_episode_stas.items()}\n",
    "    #     episode_stats[\"last_actions_min\"] = np.min(eval_window_episode_stas[\"last_actions\"])\n",
    "    #     tblogger.log_stats(episode_stats, global_step, \"metrics\")\n",
    "    \n",
    "    #     if args.save_model:\n",
    "    #         model_save_dir = tblogger.get_models_savedir()\n",
    "    #         model_save_name = f\"ppo_agent.{global_step}.ckpt.pth\"\n",
    "    #         model_save_fullpath = os.path.join(model_save_dir, model_save_name)\n",
    "\n",
    "    #         th.save(agent.state_dict(), model_save_fullpath)\n",
    "\n",
    "# Clean up\n",
    "tblogger.close() \n",
    "envs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
