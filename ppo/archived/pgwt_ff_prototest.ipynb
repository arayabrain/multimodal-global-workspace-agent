{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from math import pi\n",
    "\n",
    "# helpers\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = dict()\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache = True, key = None, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "        result = f(*args, **kwargs)\n",
    "        cache[key] = result\n",
    "        return result\n",
    "    return cached_fn\n",
    "\n",
    "def fourier_encode(x, max_freq, num_bands = 4):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "\n",
    "    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "\n",
    "    x = x * scales * pi\n",
    "    x = torch.cat([x.sin(), x.cos()], dim = -1)\n",
    "    x = torch.cat((x, orig_x), dim = -1)\n",
    "    return x\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 512]), torch.Size([5, 8, 64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main class(es)\n",
    "class Perceiver_GWT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth,\n",
    "        input_channels,\n",
    "        latent_type = \"randn\",\n",
    "        latent_learned = True,\n",
    "        num_latents = 8,\n",
    "        latent_dim = 64,\n",
    "        cross_heads = 1,\n",
    "        latent_heads = 8,\n",
    "        cross_dim_head = 64,\n",
    "        latent_dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        self_per_cross_attn = 1, # Number of self attention blocks per cross attn.\n",
    "        weight_tie_layers = False,\n",
    "        # FF related\n",
    "        max_freq = 10.,\n",
    "        num_freq_bands = 6,\n",
    "        fourier_encode_data = False,\n",
    "        input_axis = 1,\n",
    "        use_ca = True, # Well, CA should always be used ...\n",
    "        use_sa = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.use_ca = use_ca\n",
    "        self.use_sa = use_sa\n",
    "        assert use_ca or use_sa, f\"Neither Cross Attention nor Self Attention seem to be enabled.\"\n",
    "\n",
    "        # Fourier Encode related\n",
    "        self.input_axis = input_axis\n",
    "        self.max_freq = max_freq\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        self.fourier_encode_data = fourier_encode_data\n",
    "        fourier_channels = (input_axis * ((num_freq_bands * 2) + 1)) if fourier_encode_data else 0\n",
    "        input_dim = fourier_channels + input_channels\n",
    "\n",
    "        # Latent vector, supposedly equivalent to an RNN's hidden state\n",
    "        if latent_type == \"randn\":\n",
    "            self.latents = torch.randn(num_latents, latent_dim)\n",
    "        elif latent_type == \"zeros\":\n",
    "            self.latents = torch.zeros(num_latents, latent_dim)\n",
    "        \n",
    "        self.latents = nn.Parameter(self.latents, requires_grad=latent_learned)\n",
    "\n",
    "        # Defines the cross-attention and self-attention layers\n",
    "        get_cross_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, input_dim, heads = cross_heads, dim_head = cross_dim_head, dropout = attn_dropout), context_dim = input_dim)\n",
    "        get_cross_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))\n",
    "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head, dropout = attn_dropout))\n",
    "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))\n",
    "\n",
    "        get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff = map(cache_fn, (get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff))\n",
    "\n",
    "        # Populate cross-attention and self-attention layers\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            should_cache = i > 0 and weight_tie_layers\n",
    "            cache_args = {'_cache': should_cache}\n",
    "\n",
    "            self_attns = nn.ModuleList([])\n",
    "            \n",
    "            if self.use_sa:\n",
    "                for block_ind in range(self_per_cross_attn):\n",
    "                    self_attns.append(nn.ModuleList([\n",
    "                        get_latent_attn(**cache_args, key = block_ind),\n",
    "                        get_latent_ff(**cache_args, key = block_ind)\n",
    "                    ]))\n",
    "            else:\n",
    "                self_attns.append(nn.Identity())\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_cross_attn(**cache_args),\n",
    "                get_cross_ff(**cache_args),\n",
    "                self_attns\n",
    "            ]))\n",
    "\n",
    "    def seq_forward(self, data, prev_latents, masks):\n",
    "        # TODO: a more optimal method to process sequences of same length together ?\n",
    "        x_list, latents_list = [], []\n",
    "\n",
    "        B_T, feat_dim = data.shape\n",
    "        B = prev_latents.shape[0]\n",
    "        T = B_T // B # TODO: assert that B * T == B_T\n",
    "        latents = prev_latents\n",
    "\n",
    "        data = data.reshape(B, T, feat_dim)\n",
    "        masks = masks.reshape(B, T, 1)\n",
    "\n",
    "        for t in range(T):\n",
    "            x, latents = self.single_forward(data[:, t], latents, masks[:, t])\n",
    "\n",
    "            x_list.append(x)\n",
    "            latents_list.append(latents)\n",
    "        \n",
    "        # TODO: debug\n",
    "        x_list = th.stack(x_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, feat_dim]\n",
    "        latents_list = th.stack(latents_list, dim=0).flatten(start_dim=0, end_dim=1) # [B * T, num_latents, latent_dim]\n",
    "\n",
    "        return x_list, latents_list\n",
    "\n",
    "    def single_forward(self, data, prev_latents, masks):\n",
    "        if data.dim() == 2:\n",
    "            data = data[:, :, None] # From [B, feat_dim] -> [B ,feat_dim, 1]\n",
    "        \n",
    "        b, *axis, _, device, dtype = *data.shape, data.device, data.dtype\n",
    "        # assert len(axis) == self.input_axis, 'input data must have the right number of axis'\n",
    "        \n",
    "        if self.fourier_encode_data:\n",
    "            # calculate fourier encoded positions in the range of [-1, 1], for all axis\n",
    "            axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))\n",
    "            pos = torch.stack(torch.meshgrid(*axis_pos, indexing = 'ij'), dim = -1)\n",
    "            enc_pos = fourier_encode(pos, self.max_freq, self.num_freq_bands)\n",
    "            enc_pos = rearrange(enc_pos, '... n d -> ... (n d)')\n",
    "            enc_pos = repeat(enc_pos, '... -> b ...', b = b)\n",
    "\n",
    "            data = torch.cat((data, enc_pos), dim = -1)\n",
    "\n",
    "        # concat to channels of data and flatten axis\n",
    "        data = rearrange(data, 'b ... d -> b (...) d')\n",
    "\n",
    "        # If the current step is the start of a new episode,\n",
    "        # the the mask will contain 0\n",
    "        prev_latents = masks[:, :, None] * prev_latents + \\\n",
    "            (1. - masks[:, :, None]) * repeat(self.latents, 'n d -> b n d', b = b)\n",
    "        \n",
    "        x = prev_latents\n",
    "\n",
    "        # Apply cross-attention and self-attention layers successively\n",
    "        for cross_attn, cross_ff, self_attns in self.layers:\n",
    "            x = cross_attn(x, context = data, mask = None) + x\n",
    "            x = cross_ff(x) + x\n",
    "\n",
    "            if self.use_sa:\n",
    "                for self_attn, self_ff in self_attns:\n",
    "                    x = self_attn(x) + x\n",
    "                    x = self_ff(x) + x\n",
    "        \n",
    "        return x.flatten(start_dim=1), x # state_feat, latents\n",
    "    \n",
    "    def forward(self, data, prev_latents, masks):\n",
    "        \"\"\"\n",
    "            - data: observation features [NUM_ENVS, feat_dim] or [NUM_ENVS, NUM_STEPS, feat_dim]\n",
    "            - prev_latents: previous latents [B, num_latents, latent_dim]\n",
    "            - masks: not Perceiver mask, but end-of-episode signaling mask\n",
    "                - shape of [NUM_ENVS, 1] if single step forward\n",
    "                - shape of [NUM_ENVS, NUM_STEPS, 1] if sequence forward\n",
    "        \"\"\"\n",
    "        if data.size(0) == prev_latents.size(0):\n",
    "            return self.single_forward(data, prev_latents, masks)\n",
    "        else:\n",
    "            return self.seq_forward(data, prev_latents, masks)\n",
    "\n",
    "state_encoder = Perceiver_GWT(\n",
    "    depth = 1,\n",
    "    input_channels = 1,\n",
    "    num_latents = 8,\n",
    "    latent_dim = 64,\n",
    "    fourier_encode_data = True,\n",
    "    input_axis = 1\n",
    ")\n",
    "obs_feat = th.randn(5, 1024)\n",
    "prev_latents = repeat(state_encoder.latents, \"n d -> b n d\", b = 5)\n",
    "masks = th.ones(5, 1)\n",
    "outputs = state_encoder(obs_feat, prev_latents, masks)\n",
    "outputs[0].shape, outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
