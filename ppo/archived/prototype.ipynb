{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 6, 6])\n",
      "torch.Size([1, 64, 13, 3])\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Agent                                    --\n",
      "├─VisionEncoder: 1-1                     --\n",
      "│    └─Conv2d: 2-1                       6,176\n",
      "│    └─Conv2d: 2-2                       32,832\n",
      "│    └─Conv2d: 2-3                       36,928\n",
      "├─AudioEncoder: 1-2                      --\n",
      "│    └─Conv2d: 2-4                       1,632\n",
      "│    └─Conv2d: 2-5                       18,496\n",
      "│    └─Conv2d: 2-6                       36,928\n",
      "├─Linear: 1-3                            1,181,952\n",
      "├─Linear: 1-4                            1,280,448\n",
      "├─Attention2d: 1-5                       1\n",
      "│    └─Conv2d: 2-7                       2,064\n",
      "│    └─Conv2d: 2-8                       1,040\n",
      "│    └─Conv2d: 2-9                       4,160\n",
      "├─Attention2d: 1-6                       1\n",
      "│    └─Conv2d: 2-10                      2,064\n",
      "│    └─Conv2d: 2-11                      1,040\n",
      "│    └─Conv2d: 2-12                      4,160\n",
      "├─Sequential: 1-7                        --\n",
      "│    └─Flatten: 2-13                     --\n",
      "│    └─Linear: 2-14                      1,180,160\n",
      "├─Sequential: 1-8                        --\n",
      "│    └─Flatten: 2-15                     --\n",
      "│    └─Linear: 2-16                      1,278,464\n",
      "├─GRUCell: 1-9                           2,362,368\n",
      "├─Sequential: 1-10                       --\n",
      "│    └─Linear: 2-17                      786,944\n",
      "│    └─ReLU: 2-18                        --\n",
      "│    └─Linear: 2-19                      262,656\n",
      "│    └─ReLU: 2-20                        --\n",
      "│    └─Linear: 2-21                      2,052\n",
      "=================================================================\n",
      "Total params: 8,482,566\n",
      "Trainable params: 8,482,566\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "  def __init__(self, query_channels, key_value_channels):\n",
    "    super().__init__()\n",
    "    self.att_channels = query_channels // 8\n",
    "    self.Wf = nn.Conv2d(query_channels, self.att_channels, 1)\n",
    "    self.Wg = nn.Conv2d(key_value_channels, self.att_channels, 1)\n",
    "    self.Wh = nn.Conv2d(key_value_channels, key_value_channels, 1)\n",
    "    self.gamma = nn.Parameter(torch.zeros(1))  # Initialise attention at 0\n",
    "\n",
    "  def forward(self, query_input, key_value_input):\n",
    "    B, C, H, W = key_value_input.size()\n",
    "    f = self.Wf(query_input).view(B, self.att_channels, -1).permute(0, 2, 1)  # Query\n",
    "    g = self.Wg(key_value_input).view(B, self.att_channels, -1)  # Key\n",
    "    h = self.Wh(key_value_input).view(B, C, -1).permute(0, 2, 1)  # Value\n",
    "    beta = F.softmax(f @ g, dim=2)  # Attention\n",
    "    o = (beta @ h).permute(0, 2, 1).view(B, C, H, W)\n",
    "    y = self.gamma * o + key_value_input\n",
    "    return y\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, channels, 8, stride=4)\n",
    "    self.conv2 = nn.Conv2d(channels, 2 * channels, 4, stride=2)\n",
    "    self.conv3 = nn.Conv2d(2 * channels, 2 * channels, 3, stride=2)\n",
    "\n",
    "  def forward(self, rgb):\n",
    "    h = F.relu(self.conv1(rgb))\n",
    "    h = F.relu(self.conv2(h))\n",
    "    h = F.relu(self.conv3(h))\n",
    "    return h\n",
    "\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(2, channels, 5, stride=2)\n",
    "    self.conv2 = nn.Conv2d(channels, 2 * channels, 3, stride=2)\n",
    "    self.conv3 = nn.Conv2d(2 * channels, 2 * channels, 3, stride=1)\n",
    "\n",
    "  def forward(self, spectrogram):\n",
    "    h = F.relu(self.conv1(spectrogram))\n",
    "    h = F.relu(self.conv2(h))\n",
    "    h = F.relu(self.conv3(h))\n",
    "    return h\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "  def __init__(self, hidden_size, channels):\n",
    "    super().__init__()\n",
    "    self.vision_encoder, self.audio_encoder = VisionEncoder(channels), AudioEncoder(channels)\n",
    "    self.memory_vision_att_embedding, self.memory_audio_att_embedding = nn.Linear(hidden_size, 72 * channels), nn.Linear(hidden_size, 78 * channels)\n",
    "    self.vision_attention, self.audio_attention = Attention2d(4 * channels, 2 * channels), Attention2d(4 * channels, 2 * channels)\n",
    "    self.vision_embedding, self.audio_embedding = nn.Sequential(nn.Flatten(), nn.Linear(72 * channels, hidden_size)), nn.Sequential(nn.Flatten(), nn.Linear(78 * channels, hidden_size))\n",
    "    self.working_memory = nn.GRUCell(2 * hidden_size, hidden_size)\n",
    "    self.policy = nn.Sequential(nn.Linear(3 * hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 4))\n",
    "\n",
    "  def forward(self, rgb, spectrogram, memory):\n",
    "    enc_vision, enc_audio = self.vision_encoder(rgb), self.audio_encoder(spectrogram)\n",
    "    memory_vision_query, memory_audio_query = self.memory_vision_att_embedding(memory).reshape_as(enc_vision), self.memory_audio_att_embedding(memory).reshape_as(enc_audio)\n",
    "    print(memory_vision_query.shape)\n",
    "    print(memory_audio_query.shape)\n",
    "    att_vision, att_audio = self.vision_attention(torch.cat([memory_vision_query, enc_vision], dim=1), enc_vision), self.audio_attention(torch.cat([memory_audio_query, enc_audio], dim=1), enc_audio)\n",
    "    att_sensor = torch.cat([self.vision_embedding(att_vision), self.audio_embedding(att_audio)], dim=1)\n",
    "    memory = self.working_memory(att_sensor, memory)\n",
    "    logits = self.policy(torch.cat([att_sensor, memory], dim=1))\n",
    "    return Categorical(logits=logits), memory\n",
    "\n",
    "\n",
    "H, C = 512, 32\n",
    "rgb, spectrogram = torch.zeros(1, 3, 128, 128), torch.zeros(1, 2, 65, 26)\n",
    "memory = torch.zeros(1, H)\n",
    "agent = Agent(H, C)\n",
    "policy, memory = agent(rgb, spectrogram, memory)\n",
    "from torchinfo import summary\n",
    "print(summary(agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 6, 6])\n",
      "torch.Size([1, 64, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "  def __init__(self, query_channels, key_value_channels):\n",
    "    super().__init__()\n",
    "    self.att_channels = query_channels // 4\n",
    "    self.Wf = nn.Conv2d(query_channels, self.att_channels, 1)\n",
    "    self.Wg = nn.Conv2d(key_value_channels, self.att_channels, 1)\n",
    "    self.Wh = nn.Conv2d(key_value_channels, key_value_channels, 1)\n",
    "    self.gamma = nn.Parameter(torch.zeros(1))  # Initialise attention at 0\n",
    "\n",
    "  def forward(self, query_input, key_value_input):\n",
    "    B, C, H, W = key_value_input.size()\n",
    "    f = self.Wf(query_input).view(B, self.att_channels, -1).permute(0, 2, 1)  # Query\n",
    "    g = self.Wg(key_value_input).view(B, self.att_channels, -1)  # Key\n",
    "    h = self.Wh(key_value_input).view(B, C, -1).permute(0, 2, 1)  # Value\n",
    "    beta = F.softmax(f @ g, dim=2)  # Attention\n",
    "    o = (beta @ h).permute(0, 2, 1).view(B, C, H, W)\n",
    "    y = self.gamma * o + key_value_input\n",
    "    return y\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, channels, 8, stride=4)\n",
    "    self.conv2 = nn.Conv2d(channels, 2 * channels, 4, stride=2)\n",
    "    self.conv3 = nn.Conv2d(2 * channels, 2 * channels, 3, stride=2)\n",
    "\n",
    "  def forward(self, rgb):\n",
    "    h = F.relu(self.conv1(rgb))\n",
    "    h = F.relu(self.conv2(h))\n",
    "    h = F.relu(self.conv3(h))\n",
    "    return h\n",
    "\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(2, channels, 5, stride=2)\n",
    "    self.conv2 = nn.Conv2d(channels, 2 * channels, 3, stride=2)\n",
    "    self.conv3 = nn.Conv2d(2 * channels, 2 * channels, 3, stride=1)\n",
    "\n",
    "  def forward(self, spectrogram):\n",
    "    h = F.relu(self.conv1(spectrogram))\n",
    "    h = F.relu(self.conv2(h))\n",
    "    h = F.relu(self.conv3(h))\n",
    "    return h\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "  def __init__(self, hidden_size, channels):\n",
    "    super().__init__()\n",
    "    self.vision_encoder, self.audio_encoder = VisionEncoder(channels), AudioEncoder(channels)\n",
    "    self.memory_vision_att_embedding, self.memory_audio_att_embedding = nn.Linear(hidden_size, 72 * channels), nn.Linear(hidden_size, 78 * channels)\n",
    "    self.vision_attention, self.audio_attention = Attention2d(4 * channels, 2 * channels), Attention2d(4 * channels, 2 * channels)\n",
    "    self.vision_embedding, self.audio_embedding = nn.Sequential(nn.Flatten(), nn.Linear(72 * channels, hidden_size)), nn.Sequential(nn.Flatten(), nn.Linear(78 * channels, hidden_size))\n",
    "    self.working_memory = nn.GRUCell(2 * hidden_size, hidden_size)\n",
    "    self.policy = nn.Sequential(nn.Linear(3 * hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 4))\n",
    "\n",
    "  def forward(self, rgb, spectrogram, memory):\n",
    "    enc_vision, enc_audio = self.vision_encoder(rgb), self.audio_encoder(spectrogram)\n",
    "    memory_vision_query, memory_audio_query = self.memory_vision_att_embedding(memory).reshape_as(enc_vision), self.memory_audio_att_embedding(memory).reshape_as(enc_audio)\n",
    "    print(memory_vision_query.shape)\n",
    "    print(memory_audio_query.shape)\n",
    "    att_vision, att_audio = self.vision_attention(torch.cat([memory_vision_query, enc_vision], dim=1), enc_vision), self.audio_attention(torch.cat([memory_audio_query, enc_audio], dim=1), enc_audio)\n",
    "    att_sensor = torch.cat([self.vision_embedding(att_vision), self.audio_embedding(att_audio)], dim=1)\n",
    "    memory = self.working_memory(att_sensor, memory)\n",
    "    logits = self.policy(torch.cat([att_sensor, memory], dim=1))\n",
    "    return Categorical(logits=logits), memory\n",
    "\n",
    "\n",
    "H, C = 512, 32\n",
    "rgb, spectrogram = torch.zeros(1, 3, 128, 128), torch.zeros(1, 2, 65, 26)\n",
    "memory = torch.zeros(1, H)\n",
    "agent = Agent(H, C)\n",
    "policy, memory = agent(rgb, spectrogram, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Agent                                    --\n",
      "├─VisionEncoder: 1-1                     --\n",
      "│    └─Conv2d: 2-1                       6,176\n",
      "│    └─Conv2d: 2-2                       32,832\n",
      "│    └─Conv2d: 2-3                       36,928\n",
      "├─AudioEncoder: 1-2                      --\n",
      "│    └─Conv2d: 2-4                       1,632\n",
      "│    └─Conv2d: 2-5                       18,496\n",
      "│    └─Conv2d: 2-6                       36,928\n",
      "├─Linear: 1-3                            1,181,952\n",
      "├─Linear: 1-4                            1,280,448\n",
      "├─Attention2d: 1-5                       1\n",
      "│    └─Conv2d: 2-7                       4,128\n",
      "│    └─Conv2d: 2-8                       2,080\n",
      "│    └─Conv2d: 2-9                       4,160\n",
      "├─Attention2d: 1-6                       1\n",
      "│    └─Conv2d: 2-10                      4,128\n",
      "│    └─Conv2d: 2-11                      2,080\n",
      "│    └─Conv2d: 2-12                      4,160\n",
      "├─Sequential: 1-7                        --\n",
      "│    └─Flatten: 2-13                     --\n",
      "│    └─Linear: 2-14                      1,180,160\n",
      "├─Sequential: 1-8                        --\n",
      "│    └─Flatten: 2-15                     --\n",
      "│    └─Linear: 2-16                      1,278,464\n",
      "├─GRUCell: 1-9                           2,362,368\n",
      "├─Sequential: 1-10                       --\n",
      "│    └─Linear: 2-17                      786,944\n",
      "│    └─ReLU: 2-18                        --\n",
      "│    └─Linear: 2-19                      262,656\n",
      "│    └─ReLU: 2-20                        --\n",
      "│    └─Linear: 2-21                      2,052\n",
      "=================================================================\n",
      "Total params: 8,488,774\n",
      "Trainable params: 8,488,774\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 6, 6])\n",
      "torch.Size([1, 64, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "  def __init__(self, query_channels, key_value_channels):\n",
    "    super().__init__()\n",
    "    self.att_channels = query_channels // 8\n",
    "    self.Wf = nn.Conv2d(query_channels, self.att_channels, 1)\n",
    "    self.Wg = nn.Conv2d(key_value_channels, self.att_channels, 1)\n",
    "    self.Wh = nn.Conv2d(key_value_channels, key_value_channels, 1)\n",
    "    self.gamma = nn.Parameter(torch.zeros(1))  # Initialise attention at 0\n",
    "\n",
    "  def forward(self, query_input, key_value_input):\n",
    "    B, C, H, W = key_value_input.size()\n",
    "    f = self.Wf(query_input).view(B, self.att_channels, -1).permute(0, 2, 1)  # Query\n",
    "    g = self.Wg(key_value_input).view(B, self.att_channels, -1)  # Key\n",
    "    h = self.Wh(key_value_input).view(B, C, -1).permute(0, 2, 1)  # Value\n",
    "    beta = F.softmax(f @ g, dim=2)  # Attention\n",
    "    o = (beta @ h).permute(0, 2, 1).view(B, C, H, W)\n",
    "    y = self.gamma * o + key_value_input\n",
    "    return y\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, channels, 8, stride=4)\n",
    "    self.conv2 = nn.Conv2d(channels, 2 * channels, 4, stride=2)\n",
    "    self.conv3 = nn.Conv2d(2 * channels, 2 * channels, 3, stride=2)\n",
    "\n",
    "  def forward(self, rgb):\n",
    "    h = F.relu(self.conv1(rgb))\n",
    "    h = F.relu(self.conv2(h))\n",
    "    h = F.relu(self.conv3(h))\n",
    "    return h\n",
    "\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(2, channels, 5, stride=2)\n",
    "    self.conv2 = nn.Conv2d(channels, 2 * channels, 3, stride=2)\n",
    "    self.conv3 = nn.Conv2d(2 * channels, 2 * channels, 3, stride=1)\n",
    "\n",
    "  def forward(self, spectrogram):\n",
    "    h = F.relu(self.conv1(spectrogram))\n",
    "    h = F.relu(self.conv2(h))\n",
    "    h = F.relu(self.conv3(h))\n",
    "    return h\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "  def __init__(self, hidden_size, channels):\n",
    "    super().__init__()\n",
    "    self.vision_encoder, self.audio_encoder = VisionEncoder(channels), AudioEncoder(channels)\n",
    "    self.memory_vision_att_embedding, self.memory_audio_att_embedding = nn.Linear(hidden_size, 72 * channels), nn.Linear(hidden_size, 78 * channels)\n",
    "    self.vision_attention, self.audio_attention = Attention2d(4 * channels, 2 * channels), Attention2d(4 * channels, 2 * channels)\n",
    "    self.vision_embedding, self.audio_embedding = nn.Sequential(nn.Flatten(), nn.Linear(72 * channels, hidden_size)), nn.Sequential(nn.Flatten(), nn.Linear(78 * channels, hidden_size))\n",
    "    self.working_memory = nn.GRUCell(2 * hidden_size, hidden_size)\n",
    "    self.policy = nn.Sequential(nn.Linear(3 * hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 4))\n",
    "\n",
    "  def forward(self, rgb, spectrogram, memory):\n",
    "    enc_vision, enc_audio = self.vision_encoder(rgb), self.audio_encoder(spectrogram)\n",
    "    memory_vision_query, memory_audio_query = self.memory_vision_att_embedding(memory).reshape_as(enc_vision), self.memory_audio_att_embedding(memory).reshape_as(enc_audio)\n",
    "    print(memory_vision_query.shape)\n",
    "    print(memory_audio_query.shape)\n",
    "    att_vision, att_audio = self.vision_attention(torch.cat([memory_vision_query, enc_vision], dim=1), enc_vision), self.audio_attention(torch.cat([memory_audio_query, enc_audio], dim=1), enc_audio)\n",
    "    att_sensor = torch.cat([self.vision_embedding(att_vision), self.audio_embedding(att_audio)], dim=1)\n",
    "    memory = self.working_memory(att_sensor, memory)\n",
    "    logits = self.policy(torch.cat([att_sensor, memory], dim=1))\n",
    "    return Categorical(logits=logits), memory\n",
    "\n",
    "\n",
    "H, C = 512, 32\n",
    "rgb, spectrogram = torch.zeros(1, 3, 128, 128), torch.zeros(1, 2, 65, 26)\n",
    "memory = torch.zeros(1, H)\n",
    "agent = Agent(H, C)\n",
    "policy, memory = agent(rgb, spectrogram, memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
