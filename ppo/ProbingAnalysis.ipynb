{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-14 14:28:04.921747: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-14 14:28:04.987921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-14 14:28:04.987944: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# General config related\n",
    "import os\n",
    "import umap\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import rsatoolbox\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "# Custom imports\n",
    "from configurator import get_arg_dict, generate_args\n",
    "\n",
    "# ML deps\n",
    "import apex\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plottign deps\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec # TODO: move to the top\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Env config related\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "from ss_baselines.common.utils import plot_top_down_map\n",
    "\n",
    "# Dataset utils\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "# Loading pretrained agent\n",
    "import tools\n",
    "import models\n",
    "from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic\n",
    "\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"white\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "    \n",
    "    # Behavior cloning gexperiment config\n",
    "    get_arg_dict(\"dataset-path\", str, \"SAVI_Oracle_Dataset_v0\"),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/savi/savi_ss1.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.0), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"perceiver-gwt-gwwm\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, False, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    ## Special BC\n",
    "    get_arg_dict(\"prev-actions\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"burn-in\", int, 0), # Steps used to init the latent state for RNN component\n",
    "    get_arg_dict(\"batch-chunk-length\", int, 0), # For gradient accumulation\n",
    "    get_arg_dict(\"dataset-ce-weights\", bool, True, metatype=\"bool\"), # If True, will read CEL weights based on action dist. from the 'dataset_statistics.bz2' file.\n",
    "    get_arg_dict(\"ce-weights\", float, None, metatype=\"list\"), # Weights for the Cross Entropy loss\n",
    "\n",
    "    # Eval protocol\n",
    "    get_arg_dict(\"eval\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"eval-every\", int, int(1.5e4)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"eval-n-episodes\", int, 5),\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Gradient accumulation support\n",
    "if args.batch_chunk_length == 0:\n",
    "    args.batch_chunk_length = args.num_envs\n",
    "\n",
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "th.cuda.manual_seed_all(args.seed)\n",
    "th.backends.cudnn.deterministic = args.torch_deterministic\n",
    "# th.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "# Set device as GPU\n",
    "device = tools.get_device(args) if (not args.cpu and th.cuda.is_available()) else th.device(\"cpu\")\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "## Override default seed\n",
    "env_config.SEED = env_config.TASK_CONFIG.SEED = env_config.TASK_CONFIG.SIMULATOR.SEED = args.seed\n",
    "\n",
    "env_config.TASK_CONFIG.SIMULATOR.USE_RENDERED_OBSERVATIONS = False\n",
    "# For smoother video, set CONTINUOUS_VIEW_CHANGE to True, and get the additional frames in obs_dict[\"intermediate\"]\n",
    "env_config.TASK_CONFIG.SIMULATOR.CONTINUOUS_VIEW_CHANGE = False\n",
    "\n",
    "env_config.TASK_CONFIG.SIMULATOR.RGB_SENSOR.WIDTH = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.RGB_SENSOR.HEIGHT = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.DEPTH_SENSOR.WIDTH = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.DEPTH_SENSOR.HEIGHT = 256\n",
    "\n",
    "# NOTE: using less environments for eval to save up system memory -> run more experiment at the same time\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "agent_extra_rgb = False\n",
    "if args.save_videos:\n",
    "    # For RGB video sensors\n",
    "    if \"RGB_SENSOR\" not in env_config.SENSORS:\n",
    "        env_config.SENSORS.append(\"RGB_SENSOR\")\n",
    "        # Indicates to the agent that RGB obs should not be used as observational inputs\n",
    "        agent_extra_rgb = True\n",
    "    # For Waveform to generate audio over the videos\n",
    "    if \"AUDIOGOAL_SENSOR\" not in env_config.TASK_CONFIG.TASK.SENSORS:\n",
    "        env_config.TASK_CONFIG.TASK.SENSORS.append(\"AUDIOGOAL_SENSOR\")\n",
    "# Add support for TOP_DOWN_MAP\n",
    "# NOTE: it seems to induce \"'DummySimulator' object has no attribute 'pathfinder'\" error\n",
    "# If top down map really needed, probably have to run the env without pre-rendered observations ?\n",
    "# env_config.TASK_CONFIG.TASK.MEASUREMENTS.append(\"TOP_DOWN_MAP\")\n",
    "\n",
    "env_config.freeze()\n",
    "\n",
    "## Compute action coefficient for CEL of BC\n",
    "dataset_stats_filepath = f\"{args.dataset_path}/dataset_statistics.bz2\"\n",
    "# Override dataset statistics if the file already exists\n",
    "if os.path.exists(dataset_stats_filepath):\n",
    "    with open(dataset_stats_filepath, \"rb\") as f:\n",
    "        dataset_statistics = cpkl.load(f)\n",
    "    \n",
    "N_CATEGORIES = len(dataset_statistics[\"category_counts\"].keys())\n",
    "N_SCENES = len(dataset_statistics[\"scene_counts\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict(audiogoal:Box(-3.4028235e+38, 3.4028235e+38, (2, 16000), float32), depth:Box(0, 255, (128, 128, 1), uint8), rgb:Box(0, 255, (128, 128, 3), uint8), spectrogram:Box(-3.4028235e+38, 3.4028235e+38, (65, 26, 2), float32)),\n",
       " Discrete(4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake environment instantiation to create the agent models later on\n",
    "\n",
    "# TODO: add adaptive creation of single_observation_space so that RGB and RGBD based variants\n",
    "# can be evaluated at thet same time\n",
    "from gym import spaces\n",
    "single_action_space = spaces.Discrete(4)\n",
    "single_observation_space = spaces.Dict({\n",
    "    \"rgb\": spaces.Box(shape=[128,128,3], low=0, high=255, dtype=np.uint8),\n",
    "    \"depth\": spaces.Box(shape=[128,128,1], low=0, high=255, dtype=np.uint8),\n",
    "    \"audiogoal\": spaces.Box(shape=[2,16000], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32),\n",
    "    \"spectrogram\": spaces.Box(shape=[65,26,2], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32)\n",
    "})\n",
    "# single_observation_space = envs.observation_spaces[0]\n",
    "# single_action_space = envs.action_spaces[0]\n",
    "\n",
    "single_observation_space, single_action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing Analysis Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target of probing\n",
    "## \"category\" -> how easy to predict category based on the learned features / inputs\n",
    "## \"scene\" -> how easy to predict scene based on the learned features / inputs\n",
    "PROBING_TARGETS = {\n",
    "    \"category\": {\"n_classes\": 21},\n",
    "    # \"scene\": {\"n_classes\": 10}, # TODO: make this based on the dataset ?\n",
    "}\n",
    "\n",
    "# Define which fields of an agent to use for the probes\n",
    "PROBING_INPUTS = [\"state_encoder\", \"audio_encoder.cnn.7\", \"visual_encoder.cnn.7\"]\n",
    "\n",
    "# Define the probing \"subjects\", i.e. which pre-trained BC networks to probe\n",
    "# also stores info. related to the path to the weights, and pretty names for the plots\n",
    "MODEL_VARIANTS_TO_STATEDICT_PATH = {\n",
    "    # region: Random baselines\n",
    "    # Random GRU Baseline\n",
    "    \"ppo_gru__random\": {\n",
    "        \"pretty_name\": \"GRU Random\",\n",
    "        \"state_dict_path\": \"\"\n",
    "    },\n",
    "    # Random PGWT Baseline\n",
    "    # \"ppo_pgwt__random\": {\n",
    "    #     \"pretty_name\": \"TransRNN Random\",\n",
    "    #     \"state_dict_path\": \"\"\n",
    "    # },\n",
    "    # endregion: Random baselines\n",
    "\n",
    "\n",
    "    # region: SAVi BC variants; trained using RGBD + Spectrogram ; trained up to 5M steps\n",
    "    # \"ppo_bc__rgbd_spectro__gru__SAVi\": {\n",
    "    #     \"pretty_name\": \"[SAVi BC] PPO GRU | RGB Spectro\",\n",
    "    #     \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "    #         \"ppo_bc__savi_ss1_rgbd_spectro__gru_seed_111__2023_06_10_16_05_39_999286.musashi\"\n",
    "    #         \"/models/ppo_agent.4995001.ckpt.pth\"\n",
    "    # },\n",
    "    \"ppo_bc__rgbd_spectro__pgwt__SAVi\": {\n",
    "        \"pretty_name\": \"[SAVi BC] PPO TransRNN | RGB Spectro\",\n",
    "        \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "            \"ppo_bc__savi_ss1_rgbd__spectro__pgwt__dpth_1_nlats_8_latdim_64_noSA_CAnheads_1_SAnheads_4_modembed_0_CAprevlats_seed_111__2023_06_10_16_05_37_098602.musashi\"\n",
    "            \"/models/ppo_agent.4995001.ckpt.pth\"\n",
    "    },\n",
    "    # endregion: SAVi BC variants; trained using RGBD + Spectrogram ; trained up to 5M steps\n",
    "}\n",
    "\n",
    "# Indexable instantiated agent models (Torch agents)\n",
    "MODEL_VARIANTS_TO_AGENTMODEL = {}\n",
    "\n",
    "for k, v in MODEL_VARIANTS_TO_STATEDICT_PATH.items():\n",
    "    args_copy = copy.copy(args)\n",
    "    # Override args depending on the model in use\n",
    "    if k.__contains__(\"gru\"):\n",
    "        agent = ActorCritic(single_observation_space, single_action_space, args.hidden_size, extra_rgb=False,\n",
    "            analysis_layers=models.GRU_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES)\n",
    "    elif k.__contains__(\"pgwt\"):\n",
    "        agent = Perceiver_GWT_GWWM_ActorCritic(single_observation_space, single_action_space, args, extra_rgb=False,\n",
    "            analysis_layers=models.PGWT_GWWM_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES + [\"state_encoder.ca.mha\"])\n",
    "\n",
    "    agent.eval()\n",
    "    # Load the model weights\n",
    "    # TODO: add map location device to use CPU only ?\n",
    "    if v[\"state_dict_path\"] != \"\":\n",
    "        agent_state_dict = th.load(v[\"state_dict_path\"])\n",
    "        agent.load_state_dict(agent_state_dict)\n",
    "    \n",
    "    MODEL_VARIANTS_TO_AGENTMODEL[k] = agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (visual_encoder): VisualCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=2304, out_features=512, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): AudioCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(2, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=2496, out_features=512, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (state_encoder): RNNStateEncoder(\n",
       "    (rnn): GRU(1024, 512)\n",
       "  )\n",
       "  (action_distribution): CategoricalNet(\n",
       "    (linear): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       "  (critic): CriticHead(\n",
       "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_VARIANTS_TO_AGENTMODEL.keys() # ['ppo_gru__random', 'ppo_bc__rgbd_spectro__pgwt__SAVi']\n",
    "MODEL_VARIANTS_TO_AGENTMODEL[\"ppo_gru__random\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe network definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - consider adding the reference to the network this probe is in charge of ?\n",
    "class GenericProbeNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # input_dim: shape of the \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating probes\n",
    "PROBES = {}\n",
    "for probe_target_name, probe_target_info in PROBING_TARGETS.items():\n",
    "    if probe_target_name not in PROBES.keys():\n",
    "        PROBES[probe_target_name] = {}\n",
    "    for probe_input in PROBING_INPUTS: # NOTE: maybe switch order with the MODEL_VARIANTS ???\n",
    "\n",
    "        if probe_input not in PROBES[probe_target_name].keys():\n",
    "            PROBES[probe_target_name][probe_input] = {}\n",
    "\n",
    "        for agent_variant in MODEL_VARIANTS_TO_AGENTMODEL.keys():\n",
    "            probe_input_dim = 512 # TODO: make this adapt to the actual shape of the input's probe\n",
    "            probe_output_dim = probe_target_info[\"n_classes\"]\n",
    "\n",
    "            probe_network = GenericProbeNetwork(probe_input_dim, probe_output_dim).to(device)\n",
    "            if not args.cpu and th.cuda.is_available():\n",
    "                # TODO: GPU only. But what if we still want to use the default pytorch one ?\n",
    "                optimizer = apex.optimizers.FusedAdam(probe_network.parameters(), lr=args.lr, eps=1e-5, weight_decay=args.optim_wd)\n",
    "            else:\n",
    "                optimizer = th.optim.Adam(probe_network.parameters(), lr=args.lr, eps=1e-5, weight_decay=args.optim_wd)\n",
    "\n",
    "            PROBES[probe_target_name][probe_input][agent_variant] = {\n",
    "                \"probe_network\": probe_network,\n",
    "                \"probe_optimizer\": optimizer\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': {'state_encoder': {'ppo_gru__random': {'probe_network': GenericProbeNetwork(\n",
       "      (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "    ),\n",
       "    'probe_optimizer': FusedAdam (\n",
       "    Parameter Group 0\n",
       "        betas: (0.9, 0.999)\n",
       "        bias_correction: True\n",
       "        eps: 1e-05\n",
       "        lr: 0.00025\n",
       "        weight_decay: 0\n",
       "    )},\n",
       "   'ppo_bc__rgbd_spectro__pgwt__SAVi': {'probe_network': GenericProbeNetwork(\n",
       "      (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "    ),\n",
       "    'probe_optimizer': FusedAdam (\n",
       "    Parameter Group 0\n",
       "        betas: (0.9, 0.999)\n",
       "        bias_correction: True\n",
       "        eps: 1e-05\n",
       "        lr: 0.00025\n",
       "        weight_decay: 0\n",
       "    )}},\n",
       "  'audio_encoder.cnn.7': {'ppo_gru__random': {'probe_network': GenericProbeNetwork(\n",
       "      (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "    ),\n",
       "    'probe_optimizer': FusedAdam (\n",
       "    Parameter Group 0\n",
       "        betas: (0.9, 0.999)\n",
       "        bias_correction: True\n",
       "        eps: 1e-05\n",
       "        lr: 0.00025\n",
       "        weight_decay: 0\n",
       "    )},\n",
       "   'ppo_bc__rgbd_spectro__pgwt__SAVi': {'probe_network': GenericProbeNetwork(\n",
       "      (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "    ),\n",
       "    'probe_optimizer': FusedAdam (\n",
       "    Parameter Group 0\n",
       "        betas: (0.9, 0.999)\n",
       "        bias_correction: True\n",
       "        eps: 1e-05\n",
       "        lr: 0.00025\n",
       "        weight_decay: 0\n",
       "    )}},\n",
       "  'visual_encoder.cnn.7': {'ppo_gru__random': {'probe_network': GenericProbeNetwork(\n",
       "      (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "    ),\n",
       "    'probe_optimizer': FusedAdam (\n",
       "    Parameter Group 0\n",
       "        betas: (0.9, 0.999)\n",
       "        bias_correction: True\n",
       "        eps: 1e-05\n",
       "        lr: 0.00025\n",
       "        weight_decay: 0\n",
       "    )},\n",
       "   'ppo_bc__rgbd_spectro__pgwt__SAVi': {'probe_network': GenericProbeNetwork(\n",
       "      (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "    ),\n",
       "    'probe_optimizer': FusedAdam (\n",
       "    Parameter Group 0\n",
       "        betas: (0.9, 0.999)\n",
       "        bias_correction: True\n",
       "        eps: 1e-05\n",
       "        lr: 0.00025\n",
       "        weight_decay: 0\n",
       "    )}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROBES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ppo_gru__random': {'probe_network': GenericProbeNetwork(\n",
       "    (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "  ),\n",
       "  'probe_optimizer': FusedAdam (\n",
       "  Parameter Group 0\n",
       "      betas: (0.9, 0.999)\n",
       "      bias_correction: True\n",
       "      eps: 1e-05\n",
       "      lr: 0.00025\n",
       "      weight_decay: 0\n",
       "  )},\n",
       " 'ppo_bc__rgbd_spectro__pgwt__SAVi': {'probe_network': GenericProbeNetwork(\n",
       "    (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       "  ),\n",
       "  'probe_optimizer': FusedAdam (\n",
       "  Parameter Group 0\n",
       "      betas: (0.9, 0.999)\n",
       "      bias_correction: True\n",
       "      eps: 1e-05\n",
       "      lr: 0.00025\n",
       "      weight_decay: 0\n",
       "  )}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the instantiate PROBES\n",
    "PROBES[\"category\"][\"state_encoder\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset to be used for probe training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized IterDset with 29001 episodes.\n"
     ]
    }
   ],
   "source": [
    "# NOTE / TODO: probe training might benefti from using different batch sizes ?\n",
    "\n",
    "# This variant will fill each batch trajectory using cat.ed episode data\n",
    "# There is no empty step in this batch\n",
    "class BCIterableDataset3(IterableDataset):\n",
    "    def __init__(self, dataset_path, batch_length, seed=111):\n",
    "        self.seed = seed\n",
    "        self.batch_length = batch_length\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        # Read episode filenames in the dataset path\n",
    "        self.ep_filenames = os.listdir(dataset_path)\n",
    "        if \"dataset_statistics.bz2\" in self.ep_filenames:\n",
    "            self.ep_filenames.remove(\"dataset_statistics.bz2\")\n",
    "        \n",
    "        print(f\"Initialized IterDset with {len(self.ep_filenames)} episodes.\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch_length = self.batch_length\n",
    "        while True:\n",
    "            # region: Sample episode data until there is enough to fill the hole batch traj\n",
    "            obs_list = {\n",
    "                \"depth\": np.zeros([batch_length, 128, 128]), # NOTE: data was recorded using (128, 128), but ideally we should have (128, 128, 1)\n",
    "                \"rgb\": np.zeros([batch_length, 128, 128, 3]),\n",
    "                \"audiogoal\": np.zeros([batch_length, 2, 16000]),\n",
    "                \"spectrogram\": np.zeros([batch_length, 65, 26, 2]),\n",
    "                \"category\": np.zeros([batch_length, 21]),\n",
    "                \"pointgoal_with_gps_compass\": np.zeros([batch_length, 2]),\n",
    "                \"pose\": np.zeros([batch_length, 4]),\n",
    "            }\n",
    "\n",
    "            action_list, reward_list, done_list = \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1])\n",
    "            ssf = 0 # Step affected so far\n",
    "            while ssf < batch_length:\n",
    "                idx = th.randint(len(self.ep_filenames), ())\n",
    "                ep_filename = self.ep_filenames[idx]\n",
    "                ep_filepath = os.path.join(self.dataset_path, ep_filename)\n",
    "                with open(ep_filepath, \"rb\") as f:\n",
    "                    edd = cpkl.load(f)\n",
    "                # print(f\"Sampled traj idx: {idx} ; Len: {edd['ep_length']}\")\n",
    "                \n",
    "                # Append the data to the bathc trjectory\n",
    "                rs = batch_length - ssf # Reamining steps\n",
    "                horizon = ssf + min(rs, edd[\"ep_length\"])\n",
    "                for k, v in edd[\"obs_list\"].items():\n",
    "                    obs_list[k][ssf:horizon] = v[:rs]\n",
    "                action_list[ssf:horizon] = np.array(edd[\"action_list\"][:rs])[:, None]\n",
    "                reward_list[ssf:horizon] = np.array(edd[\"reward_list\"][:rs])[:, None]\n",
    "                done_list[ssf:horizon] = np.array(edd[\"done_list\"][:rs])[:, None]\n",
    "\n",
    "                ssf += edd[\"ep_length\"]\n",
    "\n",
    "                if ssf >= self.batch_length:\n",
    "                    break\n",
    "\n",
    "            # Adjust shape of \"depth\" to be [T, H, W, 1] instead of [T, H, W]\n",
    "            obs_list[\"depth\"] = obs_list[\"depth\"][:, :, :, None]\n",
    "            \n",
    "            # TODO: add enough data about the scene to be able to do the probing\n",
    "            # Since the dataset statistics can be accessed here too, we can generate\n",
    "            # the vector of targets for the scene\n",
    "            \n",
    "            yield obs_list, action_list, reward_list, done_list\n",
    "            # endregion: Sample episode data until there is enough to fill the hole batch traj\n",
    "    \n",
    "def make_dataloader3(dataset_path, batch_size, batch_length, seed=111, num_workers=2):\n",
    "    def worker_init_fn(worker_id):\n",
    "        # worker_seed = th.initial_seed() % (2 ** 32)\n",
    "        worker_seed = 133754134 + worker_id\n",
    "\n",
    "        random.seed(worker_seed)\n",
    "        np.random.seed(worker_seed)\n",
    "\n",
    "    th_seed_gen = th.Generator()\n",
    "    th_seed_gen.manual_seed(133754134 + seed)\n",
    "\n",
    "    dloader = iter(\n",
    "        DataLoader(\n",
    "            BCIterableDataset3(\n",
    "                dataset_path=dataset_path, batch_length=batch_length),\n",
    "                batch_size=batch_size, num_workers=num_workers,\n",
    "                worker_init_fn=worker_init_fn, generator=th_seed_gen\n",
    "            )\n",
    "    )\n",
    "\n",
    "    return dloader\n",
    "\n",
    "# Instantiate the dataset object\n",
    "dloader = make_dataloader3(args.dataset_path, batch_size=args.num_envs,\n",
    "                            batch_length=args.num_steps, seed=args.seed, num_workers=8)\n",
    "\n",
    "# TODO: consider pre-computing CE weights for categories / scenes to balance the CE loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing iteration over one batch of data for a given variant\n",
    "\n",
    "# region: Load batch data, and related pre-processing\n",
    "obs_list, action_list, _, done_list = \\\n",
    "    [ {k: th.Tensor(v).float().to(device) for k,v in b.items()} if isinstance(b, dict) else \n",
    "        b.float().to(device) for b in next(dloader)]\n",
    "\n",
    "# NOTE: RGB are normalized in the VisualCNN module\n",
    "# PPO networks expect input of shape T,B, ... so doing the permutation first\n",
    "# then flatten over T x B dimensions. The RNN will reshape it as necessary\n",
    "for k, v in obs_list.items():\n",
    "    if k in [\"rgb\", \"spectrogram\", \"depth\"]:\n",
    "        obs_list[k] = v.permute(1, 0, 2, 3, 4) # BTCHW -> TBCHW\n",
    "        obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-3:])\n",
    "    elif k in [\"audiogoal\"]:\n",
    "        obs_list[k] = v.permute(1, 0, 2, 3) # BTCL -> TBCL\n",
    "        obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-2:])\n",
    "    else:\n",
    "        # TODO: handle other fields like \"category\", etc...\n",
    "        pass\n",
    "\n",
    "action_list = action_list.permute(1, 0, 2)\n",
    "done_list = done_list.permute(1, 0, 2)\n",
    "mask_list = 1. - done_list\n",
    "\n",
    "prev_actions_list = th.zeros_like(action_list)\n",
    "prev_actions_list[1:] = action_list[:-1]\n",
    "prev_actions_list = F.one_hot(prev_actions_list.long()[:, :, 0], num_classes=4).float()\n",
    "prev_actions_list[0] = prev_actions_list[0] * 0.0\n",
    "\n",
    "# Finally, also flatten across T x B, let the RNN do the unflattening if needs be\n",
    "action_list = action_list.reshape(-1) # Because it is used for the target later\n",
    "done_list = done_list.reshape(-1, 1)\n",
    "mask_list = mask_list.reshape(-1, 1)\n",
    "prev_actions_list = prev_actions_list.reshape(-1, 1)\n",
    "# endregion: Load batch data, and related pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_list.keys() # ['depth', 'rgb', 'audiogoal', 'spectrogram', 'category', 'pointgoal_with_gps_compass', 'pose']\n",
    "obs_list[\"depth\"].shape # torch.Size([1500, 128, 128, 1])\n",
    "done_list.shape # torch.Size([1500, 1])\n",
    "mask_list.shape # torch.Size([1500, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each \"agent_variant\", iterate\n",
    "\n",
    "agent_variant = \"ppo_gru__random\"\n",
    "agent = MODEL_VARIANTS_TO_AGENTMODEL[\"ppo_gru__random\"]\n",
    "\n",
    "# This will be used to recompute the rnn_hidden_states when computiong the new action logprobs\n",
    "if agent_variant.__contains__(\"gru\"):\n",
    "    rnn_hidden_state = th.zeros((1, args.batch_chunk_length, args.hidden_size), device=device)\n",
    "elif agent_variant.__contains__(\"pgwt\"):\n",
    "    rnn_hidden_state = agent.state_encoder.latents.repeat(args.batch_chunk_length, 1, 1)\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unsupported agent-type:{agent_variant}\")\n",
    "\n",
    "# for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "#     if agent_variant.__contains__(\"gru\"):\n",
    "#         AGENT_RNN_HIDDEN_STATE[agent_variant] = th.zeros((1, args.num_envs, args.hidden_size), device=device)\n",
    "#     elif agent_variant.__contains__(\"pgwt\"):\n",
    "#         AGENT_RNN_HIDDEN_STATE[agent_variant] = agent_model.state_encoder.latents.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_hidden_state.shape # torch.Size([1, 10, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the networks\n",
    "agent_outputs = agent.act(obs_list, rnn_hidden_state, masks=mask_list) #, prev_actions=prev_actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 150)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_envs, args.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = obs_list[\"category\"].reshape(-1, 21).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visual_encoder.cnn.0',\n",
       " 'visual_encoder.cnn.1',\n",
       " 'visual_encoder.cnn.2',\n",
       " 'visual_encoder.cnn.3',\n",
       " 'visual_encoder.cnn.4',\n",
       " 'visual_encoder.cnn.5',\n",
       " 'visual_encoder.cnn.6',\n",
       " 'visual_encoder.cnn.7',\n",
       " 'audio_encoder.cnn.0',\n",
       " 'audio_encoder.cnn.1',\n",
       " 'audio_encoder.cnn.2',\n",
       " 'audio_encoder.cnn.3',\n",
       " 'audio_encoder.cnn.4',\n",
       " 'audio_encoder.cnn.5',\n",
       " 'audio_encoder.cnn.6',\n",
       " 'audio_encoder.cnn.7',\n",
       " 'action_distribution.linear',\n",
       " 'critic.fc',\n",
       " 'state_encoder']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent._features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._features[\"state_encoder\"] # tuple of shape 2\n",
    "agent._features[\"state_encoder\"][0].shape # torch.Size([1500, 512]), accumulated T * B, state_features\n",
    "agent._features[\"state_encoder\"][1].shape # torch.Size([1, 10, 512]), state_features for the next step T+1, unused in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._features[\"audio_encoder.cnn.7\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "category__state_encoder__probe = GenericProbeNetwork(512, 21).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "category__state_encoder__logits = category__state_encoder__probe(agent._features[\"state_encoder\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0344, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category__state_encoder__cel = F.cross_entropy(category__state_encoder__logits, category_list)\n",
    "category__state_encoder__cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_FEATURES__RAW = {k: {} for k in MODEL_VARIANTS_TO_AGENTMODEL.keys()}\n",
    "AGENT_RNN_HIDDEN_STATE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of updates: 333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category|state_encoder__ppo_gru__random__probe_loss: 2.994\n",
      "category|audio_encoder.cnn.7__ppo_gru__random__probe_loss: 3.044\n",
      "category|visual_encoder.cnn.7__ppo_gru__random__probe_loss: 2.951\n",
      "category|state_encoder__ppo_bc__rgbd_spectro__pgwt__SAVi__probe_loss: 2.889\n",
      "category|audio_encoder.cnn.7__ppo_bc__rgbd_spectro__pgwt__SAVi__probe_loss: 3.029\n",
      "category|visual_encoder.cnn.7__ppo_bc__rgbd_spectro__pgwt__SAVi__probe_loss: 3.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Could we maybe pre-cmpute all the foreward passes for all the model variants once,\n",
    "# then we don't have to re-run those in case we train for more than one epoch ?\n",
    "# Although even the \"epoch\" is not a real epoch, since we don't have the guarantee\n",
    "# that all the steps are sampled exactly once.\n",
    "\n",
    "# Training start\n",
    "start_time = time.time()\n",
    "\n",
    "# NOTE: this time total-steps means how many time .backward() is called on each probe\n",
    "# One epoch would be equual to \"DATASET_SIZE\" in steps / (num_envs * num_steps)\n",
    "n_updates = 0\n",
    "total_updates = int(500_000 / args.num_envs / args.num_steps) # How many updates expected in total for one epoch ?\n",
    "print(f\"Expected number of updates: {total_updates}\")\n",
    "\n",
    "steps_per_update = args.num_envs * args.num_steps\n",
    "for global_step in range(1, args.total_steps + steps_per_update, steps_per_update):\n",
    "    # Load batch data\n",
    "    obs_list, action_list, _, done_list = \\\n",
    "        [ {k: th.Tensor(v).float().to(device) for k,v in b.items()} if isinstance(b, dict) else \n",
    "            b.float().to(device) for b in next(dloader)]\n",
    "    \n",
    "    # NOTE: RGB are normalized in the VisualCNN module\n",
    "    # PPO networks expect input of shape T,B, ... so doing the permutation first\n",
    "    # then flatten over T x B dimensions. The RNN will reshape it as necessary\n",
    "    for k, v in obs_list.items():\n",
    "        if k in [\"rgb\", \"spectrogram\", \"depth\"]:\n",
    "            obs_list[k] = v.permute(1, 0, 2, 3, 4) # BTCHW -> TBCHW\n",
    "            obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-3:])\n",
    "        elif k in [\"audiogoal\"]:\n",
    "            obs_list[k] = v.permute(1, 0, 2, 3) # BTCL -> TBCL\n",
    "            obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-2:])\n",
    "        else:\n",
    "            # TODO: handle other fields like \"category\", etc...\n",
    "            pass\n",
    "    \n",
    "    action_list = action_list.permute(1, 0, 2) # TODO: probably uneeded for probing ?\n",
    "    done_list = done_list.permute(1, 0, 2)\n",
    "    mask_list = 1. - done_list\n",
    "    \n",
    "    prev_actions_list = th.zeros_like(action_list)\n",
    "    prev_actions_list[1:] = action_list[:-1]\n",
    "    prev_actions_list = F.one_hot(prev_actions_list.long()[:, :, 0], num_classes=4).float()\n",
    "    prev_actions_list[0] = prev_actions_list[0] * 0.0\n",
    "\n",
    "    # Finally, also flatten across T x B, let the RNN do the unflattening if needs be\n",
    "    action_list = action_list.reshape(-1) # TODO: probably uneeded for probing ?\n",
    "    done_list = done_list.reshape(-1, 1)\n",
    "    mask_list = mask_list.reshape(-1, 1)\n",
    "    prev_actions_list = prev_actions_list.reshape(-1, 1)\n",
    "\n",
    "    # Holder for the probe losses and accs.\n",
    "    probe_losses_dict = {}\n",
    "\n",
    "    # For each \"agent_variant\", iterate\n",
    "    # TODO: once we have more than \"category\", it is more efficient to iterate over the agent first,\n",
    "    # Do the forward pass, then iterate over the probe target (category, scene, etc...) then\n",
    "    for agent_variant, agent in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "        # Forward pass with the agent model to collect the intermediate features\n",
    "        # Stores in agent_features\n",
    "        # This will be used to recompute the rnn_hidden_states when computiong the new action logprobs\n",
    "        if agent_variant.__contains__(\"gru\"):\n",
    "            rnn_hidden_state = th.zeros((1, args.batch_chunk_length, args.hidden_size), device=device)\n",
    "        elif agent_variant.__contains__(\"pgwt\"):\n",
    "            rnn_hidden_state = agent.state_encoder.latents.repeat(args.batch_chunk_length, 1, 1)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported agent-type:{agent_variant}\")\n",
    "        \n",
    "        with th.no_grad():\n",
    "            agent_outputs = agent.act(obs_list, rnn_hidden_state, masks=mask_list) #, prev_actions=prev_actions_list)\n",
    "        \n",
    "        for probe_target_name, probe_target_dict in PROBES.items():\n",
    "            # probe_target_name: \"category\", \"scene\", more generally the targeted concept of the probing\n",
    "            # probe_target_dict: { \"state_encoder\": {\"agent_variant\": Torch Model} }\n",
    "            for probe_target_input_name, agent_variant_probes in probe_target_dict.items():\n",
    "                # probe_target_input_name: the input of the probe, such as \"state_encoder\", and other\n",
    "                # agent_variant_probes: dict taht holds {\"agent_variant\": Torch Model}\n",
    "                \n",
    "                probe = agent_variant_probes[agent_variant][\"probe_network\"]\n",
    "                probe_optim = agent_variant_probes[agent_variant][\"probe_optimizer\"]\n",
    "                probe_optim.zero_grad()\n",
    "\n",
    "                # Forward pass of the probe network itself\n",
    "                if probe_target_input_name == \"state_encoder\":\n",
    "                    probe_inputs = agent._features[\"state_encoder\"][0]\n",
    "                elif probe_target_input_name.__contains__(\"visual_encoder\") or \\\n",
    "                     probe_target_input_name.__contains__(\"audio_encoder\"):\n",
    "                    probe_inputs = agent._features[probe_target_input_name]\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Attempt to use {probe_target_input_name} as probe input.\")\n",
    "                \n",
    "                probe_logits = probe(probe_inputs)\n",
    "                \n",
    "                # TODO: generate probe_targets\n",
    "                if probe_target_name == \"category\":\n",
    "                    probe_targets = obs_list[\"category\"].reshape(-1, 21).argmax(axis=1)\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Unsupported probe target: {probe_target_name}.\")\n",
    "                \n",
    "                # Loss\n",
    "                # TODO: CE weights depending on the probing target and such\n",
    "                probe_ce_loss = F.cross_entropy(probe_logits, probe_targets)\n",
    "\n",
    "                probe_ce_loss.backward()\n",
    "                probe_optim.step()\n",
    "\n",
    "                # Store the loss valuesl for logging later\n",
    "                metric_stem = f\"{probe_target_name}|{probe_target_input_name}__{agent_variant}\"\n",
    "                loss_name = f\"{metric_stem}__probe_loss\"\n",
    "                probe_losses_dict[loss_name] = probe_ce_loss.item()\n",
    "                acc_name = f\"{metric_stem}__probe_acc\"\n",
    "                probe_losses_dict[acc_name] = (F.softmax(probe_logits, dim=1).argmax(1) == probe_targets).float().mean()\n",
    "\n",
    "    # Tracking the number of NN updates (for all probes)\n",
    "    n_updates += 1\n",
    "\n",
    "    if n_updates % 10 == 0:\n",
    "        for k, v in probe_losses_dict.items():\n",
    "            print(f\"{k}: {round(v,3)}\")\n",
    "        print(\"\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1007, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_logits.shape\n",
    "probe_targets.shape\n",
    "(F.softmax(probe_logits, dim=1).argmax(1) == probe_targets).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
