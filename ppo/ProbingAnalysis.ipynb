{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rousslan/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-13 17:16:08.677933: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 17:16:08.748199: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-13 17:16:08.748224: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# General config related\n",
    "import os\n",
    "import umap\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import rsatoolbox\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "# Custom imports\n",
    "from configurator import get_arg_dict, generate_args\n",
    "\n",
    "# ML deps\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plottign deps\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec # TODO: move to the top\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Env config related\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "from ss_baselines.common.utils import plot_top_down_map\n",
    "\n",
    "# Dataset utils\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import compress_pickle as cpkl\n",
    "\n",
    "# Loading pretrained agent\n",
    "import tools\n",
    "import models\n",
    "from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic\n",
    "\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"white\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "    \n",
    "    # Behavior cloning gexperiment config\n",
    "    get_arg_dict(\"dataset-path\", str, \"SAVI_Oracle_Dataset_v0\"),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/savi/savi_ss1.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 10), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.0), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"perceiver-gwt-gwwm\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, False, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    ## Special BC\n",
    "    get_arg_dict(\"prev-actions\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"burn-in\", int, 0), # Steps used to init the latent state for RNN component\n",
    "    get_arg_dict(\"batch-chunk-length\", int, 0), # For gradient accumulation\n",
    "    get_arg_dict(\"dataset-ce-weights\", bool, True, metatype=\"bool\"), # If True, will read CEL weights based on action dist. from the 'dataset_statistics.bz2' file.\n",
    "    get_arg_dict(\"ce-weights\", float, None, metatype=\"list\"), # Weights for the Cross Entropy loss\n",
    "\n",
    "    # Eval protocol\n",
    "    get_arg_dict(\"eval\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"eval-every\", int, int(1.5e4)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"eval-n-episodes\", int, 5),\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "# endregion: Generating additional hyparams\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Gradient accumulation support\n",
    "if args.batch_chunk_length == 0:\n",
    "    args.batch_chunk_length = args.num_envs\n",
    "\n",
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "th.cuda.manual_seed_all(args.seed)\n",
    "th.backends.cudnn.deterministic = args.torch_deterministic\n",
    "# th.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "# Set device as GPU\n",
    "device = tools.get_device(args) if (not args.cpu and th.cuda.is_available()) else th.device(\"cpu\")\n",
    "\n",
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "## Override default seed\n",
    "env_config.SEED = env_config.TASK_CONFIG.SEED = env_config.TASK_CONFIG.SIMULATOR.SEED = args.seed\n",
    "\n",
    "env_config.TASK_CONFIG.SIMULATOR.USE_RENDERED_OBSERVATIONS = False\n",
    "# For smoother video, set CONTINUOUS_VIEW_CHANGE to True, and get the additional frames in obs_dict[\"intermediate\"]\n",
    "env_config.TASK_CONFIG.SIMULATOR.CONTINUOUS_VIEW_CHANGE = False\n",
    "\n",
    "env_config.TASK_CONFIG.SIMULATOR.RGB_SENSOR.WIDTH = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.RGB_SENSOR.HEIGHT = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.DEPTH_SENSOR.WIDTH = 256\n",
    "env_config.TASK_CONFIG.SIMULATOR.DEPTH_SENSOR.HEIGHT = 256\n",
    "\n",
    "# NOTE: using less environments for eval to save up system memory -> run more experiment at the same time\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "agent_extra_rgb = False\n",
    "if args.save_videos:\n",
    "    # For RGB video sensors\n",
    "    if \"RGB_SENSOR\" not in env_config.SENSORS:\n",
    "        env_config.SENSORS.append(\"RGB_SENSOR\")\n",
    "        # Indicates to the agent that RGB obs should not be used as observational inputs\n",
    "        agent_extra_rgb = True\n",
    "    # For Waveform to generate audio over the videos\n",
    "    if \"AUDIOGOAL_SENSOR\" not in env_config.TASK_CONFIG.TASK.SENSORS:\n",
    "        env_config.TASK_CONFIG.TASK.SENSORS.append(\"AUDIOGOAL_SENSOR\")\n",
    "# Add support for TOP_DOWN_MAP\n",
    "# NOTE: it seems to induce \"'DummySimulator' object has no attribute 'pathfinder'\" error\n",
    "# If top down map really needed, probably have to run the env without pre-rendered observations ?\n",
    "# env_config.TASK_CONFIG.TASK.MEASUREMENTS.append(\"TOP_DOWN_MAP\")\n",
    "\n",
    "env_config.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict(audiogoal:Box(-3.4028235e+38, 3.4028235e+38, (2, 16000), float32), depth:Box(0, 255, (128, 128, 1), uint8), rgb:Box(0, 255, (128, 128, 3), uint8), spectrogram:Box(-3.4028235e+38, 3.4028235e+38, (65, 26, 2), float32)),\n",
       " Discrete(4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake environment instantiation to create the agent models later on\n",
    "\n",
    "# TODO: add dyanmicallly set single_observation_space so that RGB and RGBD based variants\n",
    "# can be evaluated at thet same time\n",
    "from gym import spaces\n",
    "single_action_space = spaces.Discrete(4)\n",
    "single_observation_space = spaces.Dict({\n",
    "    \"rgb\": spaces.Box(shape=[128,128,3], low=0, high=255, dtype=np.uint8),\n",
    "    \"depth\": spaces.Box(shape=[128,128,1], low=0, high=255, dtype=np.uint8),\n",
    "    \"audiogoal\": spaces.Box(shape=[2,16000], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32),\n",
    "    \"spectrogram\": spaces.Box(shape=[65,26,2], low=-3.4028235e+38, high=3.4028235e+38, dtype=np.float32)\n",
    "})\n",
    "# single_observation_space = envs.observation_spaces[0]\n",
    "# single_action_space = envs.action_spaces[0]\n",
    "\n",
    "single_observation_space, single_action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing Analysis Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target of probing\n",
    "## \"category\" -> how easy to predict category based on the learned features / inputs\n",
    "## \"scene\" -> how easy to predict scene based on the learned features / inputs\n",
    "PROBING_TARGETS = {\n",
    "    \"category\": {\"n_classes\": 21},\n",
    "    # \"scene\": {\"n_classes\": 10}, # TODO: make this based on the dataset ?\n",
    "}\n",
    "\n",
    "# Define which fields of an agent to use for the probes\n",
    "PROBING_INPUTS = [\"state_encoder\"]\n",
    "\n",
    "# Define the probing \"subjects\", i.e. which pre-trained BC networks to probe\n",
    "# also stores info. related to the path to the weights, and pretty names for the plots\n",
    "MODEL_VARIANTS_TO_STATEDICT_PATH = {\n",
    "    # region: Random baselines\n",
    "    # Random GRU Baseline\n",
    "    \"ppo_gru__random\": {\n",
    "        \"pretty_name\": \"GRU Random\",\n",
    "        \"state_dict_path\": \"\"\n",
    "    },\n",
    "    # Random PGWT Baseline\n",
    "    # \"ppo_pgwt__random\": {\n",
    "    #     \"pretty_name\": \"TransRNN Random\",\n",
    "    #     \"state_dict_path\": \"\"\n",
    "    # },\n",
    "    # endregion: Random baselines\n",
    "\n",
    "\n",
    "    # region: SAVi BC variants; trained using RGBD + Spectrogram ; trained up to 5M steps\n",
    "    # \"ppo_bc__rgbd_spectro__gru__SAVi\": {\n",
    "    #     \"pretty_name\": \"[SAVi BC] PPO GRU | RGB Spectro\",\n",
    "    #     \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "    #         \"ppo_bc__savi_ss1_rgbd_spectro__gru_seed_111__2023_06_10_16_05_39_999286.musashi\"\n",
    "    #         \"/models/ppo_agent.4995001.ckpt.pth\"\n",
    "    # },\n",
    "    \"ppo_bc__rgbd_spectro__pgwt__SAVi\": {\n",
    "        \"pretty_name\": \"[SAVi BC] PPO TransRNN | RGB Spectro\",\n",
    "        \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "            \"ppo_bc__savi_ss1_rgbd__spectro__pgwt__dpth_1_nlats_8_latdim_64_noSA_CAnheads_1_SAnheads_4_modembed_0_CAprevlats_seed_111__2023_06_10_16_05_37_098602.musashi\"\n",
    "            \"/models/ppo_agent.4995001.ckpt.pth\"\n",
    "    },\n",
    "    # endregion: SAVi BC variants; trained using RGBD + Spectrogram ; trained up to 5M steps\n",
    "}\n",
    "\n",
    "# Indexable instantiated agent models (Torch agents)\n",
    "MODEL_VARIANTS_TO_AGENTMODEL = {}\n",
    "\n",
    "for k, v in MODEL_VARIANTS_TO_STATEDICT_PATH.items():\n",
    "    args_copy = copy.copy(args)\n",
    "    # Override args depending on the model in use\n",
    "    if k.__contains__(\"gru\"):\n",
    "        agent = ActorCritic(single_observation_space, single_action_space, args.hidden_size, extra_rgb=False,\n",
    "            analysis_layers=models.GRU_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES)\n",
    "    elif k.__contains__(\"pgwt\"):\n",
    "        agent = Perceiver_GWT_GWWM_ActorCritic(single_observation_space, single_action_space, args, extra_rgb=False,\n",
    "            analysis_layers=models.PGWT_GWWM_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES + [\"state_encoder.ca.mha\"])\n",
    "\n",
    "    agent.eval()\n",
    "    # Load the model weights\n",
    "    # TODO: add map location device to use CPU only ?\n",
    "    if v[\"state_dict_path\"] != \"\":\n",
    "        agent_state_dict = th.load(v[\"state_dict_path\"])\n",
    "        agent.load_state_dict(agent_state_dict)\n",
    "    \n",
    "    MODEL_VARIANTS_TO_AGENTMODEL[k] = agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (visual_encoder): VisualCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=2304, out_features=512, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): AudioCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(2, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=2496, out_features=512, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (state_encoder): RNNStateEncoder(\n",
       "    (rnn): GRU(1024, 512)\n",
       "  )\n",
       "  (action_distribution): CategoricalNet(\n",
       "    (linear): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       "  (critic): CriticHead(\n",
       "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_VARIANTS_TO_AGENTMODEL.keys() # ['ppo_gru__random', 'ppo_bc__rgbd_spectro__pgwt__SAVi']\n",
    "MODEL_VARIANTS_TO_AGENTMODEL[\"ppo_gru__random\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe network definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - consider adding the reference to the network this probe is in charge of ?\n",
    "class GenericProbeNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # input_dim: shape of the \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating probes\n",
    "PROBES = {}\n",
    "for probe_target_name, probe_target_info in PROBING_TARGETS.items():\n",
    "    if probe_target_name not in PROBES.keys():\n",
    "        PROBES[probe_target_name] = {}\n",
    "    for probe_input in PROBING_INPUTS: # NOTE: maybe switch order with the MODEL_VARIANTS ???\n",
    "\n",
    "        if probe_input not in PROBES[probe_target_name].keys():\n",
    "            PROBES[probe_target_name][probe_input] = {}\n",
    "\n",
    "        for agent_variant in MODEL_VARIANTS_TO_AGENTMODEL.keys():\n",
    "            probe_input_dim = 512 # TODO: make this adapt to the actual shape of the input's probe\n",
    "            probe_output_dim = probe_target_info[\"n_classes\"]\n",
    "\n",
    "            PROBES[probe_target_name][probe_input][agent_variant] = \\\n",
    "                GenericProbeNetwork(probe_input_dim, probe_output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ppo_gru__random': GenericProbeNetwork(\n",
       "   (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       " ),\n",
       " 'ppo_bc__rgbd_spectro__pgwt__SAVi': GenericProbeNetwork(\n",
       "   (linear): Linear(in_features=512, out_features=21, bias=False)\n",
       " )}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the instantiate PROBES\n",
    "PROBES[\"category\"][\"state_encoder\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset to be used for probe training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE / TODO: probe training might benefti from using different batch sizes ?\n",
    "\n",
    "# This variant will fill each batch trajectory using cat.ed episode data\n",
    "# There is no empty step in this batch\n",
    "class BCIterableDataset3(IterableDataset):\n",
    "    def __init__(self, dataset_path, batch_length, seed=111):\n",
    "        self.seed = seed\n",
    "        self.batch_length = batch_length\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        # Read episode filenames in the dataset path\n",
    "        self.ep_filenames = os.listdir(dataset_path)\n",
    "        if \"dataset_statistics.bz2\" in self.ep_filenames:\n",
    "            self.ep_filenames.remove(\"dataset_statistics.bz2\")\n",
    "        \n",
    "        print(f\"Initialized IterDset with {len(self.ep_filenames)} episodes.\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch_length = self.batch_length\n",
    "        while True:\n",
    "            # region: Sample episode data until there is enough to fill the hole batch traj\n",
    "            obs_list = {\n",
    "                \"depth\": np.zeros([batch_length, 128, 128]), # NOTE: data was recorded using (128, 128), but ideally we should have (128, 128, 1)\n",
    "                \"rgb\": np.zeros([batch_length, 128, 128, 3]),\n",
    "                \"audiogoal\": np.zeros([batch_length, 2, 16000]),\n",
    "                \"spectrogram\": np.zeros([batch_length, 65, 26, 2]),\n",
    "                \"category\": np.zeros([batch_length, 21]),\n",
    "                \"pointgoal_with_gps_compass\": np.zeros([batch_length, 2]),\n",
    "                \"pose\": np.zeros([batch_length, 4]),\n",
    "            }\n",
    "\n",
    "            action_list, reward_list, done_list = \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1]), \\\n",
    "                np.zeros([batch_length, 1])\n",
    "            ssf = 0 # Step affected so far\n",
    "            while ssf < batch_length:\n",
    "                idx = th.randint(len(self.ep_filenames), ())\n",
    "                ep_filename = self.ep_filenames[idx]\n",
    "                ep_filepath = os.path.join(self.dataset_path, ep_filename)\n",
    "                with open(ep_filepath, \"rb\") as f:\n",
    "                    edd = cpkl.load(f)\n",
    "                # print(f\"Sampled traj idx: {idx} ; Len: {edd['ep_length']}\")\n",
    "                \n",
    "                # Append the data to the bathc trjectory\n",
    "                rs = batch_length - ssf # Reamining steps\n",
    "                horizon = ssf + min(rs, edd[\"ep_length\"])\n",
    "                for k, v in edd[\"obs_list\"].items():\n",
    "                    obs_list[k][ssf:horizon] = v[:rs]\n",
    "                action_list[ssf:horizon] = np.array(edd[\"action_list\"][:rs])[:, None]\n",
    "                reward_list[ssf:horizon] = np.array(edd[\"reward_list\"][:rs])[:, None]\n",
    "                done_list[ssf:horizon] = np.array(edd[\"done_list\"][:rs])[:, None]\n",
    "\n",
    "                ssf += edd[\"ep_length\"]\n",
    "\n",
    "                if ssf >= self.batch_length:\n",
    "                    break\n",
    "\n",
    "            # Adjust shape of \"depth\" to be [T, H, W, 1] instead of [T, H, W]\n",
    "            obs_list[\"depth\"] = obs_list[\"depth\"][:, :, :, None]\n",
    "            \n",
    "            yield obs_list, action_list, reward_list, done_list\n",
    "            # endregion: Sample episode data until there is enough to fill the hole batch traj\n",
    "    \n",
    "def make_dataloader3(dataset_path, batch_size, batch_length, seed=111, num_workers=2):\n",
    "    def worker_init_fn(worker_id):\n",
    "        # worker_seed = th.initial_seed() % (2 ** 32)\n",
    "        worker_seed = 133754134 + worker_id\n",
    "\n",
    "        random.seed(worker_seed)\n",
    "        np.random.seed(worker_seed)\n",
    "\n",
    "    th_seed_gen = th.Generator()\n",
    "    th_seed_gen.manual_seed(133754134 + seed)\n",
    "\n",
    "    dloader = iter(\n",
    "        DataLoader(\n",
    "            BCIterableDataset3(\n",
    "                dataset_path=dataset_path, batch_length=batch_length),\n",
    "                batch_size=batch_size, num_workers=num_workers,\n",
    "                worker_init_fn=worker_init_fn, generator=th_seed_gen\n",
    "            )\n",
    "    )\n",
    "\n",
    "    return dloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized IterDset with 29001 episodes.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset object\n",
    "dloader = make_dataloader3(args.dataset_path, batch_size=args.num_envs,\n",
    "                            batch_length=args.num_steps, seed=args.seed, num_workers=8)\n",
    "\n",
    "# TODO: consider pre-computing CE weights for categories / scenes to balance the CE loss ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing iteration over one batch of data for a given variant\n",
    "\n",
    "# region: Load batch data, and related pre-processing\n",
    "obs_list, action_list, _, done_list = \\\n",
    "    [ {k: th.Tensor(v).float().to(device) for k,v in b.items()} if isinstance(b, dict) else \n",
    "        b.float().to(device) for b in next(dloader)]\n",
    "\n",
    "# NOTE: RGB are normalized in the VisualCNN module\n",
    "# PPO networks expect input of shape T,B, ... so doing the permutation first\n",
    "# then flatten over T x B dimensions. The RNN will reshape it as necessary\n",
    "for k, v in obs_list.items():\n",
    "    if k in [\"rgb\", \"spectrogram\", \"depth\"]:\n",
    "        obs_list[k] = v.permute(1, 0, 2, 3, 4) # BTCHW -> TBCHW\n",
    "        obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-3:])\n",
    "    elif k in [\"audiogoal\"]:\n",
    "        obs_list[k] = v.permute(1, 0, 2, 3) # BTCL -> TBCL\n",
    "        obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-2:])\n",
    "    else:\n",
    "        # TODO: handle other fields like \"category\", etc...\n",
    "        pass\n",
    "\n",
    "action_list = action_list.permute(1, 0, 2)\n",
    "done_list = done_list.permute(1, 0, 2)\n",
    "mask_list = 1. - done_list\n",
    "\n",
    "prev_actions_list = th.zeros_like(action_list)\n",
    "prev_actions_list[1:] = action_list[:-1]\n",
    "prev_actions_list = F.one_hot(prev_actions_list.long()[:, :, 0], num_classes=4).float()\n",
    "prev_actions_list[0] = prev_actions_list[0] * 0.0\n",
    "\n",
    "# Finally, also flatten across T x B, let the RNN do the unflattening if needs be\n",
    "action_list = action_list.reshape(-1) # Because it is used for the target later\n",
    "done_list = done_list.reshape(-1, 1)\n",
    "mask_list = mask_list.reshape(-1, 1)\n",
    "prev_actions_list = prev_actions_list.reshape(-1, 1)\n",
    "# endregion: Load batch data, and related pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_list.keys() # ['depth', 'rgb', 'audiogoal', 'spectrogram', 'category', 'pointgoal_with_gps_compass', 'pose']\n",
    "obs_list[\"depth\"].shape # torch.Size([1500, 128, 128, 1])\n",
    "done_list.shape # torch.Size([1500, 1])\n",
    "mask_list.shape # torch.Size([1500, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each \"agent_variant\", iterate\n",
    "\n",
    "agent_variant = \"ppo_gru__random\"\n",
    "agent = MODEL_VARIANTS_TO_AGENTMODEL[\"ppo_gru__random\"]\n",
    "\n",
    "# This will be used to recompute the rnn_hidden_states when computiong the new action logprobs\n",
    "if agent_variant.__contains__(\"gru\"):\n",
    "    rnn_hidden_state = th.zeros((1, args.batch_chunk_length, args.hidden_size), device=device)\n",
    "elif agent_variant.__contains__(\"pgwt\"):\n",
    "    rnn_hidden_state = agent.state_encoder.latents.repeat(args.batch_chunk_length, 1, 1)\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unsupported agent-type:{agent_variant}\")\n",
    "\n",
    "# for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "#     if agent_variant.__contains__(\"gru\"):\n",
    "#         AGENT_RNN_HIDDEN_STATE[agent_variant] = th.zeros((1, args.num_envs, args.hidden_size), device=device)\n",
    "#     elif agent_variant.__contains__(\"pgwt\"):\n",
    "#         AGENT_RNN_HIDDEN_STATE[agent_variant] = agent_model.state_encoder.latents.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_hidden_state.shape # torch.Size([1, 10, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the networks\n",
    "# TODO: maybe detach the rnn_hidden_state between two chunks ?\n",
    "actions, _, _, action_logits, entropies, _, _ = \\\n",
    "    agent.act(obs_list, rnn_hidden_state, masks=mask_list) #, prev_actions=prev_actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 150)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_envs, args.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1500, 1]), torch.Size([1500, 4]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape, action_logits.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visual_encoder.cnn.0',\n",
       " 'visual_encoder.cnn.1',\n",
       " 'visual_encoder.cnn.2',\n",
       " 'visual_encoder.cnn.3',\n",
       " 'visual_encoder.cnn.4',\n",
       " 'visual_encoder.cnn.5',\n",
       " 'visual_encoder.cnn.6',\n",
       " 'visual_encoder.cnn.7',\n",
       " 'audio_encoder.cnn.0',\n",
       " 'audio_encoder.cnn.1',\n",
       " 'audio_encoder.cnn.2',\n",
       " 'audio_encoder.cnn.3',\n",
       " 'audio_encoder.cnn.4',\n",
       " 'audio_encoder.cnn.5',\n",
       " 'audio_encoder.cnn.6',\n",
       " 'audio_encoder.cnn.7',\n",
       " 'action_distribution.linear',\n",
       " 'critic.fc',\n",
       " 'state_encoder']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(agent._features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._features[\"state_encoder\"] # tuple of shape 2\n",
    "agent._features[\"state_encoder\"][0].shape # torch.Size([1500, 512]), accumulated T * B, state_features\n",
    "agent._features[\"state_encoder\"][1].shape # torch.Size([1, 10, 512]), state_features for the next step T+1, unused in our case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_FEATURES__RAW = {k: {} for k in MODEL_VARIANTS_TO_AGENTMODEL.keys()}\n",
    "AGENT_RNN_HIDDEN_STATE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported agent-type:\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39magent_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Forward pass through the networks\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# TODO: maybe detach the rnn_hidden_state between two chunks ?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m actions, _, _, action_logits, entropies, _, _ \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mact(obs_list, rnn_hidden_state,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m         masks\u001b[39m=\u001b[39;49mmask_list) \u001b[39m#, prev_actions=prev_actions_list)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# TODO: collect the state features of interest for probing\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmusashi/home/rousslan/random/rl/ss-hab/ppo/ProbingAnalysis.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/random/rl/ss-hab-headless-py39/ppo/models.py:733\u001b[0m, in \u001b[0;36mPerceiver_GWT_GWWM_ActorCritic.act\u001b[0;34m(self, observations, rnn_hidden_states, masks, deterministic, actions, value_feat_detach, actor_feat_detach)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, observations, rnn_hidden_states, masks, deterministic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, actions\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    732\u001b[0m               value_feat_detach\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, actor_feat_detach\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 733\u001b[0m     features, rnn_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(observations, rnn_hidden_states, masks)\n\u001b[1;32m    735\u001b[0m     \u001b[39m# Estimate the value function\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(features\u001b[39m.\u001b[39mdetach() \u001b[39mif\u001b[39;00m value_feat_detach \u001b[39melse\u001b[39;00m features)\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/random/rl/ss-hab-headless-py39/ppo/models.py:713\u001b[0m, in \u001b[0;36mPerceiver_GWT_GWWM_ActorCritic.forward\u001b[0;34m(self, observations, prev_latents, masks)\u001b[0m\n\u001b[1;32m    709\u001b[0m x1 \u001b[39m=\u001b[39m []\n\u001b[1;32m    711\u001b[0m \u001b[39m# Extracts audio featues\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# TODO: consider having waveform data too ?\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m audio_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_encoder(observations)\n\u001b[1;32m    714\u001b[0m x1\u001b[39m.\u001b[39mappend(audio_features[:, \u001b[39mNone\u001b[39;00m, :]) \u001b[39m# [B, H] -> [B, 1, H]\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# Extracts vision features\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39m## TODO: consider having\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[39m## - rgb and depth simulatenous input as 4 channel dim input\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[39m## - deparate encoders for rgb and depth, give one more modality to PGWT\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/random/rl/ss-hab-headless-py39/ppo/models.py:257\u001b[0m, in \u001b[0;36mAudioCNN.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    253\u001b[0m cnn_input\u001b[39m.\u001b[39mappend(audio_observations)\n\u001b[1;32m    255\u001b[0m cnn_input \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat(cnn_input, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(cnn_input)\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/ss-hab-headless-py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Could we maybe pre-cmpute all the foreward passes for all the model variants once,\n",
    "# then we don't have to re-run those in case we train for more than one epoch ?\n",
    "# Although even the \"epoch\" is not a real epoch, since we don't have the guarantee\n",
    "# that all the steps are sampled exactly once.\n",
    "\n",
    "# Training start\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "\n",
    "# NOTE: this time total-steps means how many time .backward() is called on each probe\n",
    "# One epoch would be equual to \"DATASET_SIZE\" in steps / (num_envs * num_steps)\n",
    "n_updates = int(500_000 / args.num_envs / args.num_steps) # How many updates expected in total for one epoch ?\n",
    "\n",
    "for global_step in range(1, n_updates+1):\n",
    "    # Load batch data\n",
    "    obs_list, action_list, _, done_list = \\\n",
    "        [ {k: th.Tensor(v).float().to(device) for k,v in b.items()} if isinstance(b, dict) else \n",
    "            b.float().to(device) for b in next(dloader)]\n",
    "    \n",
    "    # NOTE: RGB are normalized in the VisualCNN module\n",
    "    # PPO networks expect input of shape T,B, ... so doing the permutation first\n",
    "    # then flatten over T x B dimensions. The RNN will reshape it as necessary\n",
    "    for k, v in obs_list.items():\n",
    "        if k in [\"rgb\", \"spectrogram\", \"depth\"]:\n",
    "            obs_list[k] = v.permute(1, 0, 2, 3, 4) # BTCHW -> TBCHW\n",
    "            obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-3:])\n",
    "        elif k in [\"audiogoal\"]:\n",
    "            obs_list[k] = v.permute(1, 0, 2, 3) # BTCL -> TBCL\n",
    "            obs_list[k] = obs_list[k].reshape(-1, *obs_list[k].shape[-2:])\n",
    "        else:\n",
    "            # TODO: handle other fields like \"category\", etc...\n",
    "            pass\n",
    "    \n",
    "    action_list = action_list.permute(1, 0, 2)\n",
    "    done_list = done_list.permute(1, 0, 2)\n",
    "    mask_list = 1. - done_list\n",
    "    \n",
    "    prev_actions_list = th.zeros_like(action_list)\n",
    "    prev_actions_list[1:] = action_list[:-1]\n",
    "    prev_actions_list = F.one_hot(prev_actions_list.long()[:, :, 0], num_classes=4).float()\n",
    "    prev_actions_list[0] = prev_actions_list[0] * 0.0\n",
    "\n",
    "    # Finally, also flatten across T x B, let the RNN do the unflattening if needs be\n",
    "    action_list = action_list.reshape(-1) # Because it is used for the target later\n",
    "    done_list = done_list.reshape(-1, 1)\n",
    "    mask_list = mask_list.reshape(-1, 1)\n",
    "    prev_actions_list = prev_actions_list.reshape(-1, 1)\n",
    "\n",
    "    # For each \"agent_variant\", iterate\n",
    "    # This will be used to recompute the rnn_hidden_states when computiong the new action logprobs\n",
    "    if args.agent_type == \"ss-default\":\n",
    "        rnn_hidden_state = th.zeros((1, args.batch_chunk_length, args.hidden_size), device=device)\n",
    "    elif args.agent_type in [\"perceiver-gwt-gwwm\"]:\n",
    "        rnn_hidden_state = agent.state_encoder.latents.repeat(args.batch_chunk_length, 1, 1)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported agent-type:{args.agent_type}\")\n",
    "    \n",
    "    # Forward pass through the networks\n",
    "    # TODO: maybe detach the rnn_hidden_state between two chunks ?\n",
    "    actions, _, _, action_logits, entropies, _, _ = \\\n",
    "        agent.act(obs_list, rnn_hidden_state,\n",
    "            masks=mask_list) #, prev_actions=prev_actions_list)\n",
    "\n",
    "    # TODO: collect the state features of interest for probing\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss-hab-headless-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
