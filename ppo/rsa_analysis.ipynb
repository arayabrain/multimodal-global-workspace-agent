{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "import rsatoolbox\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"savefig.facecolor\"] = \"white\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support or argpase\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "# General config related\n",
    "from configurator import get_arg_dict, generate_args\n",
    "\n",
    "# Env config related\n",
    "from ss_baselines.av_nav.config import get_config\n",
    "from ss_baselines.savi.config.default import get_config as get_savi_config\n",
    "from ss_baselines.common.env_utils import construct_envs\n",
    "from ss_baselines.common.environments import get_env_class\n",
    "\n",
    "# region: Generating additional hyparams\n",
    "CUSTOM_ARGS = [\n",
    "    # General hyper parameters\n",
    "    get_arg_dict(\"seed\", int, 111),\n",
    "    get_arg_dict(\"total-steps\", int, 1_000_000),\n",
    "\n",
    "    # SS env config\n",
    "    get_arg_dict(\"config-path\", str, \"env_configs/audiogoal_rgb_nocont.yaml\"),\n",
    "\n",
    "    # PPO Hyper parameters\n",
    "    get_arg_dict(\"num-envs\", int, 1), # Number of parallel envs. 10 by default\n",
    "    get_arg_dict(\"num-steps\", int, 150), # For each env, how many steps are collected to form PPO Agent rollout.\n",
    "    get_arg_dict(\"num-minibatches\", int, 1), # Number of mini-batches the rollout data is split into to make the updates\n",
    "    get_arg_dict(\"update-epochs\", int, 4), # Number of gradient step for the policy and value networks\n",
    "    get_arg_dict(\"gamma\", float, 0.99),\n",
    "    get_arg_dict(\"gae-lambda\", float, 0.95),\n",
    "    get_arg_dict(\"norm-adv\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"clip-coef\", float, 0.1), # Surrogate loss clipping coefficient\n",
    "    get_arg_dict(\"clip-vloss\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"ent-coef\", float, 0.2), # Entropy loss coef; 0.2 in SS baselines\n",
    "    get_arg_dict(\"vf-coef\", float, 0.5), # Value loss coefficient\n",
    "    get_arg_dict(\"max-grad-norm\", float, 0.5),\n",
    "    get_arg_dict(\"target-kl\", float, None),\n",
    "    get_arg_dict(\"lr\", float, 2.5e-4), # Learning rate\n",
    "    get_arg_dict(\"optim-wd\", float, 0), # weight decay for adam optim\n",
    "    ## Agent network params\n",
    "    get_arg_dict(\"agent-type\", str, \"ss-default\", metatype=\"choice\",\n",
    "        choices=[\"ss-default\", \"deep-etho\",\n",
    "                    \"perceiver-gwt-gwwm\", \"perceiver-gwt-attgru\"]),\n",
    "    get_arg_dict(\"hidden-size\", int, 512), # Size of the visual / audio features and RNN hidden states \n",
    "    ## Perceiver / PerceiverIO params: TODO: num_latnets, latent_dim, etc...\n",
    "    get_arg_dict(\"pgwt-latent-type\", str, \"randn\", metatype=\"choice\",\n",
    "        choices=[\"randn\", \"zeros\"]), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-latent-learned\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-depth\", int, 1), # Depth of the Perceiver\n",
    "    get_arg_dict(\"pgwt-num-latents\", int, 8),\n",
    "    get_arg_dict(\"pgwt-latent-dim\", int, 64),\n",
    "    get_arg_dict(\"pgwt-cross-heads\", int, 1),\n",
    "    get_arg_dict(\"pgwt-latent-heads\", int, 4),\n",
    "    get_arg_dict(\"pgwt-cross-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-latent-dim-head\", int, 64),\n",
    "    get_arg_dict(\"pgwt-weight-tie-layers\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-ff\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"pgwt-num-freq-bands\", int, 6),\n",
    "    get_arg_dict(\"pgwt-max-freq\", int, 10.),\n",
    "    get_arg_dict(\"pgwt-use-sa\", bool, False, metatype=\"bool\"),\n",
    "    ## Peceiver Modality Embedding related\n",
    "    get_arg_dict(\"pgwt-mod-embed\", int, 0), # Learnable modality embeddings\n",
    "    ## Additional modalities\n",
    "    get_arg_dict(\"pgwt-ca-prev-latents\", bool, True, metatype=\"bool\"), # if True, passes the prev latent to CA as KV input data\n",
    "\n",
    "    # Logging params\n",
    "    # NOTE: While supported, video logging is expensive because the RGB generation in the\n",
    "    # envs hogs a lot of GPU, especially with multiple envs \n",
    "    get_arg_dict(\"save-videos\", bool, False, metatype=\"bool\"),\n",
    "    get_arg_dict(\"save-model\", bool, True, metatype=\"bool\"),\n",
    "    get_arg_dict(\"log-sampling-stats-every\", int, int(1.5e3)), # Every X frames || steps sampled\n",
    "    get_arg_dict(\"log-training-stats-every\", int, int(10)), # Every X model update\n",
    "    get_arg_dict(\"logdir-prefix\", str, \"./logs/\") # Overrides the default one\n",
    "]\n",
    "args = generate_args(CUSTOM_ARGS)\n",
    "\n",
    "# Additional PPO overrides\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "# Load environment config\n",
    "is_SAVi = str.__contains__(args.config_path, \"savi\")\n",
    "if is_SAVi:\n",
    "    env_config = get_savi_config(config_paths=args.config_path)\n",
    "else:\n",
    "    env_config = get_config(config_paths=args.config_path)\n",
    "# endregion: Generating additional hyparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding some envs parametes from the .yaml env config\n",
    "env_config.defrost()\n",
    "env_config.NUM_PROCESSES = 1 # Corresponds to number of envs, makes script startup faster for debugs\n",
    "env_config.USE_SYNC_VECENV = True\n",
    "# env_config.USE_VECENV = False\n",
    "# env_config.CONTINUOUS = args.env_continuous\n",
    "## In caes video saving is enabled, make sure there is also the rgb videos\n",
    "env_config.freeze()\n",
    "# print(env_config)\n",
    "\n",
    "# Environment instantiation\n",
    "envs = construct_envs(env_config, get_env_class(env_config.ENV_NAME))\n",
    "single_observation_space = envs.observation_spaces[0]\n",
    "single_action_space = envs.action_spaces[0]\n",
    "\n",
    "single_observation_space, single_action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO seeding for reproducibility ? Make sure that we can control the generated episode trajs ?\n",
    "\n",
    "# Loading pretrained agent\n",
    "import models\n",
    "from models import ActorCritic, Perceiver_GWT_GWWM_ActorCritic\n",
    "\n",
    "MODEL_VARIANTS_TO_STATEDICT_PATH = {\n",
    "    # Baseline variants: BC\n",
    "    \"ppo_gru__bc\": {\n",
    "        \"pretty_name\": \"PPO GRU (BC)\",\n",
    "        \"state_dict_path\": \n",
    "            \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "            \"ppo_bc2__ss1_rgb_spectro__gru__bsize_32_bchnklen_32__nsteps_50__cew_38.6_0.67_0.84_0.78_nogradnorm_seed_111__2022_10_08_17_24_31_002321.musashi\"\n",
    "            # \"/models/ppo_agent.3990001.ckpt.pth\"\n",
    "            \"/models/ppo_agent.990001.ckpt.pth\"\n",
    "    },\n",
    "    \"ppo_pgwt__bc\": {\n",
    "        \"pretty_name\": \"PPO PGWT (BC)\",\n",
    "        \"state_dict_path\": \n",
    "            \"/home/rousslan/random/rl/exp-logs/ss-hab-bc/\"\n",
    "            \"ppo_bc2__ss1_rgb_spectro__pgwt_gwwm__dpth_1_nlats_8_latdim_64_noSA_CAprevlats__bsize_32_bchnklen_32__nsteps_50__cew_38.6_0.67_0.84_0.78_nogradnorm_seed_111__2022_10_08_17_24_31_001985.musashi\"\n",
    "            # \"/models/ppo_agent.3990001.ckpt.pth\"\n",
    "            \"/models/ppo_agent.990001.ckpt.pth\"\n",
    "    },\n",
    "\n",
    "    # Baselin variants: RL\n",
    "    # \"ppo_gru__rl\": {\n",
    "    #     \"pretty_name\": \"PPO GRU (RL)\",\n",
    "    #     \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab/ppo_av_nav__ss1_rgb_spectro_seed_111__2022_09_06_13_59_13_931240.musashi\"\n",
    "    #     \"/models/ppo_agent.994501.ckpt.pth\"\n",
    "    # },\n",
    "    # \"ppo_pgwt__rl\": {\n",
    "    #     \"pretty_name\": \"PPO PGWT (RL)\",\n",
    "    #     \"state_dict_path\": \"/home/rousslan/random/rl/exp-logs/ss-hab/ppo_av_nav__ss1_rgb_spectro__pgwt_gwwm__dpth_1_nlats_8_latdim_64_noSA_CAnheads_1_SAnheads_4_modembed_0_CAprevlats_seed_111__2022_09_06_14_43_49_094181.musashi\"\n",
    "    #     \"/models/ppo_agent.993001.ckpt.pth\"\n",
    "    # },\n",
    "    \n",
    "    # Other variants\n",
    "    ## PPO GRU and PGWT varaint with features detache for value backprop\n",
    "    # \"ppo_gru__rl__valdet\": {\n",
    "    #     \"pretty_name\": \"PPO GRU (RL - Val. Det.)\",\n",
    "    #     \"state_dict_path\":\n",
    "    #         \"/home/rousslan/random/rl/exp-logs/ss-hab/\"\n",
    "    #         \"ppo_av_nav__ss1_rgb_spectro__value_feat_detach_seed_111__2022_10_14_10_55_46_905926.musashi\"\n",
    "    #         \"/models/ppo_agent.957001.ckpt.pth\"\n",
    "    # },\n",
    "    # \"ppo_pgwt__rl_valdet\": {\n",
    "    #     \"pretty_name\": \"PPO PGWT (RL - Val. Det.)\",\n",
    "    #     \"state_dict_path\": \n",
    "    #         \"/home/rousslan/random/rl/exp-logs/ss-hab/\"\n",
    "    #         \"ppo_av_nav__ss1_rgb_spectro__pgwt_gwwm__dpth_1_nlats_8_latdim_64_noSA_CAnheads_1_SAnheads_4_modembed_0_CAprevlats__value_feat_detach_seed_111__2022_10_14_10_25_00_965286.musashi\"\n",
    "    #         \"/models/ppo_agent.987001.ckpt.pth\"\n",
    "    # },\n",
    "\n",
    "    ## PPO GRU with features detached for policy backprop\n",
    "    # \"ppo_gru__rl__poldet\": {\n",
    "    #     \"pretty_name\": \"PPO GRU (RL - Pol. Det.)\",\n",
    "    #     \"state_dict_path\":\n",
    "    #         \"/home/rousslan/random/rl/exp-logs/ss-hab/\"\n",
    "    #         \"ppo_av_nav__ss1_rgb_spectro__actor_feat_detach_seed_111__2022_10_14_17_27_11_291954.musashi\"\n",
    "    #         \"/models/ppo_agent.982501.ckpt.pth\"\n",
    "    # },\n",
    "    # \"ppo_pgwt__rl_poldet\": {\n",
    "    #     \"pretty_name\": \"PPO PGWT (RL - Pol. Det.)\",\n",
    "    #     \"state_dict_path\": \n",
    "    #         \"/home/rousslan/random/rl/exp-logs/ss-hab/\"\n",
    "    #         \"ppo_av_nav__ss1_rgb_spectro__pgwt_gwwm__dpth_1_nlats_8_latdim_64_noSA_CAnheads_1_SAnheads_4_modembed_0_CAprevlats__actor_feat_detach_seed_111__2022_10_14_17_27_11_508537.musashi\"\n",
    "    #         \"/models/ppo_agent.982501.ckpt.pth\"\n",
    "    # }\n",
    "\n",
    "    # Random baselines\n",
    "    # \"ppo_gru__random\": {\n",
    "    #     \"pretty_name\": \"PPO GRU (Rndm)\",\n",
    "    #     \"state_dict_path\": \"\"\n",
    "    # },\n",
    "    # \"ppo_pgwt__random\": {\n",
    "    #     \"pretty_name\": \"PPO PGWT (Rndm)\",\n",
    "    #     \"state_dict_path\": \"\"\n",
    "    # },\n",
    "}\n",
    "\n",
    "# 'variant named' indexed 'torch agent'\n",
    "MODEL_VARIANTS_TO_AGENTMODEL = {}\n",
    "\n",
    "for k, v in MODEL_VARIANTS_TO_STATEDICT_PATH.items():\n",
    "    args_copy = copy.copy(args)\n",
    "    # Override args depending on the model in use\n",
    "    if k.__contains__(\"gru\"):\n",
    "        agent = ActorCritic(single_observation_space, single_action_space, args.hidden_size, extra_rgb=False,\n",
    "            analysis_layers=models.GRU_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES)\n",
    "    elif k.__contains__(\"pgwt\"):\n",
    "        agent = Perceiver_GWT_GWWM_ActorCritic(single_observation_space, single_action_space, args, extra_rgb=False,\n",
    "            analysis_layers=models.PGWT_GWWM_ACTOR_CRITIC_DEFAULT_ANALYSIS_LAYER_NAMES)\n",
    "\n",
    "    agent.eval()\n",
    "    # Load the model weights\n",
    "    # TODO: add map location device to use CPU only ?\n",
    "    if v[\"state_dict_path\"] != \"\":\n",
    "        agent_state_dict = th.load(v[\"state_dict_path\"])\n",
    "        agent.load_state_dict(agent_state_dict)\n",
    "    \n",
    "    MODEL_VARIANTS_TO_AGENTMODEL[k] = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VARIANTS_TO_AGENTMODEL[\"ppo_gru__bc\"].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting episode data for feature extraction later\n",
    "# TODO: manually inspect and dave episdoe data, form a list of around 500 or 1000 observations\n",
    "# TODO: also need to make sure which agent is used to sample it. Ideally, one of the RL agents that\n",
    "# has a good enough success rate\n",
    "\n",
    "## Helper\n",
    "dev = th.device(\"cpu\")\n",
    "def tensorize_obs_dict(obs, device, observations=None, rollout_step=None):\n",
    "    obs_th = {}\n",
    "    for obs_field, _ in obs[0].items():\n",
    "        v_th = th.Tensor(np.array([step_obs[obs_field] for step_obs in obs], dtype=np.float32)).to(device)\n",
    "        # in SS1.0, the dcepth observations comes as [B, 128, 128, 1, 1], so fix that\n",
    "        if obs_field == \"depth\" and v_th.dim() == 5:\n",
    "            v_th = v_th.squeeze(-1)\n",
    "        obs_th[obs_field] = v_th\n",
    "        # Special case when doing the rollout, also stores the \n",
    "        if observations is not None:\n",
    "            observations[obs_field][rollout_step] = v_th\n",
    "    \n",
    "    return obs_th\n",
    "\n",
    "obs_list, reward_list, done_list, info_list, action_list = [], [], [], [], []\n",
    "obs_th_list, done_th_list = [], [] \n",
    "\n",
    "rnn_hidden_state = th.zeros((1, args.num_envs, args.hidden_size), device=dev)\n",
    "\n",
    "obs, done = envs.reset(), [False for _ in range(args.num_envs)]\n",
    "done_th = th.Tensor(done).to(dev)\n",
    "masks = 1. - done_th[:, None]\n",
    "\n",
    "MAX_STEPS = 256\n",
    "collected_steps = 0\n",
    "\n",
    "# while not done_th.sum() and collected_steps < MAX_STEPS:\n",
    "#     obs_th = tensorize_obs_dict(obs, dev)\n",
    "#     done_th = th.Tensor(done).to(dev)\n",
    "#     with th.no_grad():\n",
    "#         action, action_probs, action_logprobs, _, value, rnn_hidden_state = \\\n",
    "#             MODEL_VARIANTS_TO_AGENTMODEL[\"ppo_gru__rl\"].act(obs_th, rnn_hidden_state, masks=masks)\n",
    "    \n",
    "#     outputs = envs.step([a[0].item() for a in action])\n",
    "#     next_obs, reward, next_done, info = [list(x) for x in zip(*outputs)]\n",
    "\n",
    "#     # NOTE obs_list and done_list would have one extra position than info_list\n",
    "#     obs_list.append(obs)\n",
    "#     done_list.append(done)\n",
    "#     obs_th_list.append(obs_th)\n",
    "#     done_th_list.append(done_th)\n",
    "\n",
    "#     reward_list.append(reward)\n",
    "#     info_list.append(info)\n",
    "#     action_list.append(action.cpu().numpy())\n",
    "\n",
    "#     # Prepare for the next step\n",
    "#     obs = next_obs\n",
    "#     done = next_done\n",
    "\n",
    "#     collected_steps += 1\n",
    "#     # DEBUG: producing shorter sequence for downstream tests\n",
    "#     # if len(obs_list) >= 100:\n",
    "#     #     break\n",
    "\n",
    "# len(obs_list) # Episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, load data from manually saved data\n",
    "import pickle as pkl\n",
    "\n",
    "with open(\"obs_list__2022_09_21_13_35_dump.pkl\", \"rb\") as f:\n",
    "    obs_list = pkl.load(f)\n",
    "\n",
    "obs_list = obs_list[:80] # Manual capping for better visibility\n",
    "\n",
    "# Match the expected format of the obs_th_list and done_th_list below\n",
    "# type(done_th_list), len(done_th_list) # (list, 21)\n",
    "# type(done_th_list[0]), len(done_th_list[0]) # (torch.Tensor, 1)\n",
    "done_th_list2 = [th.Tensor([0]) for _ in range(len(obs_list))]\n",
    "# type(done_th_list2), len(done_th_list2) # (list, 80)\n",
    "# type(done_th_list2[0]), len(done_th_list2[0]) # (torch.Tensor, 1)\n",
    "\n",
    "\n",
    "# type(obs_th_list), len(obs_th_list)\n",
    "# type(obs_th_list[0]), len(obs_th_list[0]), list(obs_th_list[0].keys()) # (dict, 3, ['rgb', 'audiogoal', 'spectrogram'])\n",
    "# [v.shape for v in obs_th_list[0].values()] # [torch.Size([1, 128, 128, 3]), torch.Size([1, 2, 16000]), torch.Size([1, 65, 26, 2])]\n",
    "\n",
    "obs_th_list2 = [{k: th.Tensor(v)[None, :] for k,v in obs_list[i][0].items()} for i in range(len(obs_list))]\n",
    "# type(obs_th_list2[0]), len(obs_th_list2[0]), list(obs_th_list2[0].keys()) # (dict, 3, ['rgb', 'audiogoal', 'spectrogram']), matching 'obs_th_list\" expecteation\n",
    "# [v.shape for v in obs_th_list2[0].values()] # [torch.Size([1, 128, 128, 3]), torch.Size([1, 2, 16000]), torch.Size([1, 65, 26, 2])]\n",
    "\n",
    "# TODO: if stacking different episodes together, we would also need the done data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the collected observation samples, record the intermediate features\n",
    "# for each agent varaint investigated\n",
    "\n",
    "AGENT_FEATURES__RAW = {k: {} for k in MODEL_VARIANTS_TO_AGENTMODEL.keys()}\n",
    "AGENT_RNN_HIDDEN_STATE = {}\n",
    "for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "    if agent_variant.__contains__(\"gru\"):\n",
    "        AGENT_RNN_HIDDEN_STATE[agent_variant] = th.zeros((1, args.num_envs, args.hidden_size), device=dev)\n",
    "    elif agent_variant.__contains__(\"pgwt\"):\n",
    "        AGENT_RNN_HIDDEN_STATE[agent_variant] = agent_model.state_encoder.latents.clone()\n",
    "\n",
    "# TODO: Add tqdm support ?\n",
    "# for t, (obs_th, done_th) in enumerate(zip(obs_th_list, done_th_list)):\n",
    "for t, (obs_th, done_th) in enumerate(zip(obs_th_list2, done_th_list2)):\n",
    "    # TODO: if multiple episodes are cated together, need to make sure that the done_th_list reflects that\n",
    "    # other the hidden latents will not be reset\n",
    "\n",
    "    # Feeding the same sequence of observatiosn to each type of agent\n",
    "    masks = 1. - done_th[:, None]\n",
    "    with th.no_grad():\n",
    "        for agent_variant, agent_model in MODEL_VARIANTS_TO_AGENTMODEL.items():\n",
    "            # NOTE: rnn_hidden_state or pgwt's latent are already collected in \"state_encoder\" field in _features\n",
    "            _, _, _, _, _, AGENT_RNN_HIDDEN_STATE[agent_variant] = \\\n",
    "                agent_model.act(obs_th, AGENT_RNN_HIDDEN_STATE[agent_variant], masks)\n",
    "\n",
    "            # Collecting intermediate layers results\n",
    "            for k, v in agent_model._features.items():\n",
    "                if k not in list(AGENT_FEATURES__RAW[agent_variant].keys()):\n",
    "                    AGENT_FEATURES__RAW[agent_variant][k] = []\n",
    "                AGENT_FEATURES__RAW[agent_variant][k].append(v)\n",
    "\n",
    "# After recording the intermediate layers features, process them to handle the \n",
    "# various shape depending on the layers: for example MHA has different stored features\n",
    "# shape than GRU network, and so on.\n",
    "\n",
    "## Helper for cleaning up and preparing the recorded intermediate features\n",
    "def process_analysis_feats_raw(raw_dict):\n",
    "    result_dict = {}\n",
    "\n",
    "    for k, v in raw_dict.items():\n",
    "        if isinstance(v[0], th.Tensor):\n",
    "            new_v = th.cat(v, dim=0)\n",
    "        elif isinstance(v[0], tuple):\n",
    "            new_v = None # TODO\n",
    "            n_elements = len(v[0])\n",
    "            elements = [[] for _ in range(n_elements)]\n",
    "            for j in range(n_elements):\n",
    "                for i in range(len(v)):\n",
    "                    elements[j].append(v[i][j])\n",
    "            \n",
    "            new_v = [th.cat(vv, dim=0) for vv in elements]\n",
    "        else:\n",
    "            raise Exception(f\"Unhandled type: {v[0].__class__}\")\n",
    "    \n",
    "        result_dict[k] = new_v\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "AGENT_FEATURES = {} # { agent_variant -> agent_feature_dict }\n",
    "for k, v in AGENT_FEATURES__RAW.items():\n",
    "    AGENT_FEATURES[k] = process_analysis_feats_raw(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_FEATURES[\"ppo_gru__bc\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototypoing ISC distance\n",
    "variant_features = AGENT_FEATURES[\"ppo_gru__bc\"][\"state_encoder\"][0] # Extract the features of the state encoder (RNN cell)\n",
    "# variant_features.cpu().numpy() # (80, 512)\n",
    "variant_data = rsatoolbox.data.Dataset(variant_features.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data.measurements.shape\n",
    "\n",
    "from copy import deepcopy\n",
    "from rsatoolbox.rdm.rdms import RDMs\n",
    "from rsatoolbox.data import average_dataset_by\n",
    "from rsatoolbox.util.rdm_utils import _extract_triu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_input(dataset, descriptor):\n",
    "    if descriptor is None:\n",
    "        measurements = dataset.measurements\n",
    "        desc = np.arange(measurements.shape[0])\n",
    "        descriptor = 'pattern'\n",
    "    else:\n",
    "        measurements, desc, _ = average_dataset_by(dataset, descriptor)\n",
    "    return measurements, desc, descriptor\n",
    "\n",
    "def calc_rdm_euclid(dataset, descriptor=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset (rsatoolbox.data.DatasetBase):\n",
    "            The dataset the RDM is computed from\n",
    "        descriptor (String):\n",
    "            obs_descriptor used to define the rows/columns of the RDM\n",
    "            defaults to one row/column per row in the dataset\n",
    "    Returns:\n",
    "        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM\n",
    "    \"\"\"\n",
    "\n",
    "    measurements, desc, descriptor = _parse_input(dataset, descriptor)\n",
    "    sum_sq_measurements = np.sum(measurements**2, axis=1, keepdims=True)\n",
    "    rdm = sum_sq_measurements + sum_sq_measurements.T \\\n",
    "        - 2 * np.dot(measurements, measurements.T)\n",
    "    rdm = _extract_triu_(rdm) / measurements.shape[1]\n",
    "    rdm = RDMs(dissimilarities=np.array([rdm]),\n",
    "               dissimilarity_measure='squared euclidean',\n",
    "               rdm_descriptors=deepcopy(dataset.descriptors))\n",
    "    rdm.pattern_descriptors[descriptor] = desc\n",
    "    return rdm\n",
    "\n",
    "rdm = calc_rdm_euclid(variant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematic plotting for each shared intermediate layers\n",
    "all_layers = []\n",
    "for v in AGENT_FEATURES.values():\n",
    "    all_layers.extend(v.keys())\n",
    "all_layers = set(all_layers); all_layers\n",
    "\n",
    "# Extract shared layers, and other groups of layers of interest\n",
    "shared_layers = []\n",
    "visual_encoder_layernames = []\n",
    "audio_encoder_layernames = []\n",
    "for x in all_layers:\n",
    "    x_found_in_all = True\n",
    "    for v in AGENT_FEATURES.values():\n",
    "        if x not in v.keys():\n",
    "            x_found_in_all = False\n",
    "            break\n",
    "    \n",
    "    if x_found_in_all:\n",
    "        shared_layers.append(x)\n",
    "    \n",
    "    if x.startswith(\"visual_encoder\"):\n",
    "        visual_encoder_layernames.append(x)\n",
    "    if x.startswith(\"audio_encoder\"):\n",
    "        audio_encoder_layernames.append(x)\n",
    "\n",
    "# Pre-compute all sparsities\n",
    "# Helper method\n",
    "def compute_sparsity(response_list):\n",
    "    n_images, n_neurons = response_list.shape\n",
    "    neurons_sparsity_list = []\n",
    "    \n",
    "    for i in range(n_neurons):\n",
    "        # TODO: properly handle the NaN that occurs when the response is 0\n",
    "        a = (response_list[:, i].sum().pow(2) + 1e-8) / (response_list[:, i].pow(2).sum() + 1e-8)\n",
    "        b = 1 / n_images\n",
    "        neurons_sparsity_list.append( (1 - (b * a)) / (1 - b))\n",
    "    \n",
    "    return np.array(neurons_sparsity_list)\n",
    "\n",
    "VARIANT_LAYERS_SPARSITIES = {k: {} for k in MODEL_VARIANTS_TO_STATEDICT_PATH.keys()}\n",
    "\n",
    "# NOTE: manually chose the metric for the RDM computation\n",
    "# DISS_CALC_FN = rsatoolbox.rdm.calc_rdm_euclid\n",
    "DISS_CALC_FN = rsatoolbox.rdm.calc_rdm_correlation\n",
    "\n",
    "# Placeholder for the whole RDMs for each agent varaint\n",
    "VARIANTS_RDMS = {k: {} for k in AGENT_FEATURES.keys()}\n",
    "# Placeholder for the mean and std of the dissimilaritie for each variant\n",
    "VARIANT_RDMS_MEAN_STD = {k: {} for k in AGENT_FEATURES.keys()}\n",
    "\n",
    "for layername in sorted(shared_layers):\n",
    "    for agent_variant, agent_variant_features in AGENT_FEATURES.items():\n",
    "        variant_layer_features = agent_variant_features[layername]\n",
    "        if isinstance(variant_layer_features, list):\n",
    "            # If the features record for a given layer are a \"list\", then it is probablye al yaer\n",
    "            # like the MHA state_encoder, which returns more than one output: flat state_features,\n",
    "            # and the non flattented varianst: [B, NUM_LATENTS, LATENT_DIM]S. Using [0] just takes\n",
    "            # the flattened version anyway, then futher flattens it over a batch ?\n",
    "            variant_features = variant_layer_features[0].flatten(start_dim=1)\n",
    "        else:\n",
    "            # if it is not a list, then it is probably a tensor. For example the outout of an nn.Linear\n",
    "            # or state features outpout by a GRU-based StateEncoder component in the agent\n",
    "            variant_features = variant_layer_features.flatten(start_dim=1)\n",
    "\n",
    "        variant_data = rsatoolbox.data.Dataset(variant_features.cpu().numpy())\n",
    "        # Compute the RDMs for the given agent variant\n",
    "        variant_rdm = DISS_CALC_FN(variant_data)\n",
    "        \n",
    "        # Store the RDMs for subsequent plots\n",
    "        VARIANTS_RDMS[agent_variant][layername] = variant_rdm\n",
    "\n",
    "        # Pre-compute the mean and std of dissmilarities for the current layer of the current variant\n",
    "        VARIANT_RDMS_MEAN_STD[agent_variant][layername] = {\n",
    "            \"mean\": np.mean(variant_rdm.dissimilarities),\n",
    "            \"std\": np.std(variant_rdm.dissimilarities)\n",
    "        }\n",
    "\n",
    "        # Variant's layerwise sparsity\n",
    "        VARIANT_LAYERS_SPARSITIES[agent_variant][layername] = compute_sparsity(variant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_features = AGENT_FEATURES[\"ppo_gru__bc\"][\"critic.fc\"].flatten(start_dim=1)\n",
    "# variant_features = AGENT_FEATURES[\"ppo_gru__bc\"][\"state_encoder\"][0].flatten(start_dim=1)\n",
    "variant_features.cpu().numpy().shape\n",
    "variant_data = rsatoolbox.data.Dataset(variant_features.cpu().numpy())\n",
    "variant_rdm = DISS_CALC_FN(variant_data)\n",
    "variant_rdm.dissimilarities.shape\n",
    "# np.mean(variant_rdm.dissimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Average dissimilarities overview plots\n",
    "# # TODO: make it a single plot later, with one subplots for each different componets ?\n",
    "# # TODO: In case we have more than 4 variants, it might be preferable change the plotting scheem\n",
    "# #       to have the layers name as in the reference paper, and the variant names on the x-axis,\n",
    "# #       at least for the average dissmilarities plots ?\n",
    "\n",
    "# N_VARIANTS = len(VARIANT_RDMS_MEAN_STD.keys())\n",
    "# VARIANTS_COLOR_PALETTE = sns.color_palette(n_colors=N_VARIANTS)\n",
    "# VARIANTS_BAR_SHIFTING = np.linspace(-0.1, 0.1, N_VARIANTS)\n",
    "\n",
    "# ## Visual encoder section\n",
    "# visual_encoder_layernames.sort()\n",
    "# visual_encoder_layernames_relu_skipped = visual_encoder_layernames[::2]\n",
    "\n",
    "# n_rows, n_cols = 1, 1\n",
    "# fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * 8, n_rows * 6))\n",
    "\n",
    "# x = np.arange(len(visual_encoder_layernames_relu_skipped)) # 2 PPO varaint per plot for each layer\n",
    "\n",
    "\n",
    "# for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "#     y = [agent_varaint_rdms_mean_std_dict[lynm][\"mean\"] for lynm in visual_encoder_layernames_relu_skipped]\n",
    "#     yerr = [agent_varaint_rdms_mean_std_dict[lynm][\"std\"] for lynm in visual_encoder_layernames_relu_skipped]\n",
    "\n",
    "#     ax.errorbar(x-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "#                  elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "# ax.set_xlabel(\"Layers\")\n",
    "# ax.set_ylabel(\"Average Dissimilarity\")\n",
    "# ax.legend()\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(visual_encoder_layernames_relu_skipped)\n",
    "# fig.suptitle(\"Whitened Cosine | Avg. Dissimilarity across layers | Visual Encoder\\n(Higher values -> more different)\")\n",
    "# fig.tight_layout()\n",
    "# fig.show()\n",
    "\n",
    "# ## Audio encoder section\n",
    "# audio_encoder_layernames.sort()\n",
    "# audio_encoder_layernames_relu_skipped = audio_encoder_layernames[::2]\n",
    "\n",
    "# n_rows, n_cols = 1, 1\n",
    "# fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * 8, n_rows * 6))\n",
    "\n",
    "# x = np.arange(len(audio_encoder_layernames_relu_skipped)) # 2 PPO varaint per plot for each layer\n",
    "\n",
    "# for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "#     y = [agent_varaint_rdms_mean_std_dict[lynm][\"mean\"] for lynm in audio_encoder_layernames_relu_skipped]\n",
    "#     yerr = [agent_varaint_rdms_mean_std_dict[lynm][\"std\"] for lynm in audio_encoder_layernames_relu_skipped]\n",
    "\n",
    "#     ax.errorbar(x-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "#                  elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "# ax.set_xlabel(\"Layers\")\n",
    "# ax.set_ylabel(\"Average Dissimilarity\")\n",
    "# ax.legend()\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(audio_encoder_layernames_relu_skipped)\n",
    "# fig.suptitle(\"Whitened Cosine | Avg. Dissimilarity across layers | Audio Encoder\\n(Higher values -> more different)\")\n",
    "# fig.tight_layout()\n",
    "# fig.show()\n",
    "\n",
    "# ## State features\n",
    "# n_rows, n_cols = 1, 1\n",
    "# fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * 8, n_rows * 6))\n",
    "\n",
    "# for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "#     y = agent_varaint_rdms_mean_std_dict[\"state_encoder\"][\"mean\"]\n",
    "#     yerr = agent_varaint_rdms_mean_std_dict[\"state_encoder\"][\"std\"]\n",
    "\n",
    "#     ax.errorbar(1-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "#                  elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "# ax.set_ylabel(\"Average Dissimilarity\")\n",
    "# ax.legend()\n",
    "# ax.set_xticks([1])\n",
    "# ax.set_xticklabels([\"state_encoder\"])\n",
    "# ax.set_xlim(0.5, 1.5)\n",
    "# fig.suptitle(\"Whitened Cosine | Avg. Dissimilarity of 'state features' \\n(Higher values -> more different)\")\n",
    "# fig.tight_layout()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All in one plots\n",
    "from matplotlib.gridspec import GridSpec # TODO: move to the top\n",
    "\n",
    "N_VARIANTS = len(VARIANT_RDMS_MEAN_STD.keys())\n",
    "VARIANTS_COLOR_PALETTE = sns.color_palette(n_colors=N_VARIANTS)\n",
    "VARIANTS_BAR_SHIFTING = np.linspace(-0.1, 0.1, N_VARIANTS)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "# Visual and audo encoder layers\n",
    "gs1 = GridSpec(2, 1, left=0.0, right=0.475, wspace=0)\n",
    "\n",
    "## Video encoder layers\n",
    "vis_enc_ax = fig.add_subplot(gs1[0, 0])\n",
    "\n",
    "visual_encoder_layernames.sort()\n",
    "visual_encoder_layernames_relu_skipped = visual_encoder_layernames[::2]\n",
    "x = np.arange(len(visual_encoder_layernames_relu_skipped)) # 2 PPO varaint per plot for each layer\n",
    "\n",
    "for t, (agent_variant, agent_variant_drms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "    y = [agent_variant_drms_mean_std_dict[lynm][\"mean\"] for lynm in visual_encoder_layernames_relu_skipped]\n",
    "    yerr = [agent_variant_drms_mean_std_dict[lynm][\"std\"] for lynm in visual_encoder_layernames_relu_skipped]\n",
    "\n",
    "    vis_enc_ax.errorbar(x-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "                 elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "vis_enc_ax.set_xlabel(\"Video Encoder Layers\", fontsize=14)\n",
    "vis_enc_ax.xaxis.set_label_position(\"top\")\n",
    "vis_enc_ax.set_ylabel(\"Average Dissimilarity\", fontsize=14)\n",
    "vis_enc_ax.set_xticks(x)\n",
    "visual_encoder_layernames_relu_skipped_pretty_names = [\"(1): Conv2d\", \"(3): Conv2d\", \"(5): Conv2d\", \"(7): Linear\"]\n",
    "vis_enc_ax.set_xticklabels(visual_encoder_layernames_relu_skipped_pretty_names, fontsize=12.5)\n",
    "# vis_enc_ax.legend()\n",
    "\n",
    "## Audo encoder layers\n",
    "aud_enc_ax = fig.add_subplot(gs1[1, 0])\n",
    "\n",
    "audio_encoder_layernames.sort()\n",
    "audio_encoder_layernames_relu_skipped = audio_encoder_layernames[::2]\n",
    "\n",
    "x = np.arange(len(audio_encoder_layernames_relu_skipped)) # 2 PPO varaint per plot for each layer\n",
    "\n",
    "for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "    y = [agent_varaint_rdms_mean_std_dict[lynm][\"mean\"] for lynm in audio_encoder_layernames_relu_skipped]\n",
    "    yerr = [agent_varaint_rdms_mean_std_dict[lynm][\"std\"] for lynm in audio_encoder_layernames_relu_skipped]\n",
    "\n",
    "    aud_enc_ax.errorbar(x-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "                 elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "aud_enc_ax.set_xlabel(\"Audio Encoder Layers\", fontsize=14)\n",
    "aud_enc_ax.xaxis.set_label_position(\"top\")\n",
    "aud_enc_ax.set_ylabel(\"Average Dissimilarity\", fontsize=14)\n",
    "aud_enc_ax.set_xticks(x)\n",
    "audio_encoder_layernames_relu_skipped_pretty_names = [\"(1): Conv2d\", \"(3): Conv2d\", \"(5): Conv2d\", \"(7): Linear\"]\n",
    "aud_enc_ax.set_xticklabels(audio_encoder_layernames_relu_skipped_pretty_names, fontsize=12.5)\n",
    "# aud_enc_ax.legend()\n",
    "\n",
    "# State features plot\n",
    "gs2 = GridSpec(1,1, left=0.515, right=0.73, wspace=0)\n",
    "\n",
    "state_feat_ax = fig.add_subplot(gs2[0, 0])\n",
    "\n",
    "for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "    y = agent_varaint_rdms_mean_std_dict[\"state_encoder\"][\"mean\"]\n",
    "    yerr = agent_varaint_rdms_mean_std_dict[\"state_encoder\"][\"std\"]\n",
    "\n",
    "    state_feat_ax.errorbar(1-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "                 elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "state_feat_ax.set_ylabel(\"Average Dissimilarity\", fontsize=14)\n",
    "state_feat_ax.set_ylim(-0.1, 1)\n",
    "state_feat_ax.set_xticks([1])\n",
    "state_feat_ax.set_xticklabels([\"State features\"], fontsize=12.5)\n",
    "state_feat_ax.set_xlabel(\"RNN Cell\", fontsize=14)\n",
    "state_feat_ax.xaxis.set_label_position(\"top\")\n",
    "state_feat_ax.set_xlim(0.5, 1.5)\n",
    "# state_feat_ax.legend()\n",
    "\n",
    "# State \n",
    "gs3 = GridSpec(1, 1, left=0.775, right=1, wspace=0)\n",
    "\n",
    "## Actor componet\n",
    "actor_ax = fig.add_subplot(gs3[0, 0])\n",
    "\n",
    "for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "    y = agent_varaint_rdms_mean_std_dict[\"action_distribution.linear\"][\"mean\"]\n",
    "    yerr = agent_varaint_rdms_mean_std_dict[\"action_distribution.linear\"][\"std\"]\n",
    "\n",
    "    actor_ax.errorbar(1-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "                 elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "actor_ax.set_ylabel(\"Average Dissimilarity\", fontsize=14)\n",
    "actor_ax.set_ylim(-0.1, 1)\n",
    "actor_ax.set_xticks([1])\n",
    "actor_ax.set_xticklabels([\"Policy logits\"], fontsize=12.5)\n",
    "actor_ax.xaxis.set_label_position(\"top\")\n",
    "actor_ax.set_xlabel(\"Actor network\", fontsize=14)\n",
    "actor_ax.set_xlim(0.5, 1.5)\n",
    "actor_ax.legend(fontsize=15)\n",
    "\n",
    "## Critic omponet\n",
    "# NOTE: due to its shape, there is nothing much to analyse at the critic level\n",
    "# critic_ax = fig.add_subplot(gs3[1, 0])\n",
    "\n",
    "# for t, (agent_variant, agent_varaint_rdms_mean_std_dict) in enumerate(VARIANT_RDMS_MEAN_STD.items()):\n",
    "#     y = agent_varaint_rdms_mean_std_dict[\"critic.fc\"][\"mean\"]\n",
    "#     yerr = agent_varaint_rdms_mean_std_dict[\"critic.fc\"][\"std\"]\n",
    "\n",
    "#     critic_ax.errorbar(1-VARIANTS_BAR_SHIFTING[t], y, yerr, fmt=\"none\",\n",
    "#                  elinewidth=3, label=MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], color=VARIANTS_COLOR_PALETTE[t])\n",
    "\n",
    "# critic_ax.set_ylabel(\"Average Dissimilarity\", fontsize=14)\n",
    "# critic_ax.set_ylim(-0.1, 1)\n",
    "# critic_ax.set_xticks([1])\n",
    "# critic_ax.set_xticklabels([\"Value logits\"], fontsize=12.5)\n",
    "# critic_ax.set_xlim(0.5, 1.5)\n",
    "# critic_ax.legend(fontsize=13)\n",
    "\n",
    "fig.suptitle(\"Component's Layer-wise Whitened Cosine-based Average Dissimilarity\", fontsize=18)\n",
    "# fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RSA Dissimilarities plots \n",
    "# ## TODO: add colorbar\n",
    "\n",
    "# ## General\n",
    "# N_VARIANTS = len(VARIANT_RDMS_MEAN_STD.keys())\n",
    "\n",
    "# ## Visual encoder\n",
    "# visual_encoder_layernames.sort()\n",
    "# visual_encoder_layernames_relu_skipped = visual_encoder_layernames[::2]\n",
    "\n",
    "# n_rows, n_cols = N_VARIANTS, len(visual_encoder_layernames_relu_skipped)\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "# dividers = [make_axes_locatable(a) for a in axes[:, -1]]\n",
    "# caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in dividers]\n",
    "\n",
    "# # VMIN, VMAX = None, None\n",
    "# VMIN, VMAX = 0, 1\n",
    "# for ax_idx, layername in enumerate(visual_encoder_layernames_relu_skipped):\n",
    "#     for r_idx, (agent_variant, agent_variant_rdm) in enumerate(VARIANTS_RDMS.items()):\n",
    "#         im = rsatoolbox.vis.show_rdm_panel(agent_variant_rdm[layername], ax=axes[r_idx][ax_idx],\n",
    "#                                       cmap=mpl.colormaps[\"viridis\"], vmin=VMIN, vmax=VMAX)\n",
    "#         fig.colorbar(im, cax=caxes[r_idx], orientation='vertical')\n",
    "\n",
    "#         axes[r_idx][ax_idx].set_xlabel(layername, fontsize=20, position=\"above\")\n",
    "#         axes[r_idx][0].set_ylabel(MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], fontsize=24)\n",
    "\n",
    "# fig.suptitle(\"Vis. Encoder | Layer-wise Dissimilarities Matrix Plots | (Higher values -> more different, color range: [0,1])\", fontsize=26)\n",
    "# fig.tight_layout()\n",
    "# fig.subplots_adjust(top=0.95)\n",
    "# fig.show()\n",
    "\n",
    "# ## Audio encoder\n",
    "# audio_encoder_layernames.sort()\n",
    "# audio_encoder_layernames_relu_skipped = audio_encoder_layernames[::2]\n",
    "\n",
    "# n_rows, n_cols = N_VARIANTS, len(audio_encoder_layernames_relu_skipped)\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "# dividers = [make_axes_locatable(a) for a in axes[:, -1]]\n",
    "# caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in dividers]\n",
    "\n",
    "# # VMIN, VMAX = None, None\n",
    "# VMIN, VMAX = 0, 1\n",
    "# for ax_idx, layername in enumerate(audio_encoder_layernames_relu_skipped):\n",
    "#     for r_idx, (agent_variant, agent_variant_rdm) in enumerate(VARIANTS_RDMS.items()):\n",
    "#         im = rsatoolbox.vis.show_rdm_panel(agent_variant_rdm[layername], ax=axes[r_idx][ax_idx],\n",
    "#                                       cmap=mpl.colormaps[\"viridis\"], vmin=VMIN, vmax=VMAX)\n",
    "#         fig.colorbar(im, cax=caxes[r_idx], orientation='vertical')\n",
    "        \n",
    "#         axes[r_idx][ax_idx].set_xlabel(layername, fontsize=20, position=\"above\")\n",
    "#         axes[r_idx][0].set_ylabel(MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], fontsize=24)\n",
    "\n",
    "# fig.suptitle(\"Aud. Encoder | Layer-wise Dissimilarities Matrix Plots | (Higher values -> more different, color range: [0,1])\", fontsize=26)\n",
    "# fig.tight_layout()\n",
    "# fig.subplots_adjust(top=0.95)\n",
    "# fig.show()\n",
    "\n",
    "# # ## State features\n",
    "# n_rows, n_cols = N_VARIANTS, 1\n",
    "# fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "# dividers = [make_axes_locatable(a) for a in ax]\n",
    "# cax = [ d.append_axes('right', size='5%', pad=0.05) for d in dividers]\n",
    "\n",
    "# for r_idx, (agent_variant, agent_variant_rdm) in enumerate(VARIANTS_RDMS.items()):\n",
    "#     im = rsatoolbox.vis.show_rdm_panel(agent_variant_rdm[\"state_encoder\"], ax=ax[r_idx],\n",
    "#                                        cmap=mpl.colormaps[\"viridis\"], vmin=0, vmax=1)\n",
    "#     [fig.colorbar(im, cax=cax_i, orientation='vertical') for cax_i in cax]\n",
    "\n",
    "#     ax[r_idx].set_xlabel(\"state_encoder\", fontsize=20, position=\"above\")\n",
    "#     ax[r_idx].set_ylabel(MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], fontsize=24)\n",
    "    \n",
    "\n",
    "# fig.set_facecolor(\"white\")\n",
    "# fig.suptitle(\"State features | Layer-wise Dissimilarities Matrix Plots\\n(Higher values -> more different, color range: [0,1])\", fontsize=24)\n",
    "# fig.tight_layout()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All in one Plotn\n",
    "\n",
    "fig = plt.figure(figsize=(22,16))\n",
    "\n",
    "## General\n",
    "N_VARIANTS = len(VARIANT_RDMS_MEAN_STD.keys())\n",
    "\n",
    "## Visual encoder\n",
    "\n",
    "visual_encoder_layernames.sort()\n",
    "visual_encoder_layernames_relu_skipped = visual_encoder_layernames[::2]\n",
    "visual_encoder_layernames_relu_skipped_pretty_names = [\"(1): Conv2d\", \"(3): Conv2d\", \"(5): Conv2d\", \"(7): Linear\"]\n",
    "\n",
    "n_rows, n_cols = N_VARIANTS, len(visual_encoder_layernames_relu_skipped)\n",
    "\n",
    "gs1 = GridSpec(n_rows, n_cols, left=0.0, right=0.735, top=1.0, bottom=0.52, wspace=0.05, hspace=0.05)\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "# dividers = [make_axes_locatable(a) for a in axes[:, -1]]\n",
    "# caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in dividers]\n",
    "\n",
    "vis_enc_axes = [[fig.add_subplot(gs1[i, j]) for j in range(n_cols)] for i in range(n_rows)]\n",
    "vis_dividers = [make_axes_locatable(row_ax[-1]) for row_ax in vis_enc_axes]\n",
    "caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in vis_dividers]\n",
    "\n",
    "# VMIN, VMAX = None, None\n",
    "VMIN, VMAX = 0, 1\n",
    "for ax_idx, layername in enumerate(visual_encoder_layernames_relu_skipped):\n",
    "    for r_idx, (agent_variant, agent_variant_rdm) in enumerate(VARIANTS_RDMS.items()):\n",
    "        im = rsatoolbox.vis.show_rdm_panel(agent_variant_rdm[layername], ax=vis_enc_axes[r_idx][ax_idx],\n",
    "                                      cmap=mpl.colormaps[\"viridis\"], vmin=VMIN, vmax=VMAX)\n",
    "        fig.colorbar(im, cax=caxes[r_idx], orientation='vertical')\n",
    "\n",
    "        vis_enc_axes[r_idx][0].set_ylabel(MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], fontsize=14)\n",
    "        if r_idx == 0:\n",
    "            vis_enc_axes[r_idx][ax_idx].set_xlabel(f\"Visual Encoder - {visual_encoder_layernames_relu_skipped_pretty_names[ax_idx]}\\n\", fontsize=13)\n",
    "            vis_enc_axes[r_idx][ax_idx].xaxis.set_label_position(\"top\")\n",
    "            \n",
    "\n",
    "## Audio encoder\n",
    "\n",
    "audio_encoder_layernames.sort()\n",
    "audio_encoder_layernames_relu_skipped = audio_encoder_layernames[::2]\n",
    "audio_encoder_layernames_relu_skipped_pretty_names = [\"(1): Conv2d\", \"(3): Conv2d\", \"(5): Conv2d\", \"(7): Linear\"]\n",
    "\n",
    "n_rows, n_cols = N_VARIANTS, len(audio_encoder_layernames_relu_skipped)\n",
    "\n",
    "# gs2 = GridSpec(n_rows, n_cols, left=0.0, right=0.75, wspace=0)\n",
    "gs2 = GridSpec(n_rows, n_cols, left=0.0, right=0.735, top=0.48, bottom=0.0, wspace=0.05, hspace=0.05)\n",
    "\n",
    "aud_enc_axes = [[fig.add_subplot(gs2[i, j]) for j in range(n_cols)] for i in range(n_rows)]\n",
    "aud_dividers = [make_axes_locatable(row_ax[-1]) for row_ax in aud_enc_axes]\n",
    "caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in aud_dividers]\n",
    "\n",
    "# VMIN, VMAX = None, None\n",
    "VMIN, VMAX = 0, 1\n",
    "for ax_idx, layername in enumerate(audio_encoder_layernames_relu_skipped):\n",
    "    for r_idx, (agent_variant, agent_variant_rdm) in enumerate(VARIANTS_RDMS.items()):\n",
    "        im = rsatoolbox.vis.show_rdm_panel(agent_variant_rdm[layername], ax=aud_enc_axes[r_idx][ax_idx],\n",
    "                                      cmap=mpl.colormaps[\"viridis\"], vmin=VMIN, vmax=VMAX)\n",
    "        fig.colorbar(im, cax=caxes[r_idx], orientation='vertical')\n",
    "\n",
    "        aud_enc_axes[r_idx][0].set_ylabel(MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], fontsize=14)\n",
    "        if r_idx == 0:\n",
    "            aud_enc_axes[r_idx][ax_idx].set_xlabel(f\"Audio Encoder - {audio_encoder_layernames_relu_skipped_pretty_names[ax_idx]}\\n\", fontsize=13)\n",
    "            aud_enc_axes[r_idx][ax_idx].xaxis.set_label_position(\"top\")\n",
    "\n",
    "## State features\n",
    "\n",
    "n_rows, n_cols = N_VARIANTS, 1\n",
    "gs3 = GridSpec(n_rows, n_cols, left=0.765, right=0.95, top=0.75, bottom=0.25, wspace=0)\n",
    "\n",
    "state_feat_axes = [fig.add_subplot(gs3[r_idx, 0]) for r_idx in range(n_rows)]\n",
    "state_feat_dividers = [make_axes_locatable(a) for a in state_feat_axes]\n",
    "state_feat_cax = [ d.append_axes('right', size='5%', pad=0.05) for d in state_feat_dividers]\n",
    "\n",
    "for r_idx, (agent_variant, agent_variant_rdm) in enumerate(VARIANTS_RDMS.items()):\n",
    "    im = rsatoolbox.vis.show_rdm_panel(agent_variant_rdm[\"state_encoder\"], ax=state_feat_axes[r_idx],\n",
    "                                       cmap=mpl.colormaps[\"viridis\"], vmin=0, vmax=1)\n",
    "    [fig.colorbar(im, cax=cax_i, orientation='vertical') for cax_i in state_feat_cax]\n",
    "\n",
    "    state_feat_axes[r_idx].set_ylabel(MODEL_VARIANTS_TO_STATEDICT_PATH[agent_variant][\"pretty_name\"], fontsize=14)\n",
    "    \n",
    "state_feat_axes[0].set_xlabel(\"State features (RNN Cell)\\n\", fontsize=13, position=\"above\")\n",
    "state_feat_axes[0].xaxis.set_label_position(\"top\")\n",
    "\n",
    "# TODO: position this on top better\n",
    "# fig.suptitle(\"Component's Layer-wise Whitened Cosine-based Average Dissimilarity\", fontsize=18)\n",
    "# fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDM comparisons\n",
    "N_VARIANTS = len(list(VARIANTS_RDMS.keys()))\n",
    "xy_ticks = np.arange(N_VARIANTS)\n",
    "xy_labels = [MODEL_VARIANTS_TO_STATEDICT_PATH[k][\"pretty_name\"] for k, _ in VARIANTS_RDMS.items()]\n",
    "rdm_comparisons = np.zeros([N_VARIANTS, N_VARIANTS])\n",
    "\n",
    "# Comparison method\n",
    "METHOD=\"cosine\"\n",
    "METHOD=\"spearman\"\n",
    "# TODO: optimze by computing only upper or lower triangle of the matrix\n",
    "\n",
    "## Visual encoder\n",
    "visual_encoder_layernames.sort()\n",
    "visual_encoder_layernames_relu_skipped = visual_encoder_layernames[::2]\n",
    "\n",
    "n_rows, n_cols = 1, len(visual_encoder_layernames_relu_skipped)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "dividers = [make_axes_locatable(a) for a in axes]\n",
    "caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in dividers]\n",
    "\n",
    "for layer_idx, layername in enumerate(visual_encoder_layernames_relu_skipped):\n",
    "    # Precompute the RDM compariso matrix\n",
    "    for i, (agent_i_variant, i_rdms) in enumerate(VARIANTS_RDMS.items()):\n",
    "        for j, (agent_j_variant, j_rdms) in enumerate(VARIANTS_RDMS.items()):\n",
    "            rdm_comparisons[i, j] = rsatoolbox.rdm.compare(i_rdms[layername], j_rdms[layername], method=METHOD)\n",
    "    \n",
    "    # Plot comparison RDM matrix\n",
    "    VMIN, VMAX = rdm_comparisons.min(), rdm_comparisons.max() # use the min and max value of similarity value to emphasize the difference\n",
    "    VMIN, VMAX = 0, 1 # Classical colorbar size\n",
    "    im = axes[layer_idx].imshow(rdm_comparisons, cmap=mpl.colormaps[\"magma\"], vmin=VMIN, vmax=VMAX)\n",
    "    fig.colorbar(im, cax=caxes[layer_idx])\n",
    "    axes[layer_idx].set_xticks(xy_ticks, labels=xy_labels, rotation=\"vertical\"), axes[layer_idx].set_yticks(xy_ticks, labels=xy_labels)\n",
    "    axes[layer_idx].set_title(layername)\n",
    "\n",
    "fig.suptitle(\"Whitened Cosine | Visual encoder's layer-wise Similarity between model variants | (Higher value means more similar)\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "## Audio encoder\n",
    "audio_encoder_layernames.sort()\n",
    "audio_encoder_layernames_relu_skipped = audio_encoder_layernames[::2]\n",
    "\n",
    "n_rows, n_cols = 1, len(audio_encoder_layernames_relu_skipped)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "dividers = [make_axes_locatable(a) for a in axes]\n",
    "caxes = [ d.append_axes('right', size='5%', pad=0.1) for d in dividers]\n",
    "\n",
    "for layer_idx, layername in enumerate(audio_encoder_layernames_relu_skipped):\n",
    "    # Precompute the RDM compariso matrix\n",
    "    for i, (agent_i_variant, i_rdms) in enumerate(VARIANTS_RDMS.items()):\n",
    "        for j, (agent_j_variant, j_rdms) in enumerate(VARIANTS_RDMS.items()):\n",
    "            rdm_comparisons[i, j] = rsatoolbox.rdm.compare(i_rdms[layername], j_rdms[layername], method=METHOD)\n",
    "    \n",
    "    # Plot comparison RDM matrix\n",
    "    VMIN, VMAX = rdm_comparisons.min(), rdm_comparisons.max() # use the min and max value of similarity value to emphasize the difference\n",
    "    VMIN, VMAX = 0, 1 # Classical colorbar size\n",
    "    im = axes[layer_idx].imshow(rdm_comparisons, cmap=mpl.colormaps[\"magma\"], vmin=VMIN, vmax=VMAX)\n",
    "    fig.colorbar(im, cax=caxes[layer_idx])\n",
    "    axes[layer_idx].set_xticks(xy_ticks, labels=xy_labels, rotation=\"vertical\"), axes[layer_idx].set_yticks(xy_ticks, labels=xy_labels)\n",
    "    axes[layer_idx].set_title(layername)\n",
    "\n",
    "fig.suptitle(\"Whitened Cosine | Audio encoder's layer-wise Similarity between model variants | (Higher value means more similar)\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# State encoder (state features)\n",
    "for i, (agent_i_variant, i_rdms) in enumerate(VARIANTS_RDMS.items()):\n",
    "    for j, (agent_j_variant, j_rdms) in enumerate(VARIANTS_RDMS.items()):\n",
    "        # RDMs are compute for each layer\n",
    "        for layername in [\"state_encoder\"]: # TODO: broader comparison across other intermediate layers\n",
    "            rdm_comparisons[i, j] = rsatoolbox.rdm.compare(i_rdms[layername], j_rdms[layername], method=METHOD)\n",
    "\n",
    "# Plotting RDM comparision matp\n",
    "fig, ax = plt.subplots(1,1, figsize=((6,6)))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "VMIN, VMAX = rdm_comparisons.min(), rdm_comparisons.max() # use the min and max value of similarity value to emphasize the difference\n",
    "VMIN, VMAX = 0, 1 # Classical colorbar size\n",
    "im = ax.imshow(rdm_comparisons, cmap=mpl.colormaps[\"magma\"], vmin=VMIN, vmax=VMAX)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "ax.set_title(\"state_encoder\")\n",
    "\n",
    "ax.set_xticks(xy_ticks, labels=xy_labels, rotation=\"vertical\"), ax.set_yticks(xy_ticks)\n",
    "ax.set_xticklabels(xy_labels), ax.set_yticklabels(xy_labels)\n",
    "fig.suptitle(\"Whitened Cosine | Layer-wise Similarity between model variants\\n(Higher value means more similar)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: all in one plots for the paper, potentially add plots for the critic and value distribution, especially if we add those varaints into the comparison\n",
    "# TODO: more summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity plots components wise\n",
    "## Note for interpretation: higher sparsity values means that neurons specialize to a certain type of input.\n",
    "## From the paper, \"Higher sparsity values means individual units respnd selectively to a smaller number of images\"\n",
    "\n",
    "from ast import Pass\n",
    "\n",
    "\n",
    "N_VARIANTS = len(list(VARIANT_LAYERS_SPARSITIES.keys()))\n",
    "x_ticks = np.arange(N_VARIANTS)\n",
    "x_tickslabels = [MODEL_VARIANTS_TO_STATEDICT_PATH[k][\"pretty_name\"] for k, _ in VARIANTS_RDMS.items()]\n",
    "\n",
    "# TODO: make it a single plot later, with one subplots for each different componets ?\n",
    "## Visual encoder section\n",
    "visual_encoder_layernames.sort()\n",
    "visual_encoder_layernames_relu_skipped = visual_encoder_layernames[::2]\n",
    "\n",
    "n_rows, n_cols = 1, len(visual_encoder_layernames_relu_skipped)\n",
    "fig, axes = plt.subplots(1, n_cols, figsize=(n_cols * 4, n_rows * 8))\n",
    "\n",
    "for ax_idx, layername in enumerate(visual_encoder_layernames_relu_skipped):\n",
    "    violin_parts = axes[ax_idx].violinplot([\n",
    "        agent_varaint_sparsities[layername] for _, agent_varaint_sparsities in VARIANT_LAYERS_SPARSITIES.items()\n",
    "    ], positions=x_ticks, showmedians=True)\n",
    "\n",
    "    # TODO: darker orange for the PPO PGWT varaint\n",
    "    # TODO: systematic color application\n",
    "    # violin_parts[\"bodies\"][1].set_facecolor(\"orange\")\n",
    "    # violin_parts[\"bodies\"][1].set_edgecolor(\"orange\")\n",
    "    \n",
    "    for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "        vp = violin_parts[partname]\n",
    "        # vp.set_edgecolor(\"orange\")\n",
    "        vp.set_linewidth(2.5)\n",
    "    violin_parts[\"cmedians\"].set_edgecolor(\"red\")\n",
    "\n",
    "    axes[ax_idx].set_xticks(x_ticks, labels=x_tickslabels, rotation=\"vertical\")\n",
    "    # axes[ax_idx].set_xticklabels(x_tickslabels)\n",
    "    axes[ax_idx].set_title(layername)\n",
    "\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.suptitle(\"Layer-wise sparsity for the Visual Encoder\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "## Audio encoder section\n",
    "audio_encoder_layernames.sort()\n",
    "audio_encoder_layernames_relu_skipped = audio_encoder_layernames[::2]\n",
    "\n",
    "n_rows, n_cols = 1, len(audio_encoder_layernames_relu_skipped)\n",
    "fig, axes = plt.subplots(1, n_cols, figsize=(n_cols * 4, n_rows * 8))\n",
    "\n",
    "for ax_idx, layername in enumerate(audio_encoder_layernames_relu_skipped):\n",
    "    violin_parts = axes[ax_idx].violinplot([\n",
    "        agent_varaint_sparsities[layername] for _, agent_varaint_sparsities in VARIANT_LAYERS_SPARSITIES.items()\n",
    "    ], positions=x_ticks, showmedians=True)\n",
    "\n",
    "    # TODO: darker orange for the PPO PGWT varaint\n",
    "    # TODO: systematic color application\n",
    "    # violin_parts[\"bodies\"][1].set_facecolor(\"orange\")\n",
    "    # violin_parts[\"bodies\"][1].set_edgecolor(\"orange\")\n",
    "    \n",
    "    for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "        vp = violin_parts[partname]\n",
    "        # vp.set_edgecolor(\"orange\")\n",
    "        vp.set_linewidth(2.5)\n",
    "    violin_parts[\"cmedians\"].set_edgecolor(\"red\")\n",
    "\n",
    "    axes[ax_idx].set_xticks(x_ticks, labels=x_tickslabels, rotation=\"vertical\")\n",
    "    # axes[ax_idx].set_xticklabels(x_tickslabels)\n",
    "    axes[ax_idx].set_title(layername)\n",
    "\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.suptitle(\"Layer-wise sparsity for the Visual Encoder\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# region: state features sparsity violin plot\n",
    "VARIANT_ALLLAYERS_SPARSITIES = {}\n",
    "for agent_variant, agent_variant_layers_sparsities in VARIANT_LAYERS_SPARSITIES.items():\n",
    "    for layername, agent_layer_sparsities in agent_variant_layers_sparsities.items():\n",
    "        if agent_variant not in VARIANT_ALLLAYERS_SPARSITIES.keys():\n",
    "            VARIANT_ALLLAYERS_SPARSITIES[agent_variant] = []\n",
    "        VARIANT_ALLLAYERS_SPARSITIES[agent_variant].extend(agent_layer_sparsities)\n",
    "\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,8))\n",
    "violin_parts = ax.violinplot([\n",
    "    agent_varaint_sparsities[\"state_encoder\"] for _, agent_varaint_sparsities in VARIANT_LAYERS_SPARSITIES.items()\n",
    "], positions=x_ticks, showmedians=True)\n",
    "# TODO: darker orange for the PPO PGWT varaint\n",
    "# violin_parts[\"bodies\"][1].set_facecolor(\"orange\")\n",
    "# violin_parts[\"bodies\"][1].set_edgecolor(\"orange\")\n",
    "violin_parts[\"cmedians\"].set_edgecolor(\"red\")\n",
    "\n",
    "for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "    vp = violin_parts[partname]\n",
    "    # vp.set_edgecolor(\"orange\")\n",
    "    vp.set_linewidth(2.5)\n",
    "violin_parts[\"cmedians\"].set_edgecolor(\"red\")\n",
    "\n",
    "ax.set_xticks(x_ticks, labels=x_tickslabels, rotation=\"vertical\")\n",
    "fig.suptitle(\"Agent network overal sparsity\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "# endregion: state features sparsity violin plot\n",
    "\n",
    "# region: For the whole network + violin\n",
    "VARIANT_ALLLAYERS_SPARSITIES = {}\n",
    "for agent_variant, agent_variant_layers_sparsities in VARIANT_LAYERS_SPARSITIES.items():\n",
    "    for layername, agent_layer_sparsities in agent_variant_layers_sparsities.items():\n",
    "        if agent_variant not in VARIANT_ALLLAYERS_SPARSITIES.keys():\n",
    "            VARIANT_ALLLAYERS_SPARSITIES[agent_variant] = []\n",
    "        VARIANT_ALLLAYERS_SPARSITIES[agent_variant].extend(agent_layer_sparsities)\n",
    "\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,8))\n",
    "violin_parts = ax.violinplot([\n",
    "    agent_variant_all_sparsities for _, agent_variant_all_sparsities in VARIANT_ALLLAYERS_SPARSITIES.items()\n",
    "], positions=x_ticks, showmedians=True)\n",
    "# TODO: darker orange for the PPO PGWT varaint\n",
    "# violin_parts[\"bodies\"][1].set_facecolor(\"orange\")\n",
    "# violin_parts[\"bodies\"][1].set_edgecolor(\"orange\")\n",
    "violin_parts[\"cmedians\"].set_edgecolor(\"red\")\n",
    "\n",
    "for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "    vp = violin_parts[partname]\n",
    "    # vp.set_edgecolor(\"orange\")\n",
    "    vp.set_linewidth(2.5)\n",
    "violin_parts[\"cmedians\"].set_edgecolor(\"red\")\n",
    "\n",
    "ax.set_xticks(x_ticks, labels=x_tickslabels, rotation=\"vertical\")\n",
    "fig.suptitle(\"Agent network overal sparsity\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "# endregion: For the whole network + violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ss-hab-headless-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8606c1569764bc263c51958f68bba938f45460ba430fa08f16cdd64c0c2e55c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
